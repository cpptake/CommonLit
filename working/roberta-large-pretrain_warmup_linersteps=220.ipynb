{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"},"papermill":{"default_parameters":{},"duration":2702.979398,"end_time":"2021-07-10T17:19:56.452391","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2021-07-10T16:34:53.472993","version":"2.3.3"},"colab":{"name":"roberta-large-pretrain_warmup_linersteps=220.ipynb","provenance":[{"file_id":"1iS7G6cID2A7hr6fcXk7Lzx5PsaAh4GVH","timestamp":1626706774035},{"file_id":"1Tj9UrIvHqTWpnWguX9SPgT3MI6RZ6wI3","timestamp":1626701409698}],"collapsed_sections":["KS-MlzVLkY1i"],"machine_shape":"hm"}},"cells":[{"cell_type":"markdown","metadata":{"id":"UrbX1a6A8Doq"},"source":["# Prerequisite"],"id":"UrbX1a6A8Doq"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YdDxPgc_8SBn","executionInfo":{"status":"ok","timestamp":1626931968683,"user_tz":-540,"elapsed":19329,"user":{"displayName":"小林健史","photoUrl":"","userId":"11137570562930914237"}},"outputId":"2a6d075e-a172-4bb5-b957-ba2dcef0a4a2"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"id":"YdDxPgc_8SBn","execution_count":3,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q-21rfg88O1A","executionInfo":{"status":"ok","timestamp":1626931968684,"user_tz":-540,"elapsed":8,"user":{"displayName":"小林健史","photoUrl":"","userId":"11137570562930914237"}},"outputId":"b5624ab7-f629-4bbc-8843-b5917b89a488"},"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n","  print('and then re-execute this cell.')\n","else:\n","  print(gpu_info)"],"id":"Q-21rfg88O1A","execution_count":4,"outputs":[{"output_type":"stream","text":["Thu Jul 22 05:32:49 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   40C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KMaOL7upRq2w"},"source":["## Install same version of library as Kaggle Notebook"],"id":"KMaOL7upRq2w"},{"cell_type":"code","metadata":{"id":"LBS2Zv7DR2tk"},"source":["# # cp でrequirementsをカレントdirにコピー\n","\n","# !mkdir /content/drive/MyDrive/CommonLit/input/commonlitreadabilityprize/roberta-large-mlm\n","# !unzip -n /content/drive/MyDrive/CommonLit/input/commonlitreadabilityprize/Roberta-large-mlm.zip -d /content/drive/MyDrive/CommonLit/input/commonlitreadabilityprize/roberta-large-mlm"],"id":"LBS2Zv7DR2tk","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k7y1hmfoS-Q1","executionInfo":{"status":"ok","timestamp":1626932022732,"user_tz":-540,"elapsed":35481,"user":{"displayName":"小林健史","photoUrl":"","userId":"11137570562930914237"}},"outputId":"4b4f257e-005f-4114-a133-d31a98c94df8"},"source":["!pip uninstall -r /content/drive/MyDrive/CommonLit/working/requirements.txt -y"],"id":"k7y1hmfoS-Q1","execution_count":5,"outputs":[{"output_type":"stream","text":["Found existing installation: pandas 1.1.5\n","Uninstalling pandas-1.1.5:\n","  Successfully uninstalled pandas-1.1.5\n","Found existing installation: sklearn 0.0\n","Uninstalling sklearn-0.0:\n","  Successfully uninstalled sklearn-0.0\n","Found existing installation: sklearn-pandas 1.8.0\n","Uninstalling sklearn-pandas-1.8.0:\n","  Successfully uninstalled sklearn-pandas-1.8.0\n","Found existing installation: torch 1.9.0+cu102\n","Uninstalling torch-1.9.0+cu102:\n","  Successfully uninstalled torch-1.9.0+cu102\n","\u001b[33mWARNING: Skipping torchmetrics as it is not installed.\u001b[0m\n","Found existing installation: torchvision 0.10.0+cu102\n","Uninstalling torchvision-0.10.0+cu102:\n","  Successfully uninstalled torchvision-0.10.0+cu102\n","Found existing installation: transformers 4.8.2\n","Uninstalling transformers-4.8.2:\n","  Successfully uninstalled transformers-4.8.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"DhM67RrVQYws","executionInfo":{"status":"ok","timestamp":1626932113402,"user_tz":-540,"elapsed":90672,"user":{"displayName":"小林健史","photoUrl":"","userId":"11137570562930914237"}},"outputId":"3ebe4a99-a50a-4970-df9b-9d8ad71c6056"},"source":["!pip install -r /content/drive/MyDrive/CommonLit/working/requirements.txt"],"id":"DhM67RrVQYws","execution_count":6,"outputs":[{"output_type":"stream","text":["Collecting pandas==1.2.3\n","  Downloading pandas-1.2.3-cp37-cp37m-manylinux1_x86_64.whl (9.9 MB)\n","\u001b[K     |████████████████████████████████| 9.9 MB 6.2 MB/s \n","\u001b[?25hCollecting sklearn==0.0\n","  Downloading sklearn-0.0.tar.gz (1.1 kB)\n","Collecting sklearn-pandas==2.1.0\n","  Downloading sklearn_pandas-2.1.0-py2.py3-none-any.whl (10 kB)\n","Collecting torch==1.7.0\n","  Downloading torch-1.7.0-cp37-cp37m-manylinux1_x86_64.whl (776.7 MB)\n","\u001b[K     |████████████████████████████████| 776.7 MB 4.1 kB/s \n","\u001b[?25hCollecting torchmetrics==0.2.0\n","  Downloading torchmetrics-0.2.0-py3-none-any.whl (176 kB)\n","\u001b[K     |████████████████████████████████| 176 kB 55.2 MB/s \n","\u001b[?25hCollecting torchvision==0.8.1\n","  Downloading torchvision-0.8.1-cp37-cp37m-manylinux1_x86_64.whl (12.7 MB)\n","\u001b[K     |████████████████████████████████| 12.7 MB 26.5 MB/s \n","\u001b[?25hCollecting transformers==4.5.1\n","  Downloading transformers-4.5.1-py3-none-any.whl (2.1 MB)\n","\u001b[K     |████████████████████████████████| 2.1 MB 56.6 MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.2.3->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 1)) (2.8.1)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.2.3->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 1)) (2018.9)\n","Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from pandas==1.2.3->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 1)) (1.19.5)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn==0.0->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 2)) (0.22.2.post1)\n","Collecting scipy>=1.5.1\n","  Downloading scipy-1.7.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (28.5 MB)\n","\u001b[K     |████████████████████████████████| 28.5 MB 1.4 MB/s \n","\u001b[?25hCollecting scikit-learn\n","  Downloading scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n","\u001b[K     |████████████████████████████████| 22.3 MB 1.3 MB/s \n","\u001b[?25hCollecting dataclasses\n","  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 4)) (0.16.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 4)) (3.7.4.3)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.8.1->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 7)) (7.1.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 8)) (2019.12.20)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 8)) (0.10.3)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 8)) (4.6.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 8)) (3.0.12)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 8)) (21.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 8)) (0.0.45)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 8)) (4.41.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 8)) (2.23.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas==1.2.3->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 1)) (1.15.0)\n","Collecting threadpoolctl>=2.0.0\n","  Downloading threadpoolctl-2.2.0-py3-none-any.whl (12 kB)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn==0.0->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 2)) (1.0.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.5.1->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 8)) (3.5.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.5.1->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 8)) (2.4.7)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 8)) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 8)) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 8)) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 8)) (2021.5.30)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.1->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 8)) (7.1.2)\n","Building wheels for collected packages: sklearn\n","  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1309 sha256=e1e914a71743636be5c2f5b54fe9666c6c04f1deac0fcd097393d1b5c0494b5c\n","  Stored in directory: /root/.cache/pip/wheels/46/ef/c3/157e41f5ee1372d1be90b09f74f82b10e391eaacca8f22d33e\n","Successfully built sklearn\n","Installing collected packages: threadpoolctl, scipy, dataclasses, torch, scikit-learn, pandas, transformers, torchvision, torchmetrics, sklearn-pandas, sklearn\n","  Attempting uninstall: scipy\n","    Found existing installation: scipy 1.4.1\n","    Uninstalling scipy-1.4.1:\n","      Successfully uninstalled scipy-1.4.1\n","  Attempting uninstall: scikit-learn\n","    Found existing installation: scikit-learn 0.22.2.post1\n","    Uninstalling scikit-learn-0.22.2.post1:\n","      Successfully uninstalled scikit-learn-0.22.2.post1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchtext 0.10.0 requires torch==1.9.0, but you have torch 1.7.0 which is incompatible.\n","google-colab 1.0.0 requires pandas~=1.1.0; python_version >= \"3.0\", but you have pandas 1.2.3 which is incompatible.\n","albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Successfully installed dataclasses-0.6 pandas-1.2.3 scikit-learn-0.24.2 scipy-1.7.0 sklearn-0.0 sklearn-pandas-2.1.0 threadpoolctl-2.2.0 torch-1.7.0 torchmetrics-0.2.0 torchvision-0.8.1 transformers-4.5.1\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["dataclasses","pandas","scipy","torch","transformers"]}}},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HBMkIBcxUIb2","executionInfo":{"status":"ok","timestamp":1626932114109,"user_tz":-540,"elapsed":711,"user":{"displayName":"小林健史","photoUrl":"","userId":"11137570562930914237"}},"outputId":"5baadb2c-bc97-4d19-fe3d-42e9b75b7187"},"source":["!pip freeze |grep -e random -e math -e numpy -e pandas -e torch -e transformers -e sklearn -e gc"],"id":"HBMkIBcxUIb2","execution_count":7,"outputs":[{"output_type":"stream","text":["mpmath==1.2.1\n","numpy==1.19.5\n","pandas==1.2.3\n","pandas-datareader==0.9.0\n","pandas-gbq==0.13.3\n","pandas-profiling==1.4.1\n","sklearn==0.0\n","sklearn-pandas==2.1.0\n","tensorflow-gcs-config==2.5.0\n","torch==1.7.0\n","torchmetrics==0.2.0\n","torchsummary==1.5.1\n","torchtext==0.10.0\n","torchvision==0.8.1\n","transformers==4.5.1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"De2yfzg48VPx"},"source":["## Prepare dataset"],"id":"De2yfzg48VPx"},{"cell_type":"markdown","metadata":{"id":"_YZk0yJH8mWO"},"source":["### kaggle.json"],"id":"_YZk0yJH8mWO"},{"cell_type":"code","metadata":{"id":"DEuB9fOD8j8l"},"source":["# !mkdir -p /root/.kaggle/\n","# !cp ./drive/MyDrive/kaggle/commonlit/kaggle.json ~/.kaggle/kaggle.json\n","# !chmod 600 ~/.kaggle/kaggle.json\n","\n","# !pip install -q kaggle\n","# !mkdir /root/.kaggle\n","# !cp /content/drive/MyDrive/Colab\\ Notebooks/kaggle.json /root/.kaggle/"],"id":"DEuB9fOD8j8l","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4oKBPd9H8q0L"},"source":["### Competition dataset"],"id":"4oKBPd9H8q0L"},{"cell_type":"code","metadata":{"id":"lv4fLHsg8vsO"},"source":["# !mkdir -p /content/drive/MyDrive/CommonLit/input/commonlitreadabilityprize/\n","# !kaggle competitions download -c commonlitreadabilityprize -p /content/drive/MyDrive/CommonLit/input/commonlitreadabilityprize/\n","# !cp -f ./drive/MyDrive/kaggle/commonlit/train_stratiKfold.csv.zip /content/drive/MyDrive/CommonLit/input/commonlitreadabilityprize/"],"id":"lv4fLHsg8vsO","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R1QcZXR79EO2"},"source":["# !unzip -o ../input/commonlitreadabilityprize/train.csv.zip -d ../input/commonlitreadabilityprize/\n","# !unzip -o ../input/commonlitreadabilityprize/train_stratiKfold.csv.zip -d ../input/commonlitreadabilityprize/"],"id":"R1QcZXR79EO2","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FAJaSyc89GiB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626932114522,"user_tz":-540,"elapsed":414,"user":{"displayName":"小林健史","photoUrl":"","userId":"11137570562930914237"}},"outputId":"38aa0df7-82f4-49ed-902f-2ec04f862bf6"},"source":["!ls ../input/commonlitreadabilityprize/"],"id":"FAJaSyc89GiB","execution_count":8,"outputs":[{"output_type":"stream","text":["ls: cannot access '../input/commonlitreadabilityprize/': No such file or directory\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"03O1cJpp9HAO"},"source":["### Pretrained RoBERTa Large \n","- Pretrain RoBERTa Large in the same way as this notebook\n","  - https://www.kaggle.com/maunish/clrp-pytorch-roberta-pretrain\n","- Dataset:\n","  - https://www.kaggle.com/iamnishipy/roberta-large-20210712191259-mlm"],"id":"03O1cJpp9HAO"},{"cell_type":"code","metadata":{"id":"7DFyAMzl9Rs_","executionInfo":{"status":"ok","timestamp":1626932114522,"user_tz":-540,"elapsed":4,"user":{"displayName":"小林健史","photoUrl":"","userId":"11137570562930914237"}}},"source":["# !mkdir -p /content/drive/MyDrive/CommonLit/input/commonlitreadabilityprize/pretrained-model/\n","# !kaggle datasets download iamnishipy/roberta-large-20210712191259-mlm"],"id":"7DFyAMzl9Rs_","execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"Izsn9ikg99Bn","executionInfo":{"status":"ok","timestamp":1626932114523,"user_tz":-540,"elapsed":5,"user":{"displayName":"小林健史","photoUrl":"","userId":"11137570562930914237"}}},"source":["# !unzip -o ./roberta-large-20210712191259-mlm.zip -d ../input/commonlitreadabilityprize/pretrained-model/"],"id":"Izsn9ikg99Bn","execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"WWsjc3K2UJrO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626932114523,"user_tz":-540,"elapsed":5,"user":{"displayName":"小林健史","photoUrl":"","userId":"11137570562930914237"}},"outputId":"036082e4-8338-4e0b-9545-4e96869ae2e1"},"source":["!ls ../input/commonlitreadabilityprize/pretrained-model/"],"id":"WWsjc3K2UJrO","execution_count":11,"outputs":[{"output_type":"stream","text":["ls: cannot access '../input/commonlitreadabilityprize/pretrained-model/': No such file or directory\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AEGf6ny98zoo"},"source":[""],"id":"AEGf6ny98zoo"},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.020873,"end_time":"2021-07-10T16:35:02.554689","exception":false,"start_time":"2021-07-10T16:35:02.533816","status":"completed"},"tags":[],"id":"central-liberia"},"source":["# Overview\n","This is kernel is almost the same as [Lightweight Roberta solution in PyTorch](https://www.kaggle.com/andretugan/lightweight-roberta-solution-in-pytorch), but instead of \"roberta-base\", it starts from [Maunish's pre-trained model](https://www.kaggle.com/maunish/clrp-roberta-base).\n","\n","Acknowledgments: some ideas were taken from kernels by [Torch](https://www.kaggle.com/rhtsingh) and [Maunish](https://www.kaggle.com/maunish)."],"id":"central-liberia"},{"cell_type":"code","metadata":{"id":"Dsgr4s1G-m73","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626932341495,"user_tz":-540,"elapsed":3757,"user":{"displayName":"小林健史","photoUrl":"","userId":"11137570562930914237"}},"outputId":"12301497-853f-4e27-8957-dede8628c03c"},"source":["!pip install transformers accelerate datasets"],"id":"Dsgr4s1G-m73","execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.5.1)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.7/dist-packages (0.3.0)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.10.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: pyaml>=20.4.0 in /usr/local/lib/python3.7/dist-packages (from accelerate) (20.4.0)\n","Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from accelerate) (1.7.0)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyaml>=20.4.0->accelerate) (3.13)\n","Requirement already satisfied: dataclasses in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->accelerate) (0.6)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->accelerate) (0.16.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->accelerate) (3.7.4.3)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n","Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2021.7.0)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.2.3)\n","Requirement already satisfied: huggingface-hub<0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.0.12)\n","Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.2)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2021-07-10T16:35:02.605794Z","iopub.status.busy":"2021-07-10T16:35:02.602341Z","iopub.status.idle":"2021-07-10T16:35:11.998468Z","shell.execute_reply":"2021-07-10T16:35:11.999041Z","shell.execute_reply.started":"2021-07-10T16:33:36.630414Z"},"papermill":{"duration":9.425549,"end_time":"2021-07-10T16:35:11.999387","exception":false,"start_time":"2021-07-10T16:35:02.573838","status":"completed"},"tags":[],"id":"classical-garage","executionInfo":{"status":"ok","timestamp":1626932349175,"user_tz":-540,"elapsed":600,"user":{"displayName":"小林健史","photoUrl":"","userId":"11137570562930914237"}}},"source":["import os\n","import math\n","import random\n","import time\n","\n","import numpy as np\n","import pandas as pd\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","\n","from transformers import AdamW\n","from transformers import AutoTokenizer\n","# from transformers import AutoModel\n","from transformers import AutoConfig\n","from transformers import get_cosine_schedule_with_warmup,get_linear_schedule_with_warmup\n","\n","from sklearn.model_selection import KFold\n","\n","import gc\n","gc.enable()"],"id":"classical-garage","execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":447},"id":"v-1qinwvuDIX","executionInfo":{"status":"error","timestamp":1626932270673,"user_tz":-540,"elapsed":242,"user":{"displayName":"小林健史","photoUrl":"","userId":"11137570562930914237"}},"outputId":"6d353ed4-352a-4740-e0fa-eedf6a4b3176"},"source":["from sklearn.model_selection import KFold"],"id":"v-1qinwvuDIX","execution_count":16,"outputs":[{"output_type":"error","ename":"ImportError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-bb1845f52b43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKFold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_distributor_init\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__check_build\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_versions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_IS_32BIT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m from .utils._tags import (\n\u001b[1;32m     19\u001b[0m     \u001b[0m_DEFAULT_TAGS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmurmurhash\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmurmurhash3_32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompute_class_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_joblib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataConversionWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/class_weight.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mvalidation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_deprecate_positional_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcontextlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msuppress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_object_dtype_isnan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_config\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_get_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPositiveSpectrumWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlsqr\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msparse_lsqr\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMaskedArray\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_MaskedArray\u001b[0m  \u001b[0;31m# TODO: remove in 1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/stats/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    439\u001b[0m \"\"\"\n\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmorestats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/stats/stats.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspatial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcdist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndimage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmeasurements\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m from scipy._lib._util import (check_random_state, MapWrapper,\n\u001b[0m\u001b[1;32m     40\u001b[0m                               rng_integers, float_factorial)\n\u001b[1;32m     41\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mspecial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'rng_integers' from 'scipy._lib._util' (/usr/local/lib/python3.7/dist-packages/scipy/_lib/_util.py)","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.017537,"end_time":"2021-07-10T16:35:12.036899","exception":false,"start_time":"2021-07-10T16:35:12.019362","status":"completed"},"tags":[],"id":"challenging-bottle"},"source":["## Prepare dataset"],"id":"challenging-bottle"},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-07-10T16:35:12.966641Z","iopub.status.busy":"2021-07-10T16:35:12.965516Z","iopub.status.idle":"2021-07-10T16:35:12.969362Z","shell.execute_reply":"2021-07-10T16:35:12.968739Z","shell.execute_reply.started":"2021-06-30T12:51:59.456435Z"},"papermill":{"duration":0.076388,"end_time":"2021-07-10T16:35:12.969525","exception":false,"start_time":"2021-07-10T16:35:12.893137","status":"completed"},"tags":[],"id":"measured-cornwall"},"source":["NUM_FOLDS = 5\n","NUM_EPOCHS = 3\n","BATCH_SIZE = 8\n","MAX_LEN = 248\n","EVAL_SCHEDULE = [(0.50, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1., 1)]\n","\n","ROBERTA_PATH = \"/content/drive/MyDrive/CommonLit/input/commonlitreadabilityprize/roberta-large-mlm/clrp_roberta_large\"\n","TOKENIZER_PATH = \"/content/drive/MyDrive/CommonLit/input/commonlitreadabilityprize/roberta-large-mlm/clrp_roberta_large\"\n","# ROBERTA_PATH = \"/content/drive/MyDrive/CommonLit/input/commonlitreadabilityprize/roberta-large\"\n","# TOKENIZER_PATH = \"/content/drive/MyDrive/CommonLit/input/commonlitreadabilityprize/roberta-large\"\n","\n","# ROBERTA_PATH = \"../input/clrp-roberta-base/clrp_roberta_base\"\n","# TOKENIZER_PATH = \"../input/clrp-roberta-base/clrp_roberta_base\"\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""],"id":"measured-cornwall","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-07-10T16:35:13.013652Z","iopub.status.busy":"2021-07-10T16:35:13.012651Z","iopub.status.idle":"2021-07-10T16:35:13.016079Z","shell.execute_reply":"2021-07-10T16:35:13.015529Z","shell.execute_reply.started":"2021-06-30T12:51:59.470504Z"},"papermill":{"duration":0.028052,"end_time":"2021-07-10T16:35:13.016225","exception":false,"start_time":"2021-07-10T16:35:12.988173","status":"completed"},"tags":[],"id":"sitting-brook"},"source":["def set_random_seed(random_seed):\n","    random.seed(random_seed)\n","    np.random.seed(random_seed)\n","    os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n","\n","    torch.manual_seed(random_seed)\n","    torch.cuda.manual_seed(random_seed)\n","    torch.cuda.manual_seed_all(random_seed)\n","\n","    torch.backends.cudnn.deterministic = True"],"id":"sitting-brook","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-07-10T16:35:13.059695Z","iopub.status.busy":"2021-07-10T16:35:13.059033Z","iopub.status.idle":"2021-07-10T16:35:13.181860Z","shell.execute_reply":"2021-07-10T16:35:13.181008Z","shell.execute_reply.started":"2021-06-30T12:51:59.485325Z"},"papermill":{"duration":0.147365,"end_time":"2021-07-10T16:35:13.182094","exception":false,"start_time":"2021-07-10T16:35:13.034729","status":"completed"},"tags":[],"id":"banner-plastic"},"source":["train_df = pd.read_csv(\"/content/drive/MyDrive/CommonLit/input/commonlitreadabilityprize/train.csv\")\n","\n","# Remove incomplete entries if any.\n","train_df.drop(train_df[(train_df.target == 0) & (train_df.standard_error == 0)].index,\n","              inplace=True)\n","train_df.reset_index(drop=True, inplace=True)\n","\n","test_df = pd.read_csv(\"/content/drive/MyDrive/CommonLit/input/commonlitreadabilityprize/test.csv\")\n","submission_df = pd.read_csv(\"/content/drive/MyDrive/CommonLit/input/commonlitreadabilityprize/sample_submission.csv\")"],"id":"banner-plastic","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-07-10T16:35:13.223654Z","iopub.status.busy":"2021-07-10T16:35:13.222955Z","iopub.status.idle":"2021-07-10T16:35:13.465852Z","shell.execute_reply":"2021-07-10T16:35:13.464700Z","shell.execute_reply.started":"2021-06-30T12:51:59.537207Z"},"papermill":{"duration":0.265264,"end_time":"2021-07-10T16:35:13.466048","exception":false,"start_time":"2021-07-10T16:35:13.200784","status":"completed"},"tags":[],"id":"unavailable-philadelphia"},"source":["tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)"],"id":"unavailable-philadelphia","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.018281,"end_time":"2021-07-10T16:35:13.502997","exception":false,"start_time":"2021-07-10T16:35:13.484716","status":"completed"},"tags":[],"id":"intermediate-brand"},"source":["# Dataset"],"id":"intermediate-brand"},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-07-10T16:35:13.549331Z","iopub.status.busy":"2021-07-10T16:35:13.548372Z","iopub.status.idle":"2021-07-10T16:35:13.552476Z","shell.execute_reply":"2021-07-10T16:35:13.551942Z","shell.execute_reply.started":"2021-06-30T12:51:59.797452Z"},"papermill":{"duration":0.031082,"end_time":"2021-07-10T16:35:13.552607","exception":false,"start_time":"2021-07-10T16:35:13.521525","status":"completed"},"tags":[],"id":"adopted-prayer"},"source":["class LitDataset(Dataset):\n","    def __init__(self, df, inference_only=False):\n","        super().__init__()\n","\n","        self.df = df        \n","        self.inference_only = inference_only\n","        self.text = df.excerpt.tolist()\n","        #self.text = [text.replace(\"\\n\", \" \") for text in self.text]\n","        \n","        if not self.inference_only:\n","            self.target = torch.tensor(df.target.values, dtype=torch.float32)        \n","    \n","        self.encoded = tokenizer.batch_encode_plus(\n","            self.text,\n","            padding = 'max_length',            \n","            max_length = MAX_LEN,\n","            truncation = True,\n","            return_attention_mask=True\n","        )        \n"," \n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    \n","    def __getitem__(self, index):        \n","        input_ids = torch.tensor(self.encoded['input_ids'][index])\n","        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n","        \n","        if self.inference_only:\n","            return (input_ids, attention_mask)            \n","        else:\n","            target = self.target[index]\n","            return (input_ids, attention_mask, target)"],"id":"adopted-prayer","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.018016,"end_time":"2021-07-10T16:35:13.588706","exception":false,"start_time":"2021-07-10T16:35:13.570690","status":"completed"},"tags":[],"id":"sonic-cooperative"},"source":["# Model\n","The model is inspired by the one from [Maunish](https://www.kaggle.com/maunish/clrp-roberta-svm)."],"id":"sonic-cooperative"},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-07-10T16:35:13.662281Z","iopub.status.busy":"2021-07-10T16:35:13.661368Z","iopub.status.idle":"2021-07-10T16:35:13.679333Z","shell.execute_reply":"2021-07-10T16:35:13.680266Z","shell.execute_reply.started":"2021-06-30T12:51:59.811644Z"},"papermill":{"duration":0.063207,"end_time":"2021-07-10T16:35:13.680545","exception":false,"start_time":"2021-07-10T16:35:13.617338","status":"completed"},"tags":[],"id":"listed-coordinate"},"source":["class LitModel(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n","        config.update({\"output_hidden_states\":True, \n","                       \"hidden_dropout_prob\": 0.0,\n","                       \"layer_norm_eps\": 1e-7})                       \n","        \n","        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config)\n","        #https://towardsdatascience.com/attention-based-deep-multiple-instance-learning-1bb3df857e24\n","        # 768: node fully connected layer\n","        # 512: node attention layer\n","        # self.attention = nn.Sequential(            \n","        #     nn.Linear(768, 512),            \n","        #     nn.Tanh(),                       \n","        #     nn.Linear(512, 1),\n","        #     nn.Softmax(dim=1)\n","        # )        \n","\n","        # self.regressor = nn.Sequential(                        \n","        #     nn.Linear(768, 1)                        \n","        # )\n","\n","\n","        #768 -> 1024\n","        #512 -> 768\n","        self.attention = nn.Sequential(            \n","            nn.Linear(1024, 768),            \n","            nn.Tanh(),                       \n","            nn.Linear(768, 1),\n","            nn.Softmax(dim=1)\n","        )        \n","\n","        self.regressor = nn.Sequential(                        \n","            nn.Linear(1024, 1)                        \n","        )\n","        \n","\n","    def forward(self, input_ids, attention_mask):\n","        # roberta_output = self.roberta(input_ids=input_ids,\n","        #                               attention_mask=attention_mask)\n","        \n","        roberta_output = self.roberta(input_ids=input_ids,\n","                                      attention_mask=attention_mask)\n","        \n","        #### mean pooling ###\n","        # last_hidden_state = roberta_output[0]\n","        # input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n","        # sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n","        # sum_mask = input_mask_expanded.sum(1)\n","        # sum_mask = torch.clamp(sum_mask, min=1e-9)\n","        # mean_embeddings = sum_embeddings / sum_mask\n","\n","        # # print(mean_embeddings.shape)\n","\n","        # logits = self.regressor(mean_embeddings)\n","        \n","        # preds = logits.squeeze(-1).squeeze(-1)\n","\n","        # return preds\n","\n","        ### nishipy original ####\n","        # There are a total of 13 layers of hidden states.\n","        # 1 for the embedding layer, and 12 for the 12 Roberta layers.\n","        # We take the hidden states from the last Roberta layer.\n","        last_layer_hidden_states = roberta_output.hidden_states[-1]\n","\n","        # The number of cells is MAX_LEN.\n","        # The size of the hidden state of each cell is 768 (for roberta-base).\n","        # In order to condense hidden states of all cells to a context vector,\n","        # we compute a weighted average of the hidden states of all cells.\n","        # We compute the weight of each cell, using the attention neural network.\n","        weights = self.attention(last_layer_hidden_states)\n","                \n","        # weights.shape is BATCH_SIZE x MAX_LEN x 1\n","        # last_layer_hidden_states.shape is BATCH_SIZE x MAX_LEN x 768        \n","        # Now we compute context_vector as the weighted average.\n","        # context_vector.shape is BATCH_SIZE x 768\n","        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n","        \n","        # Now we reduce the context vector to the prediction score.\n","        return self.regressor(context_vector)"],"id":"listed-coordinate","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-07-10T16:35:13.776053Z","iopub.status.busy":"2021-07-10T16:35:13.774791Z","iopub.status.idle":"2021-07-10T16:35:13.782919Z","shell.execute_reply":"2021-07-10T16:35:13.784211Z","shell.execute_reply.started":"2021-06-30T12:51:59.831662Z"},"papermill":{"duration":0.069033,"end_time":"2021-07-10T16:35:13.784378","exception":false,"start_time":"2021-07-10T16:35:13.715345","status":"completed"},"tags":[],"id":"marked-citation"},"source":["def eval_mse(model, data_loader):\n","    \"\"\"Evaluates the mean squared error of the |model| on |data_loader|\"\"\"\n","    model.eval()            \n","    mse_sum = 0\n","\n","    with torch.no_grad():\n","        for batch_num, (input_ids, attention_mask, target) in enumerate(data_loader):\n","            input_ids = input_ids.to(DEVICE)\n","            attention_mask = attention_mask.to(DEVICE)                        \n","            target = target.to(DEVICE)           \n","            \n","            pred = model(input_ids, attention_mask)                       \n","\n","            mse_sum += nn.MSELoss(reduction=\"sum\")(pred.flatten(), target).item()\n","                \n","\n","    return mse_sum / len(data_loader.dataset)"],"id":"marked-citation","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-07-10T16:35:13.860639Z","iopub.status.busy":"2021-07-10T16:35:13.859518Z","iopub.status.idle":"2021-07-10T16:35:13.865246Z","shell.execute_reply":"2021-07-10T16:35:13.867010Z","shell.execute_reply.started":"2021-06-30T12:51:59.844758Z"},"papermill":{"duration":0.049393,"end_time":"2021-07-10T16:35:13.867227","exception":false,"start_time":"2021-07-10T16:35:13.817834","status":"completed"},"tags":[],"id":"associate-astrology"},"source":["def predict(model, data_loader):\n","    \"\"\"Returns an np.array with predictions of the |model| on |data_loader|\"\"\"\n","    model.eval()\n","\n","    result = np.zeros(len(data_loader.dataset))    \n","    index = 0\n","    \n","    with torch.no_grad():\n","        for batch_num, (input_ids, attention_mask) in enumerate(data_loader):\n","            input_ids = input_ids.to(DEVICE)\n","            attention_mask = attention_mask.to(DEVICE)\n","                        \n","            pred = model(input_ids, attention_mask)                        \n","\n","            result[index : index + pred.shape[0]] = pred.flatten().to(\"cpu\")\n","            index += pred.shape[0]\n","\n","    return result"],"id":"associate-astrology","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-07-10T16:35:13.957154Z","iopub.status.busy":"2021-07-10T16:35:13.955894Z","iopub.status.idle":"2021-07-10T16:35:13.970258Z","shell.execute_reply":"2021-07-10T16:35:13.971515Z","shell.execute_reply.started":"2021-06-30T12:51:59.863562Z"},"papermill":{"duration":0.064065,"end_time":"2021-07-10T16:35:13.971767","exception":false,"start_time":"2021-07-10T16:35:13.907702","status":"completed"},"tags":[],"id":"impressed-minnesota"},"source":["def train(model, model_path, train_loader, val_loader,\n","          optimizer, scheduler=None, num_epochs=NUM_EPOCHS):    \n","    best_val_rmse = None\n","    best_epoch = 0\n","    step = 0\n","    last_eval_step = 0\n","    eval_period = EVAL_SCHEDULE[0][1]    \n","\n","    start = time.time()\n","\n","    for epoch in range(num_epochs):                           \n","        val_rmse = None         \n","\n","        for batch_num, (input_ids, attention_mask, target) in enumerate(train_loader):\n","            input_ids = input_ids.to(DEVICE)\n","            attention_mask = attention_mask.to(DEVICE)            \n","            target = target.to(DEVICE)                        \n","\n","            optimizer.zero_grad()\n","            \n","            model.train()\n","\n","            pred = model(input_ids, attention_mask)\n","                                                        \n","            mse = nn.MSELoss(reduction=\"mean\")(pred.flatten(), target)\n","                        \n","            mse.backward()\n","\n","            optimizer.step()\n","            if scheduler:\n","                scheduler.step()\n","            \n","            if step >= last_eval_step + eval_period:\n","                # Evaluate the model on val_loader.\n","                elapsed_seconds = time.time() - start\n","                num_steps = step - last_eval_step\n","                print(f\"\\n{num_steps} steps took {elapsed_seconds:0.3} seconds\")\n","                last_eval_step = step\n","                \n","                val_rmse = math.sqrt(eval_mse(model, val_loader))                            \n","\n","                print(f\"Epoch: {epoch} batch_num: {batch_num}\", \n","                      f\"val_rmse: {val_rmse:0.4}\")\n","\n","                for rmse, period in EVAL_SCHEDULE:\n","                    if val_rmse >= rmse:\n","                        eval_period = period\n","                        break                               \n","                \n","                if not best_val_rmse or val_rmse < best_val_rmse:                    \n","                    best_val_rmse = val_rmse\n","                    best_epoch = epoch\n","                    torch.save(model.state_dict(), model_path)\n","                    print(f\"New best_val_rmse: {best_val_rmse:0.4}\")\n","                else:       \n","                    print(f\"Still best_val_rmse: {best_val_rmse:0.4}\",\n","                          f\"(from epoch {best_epoch})\")                                    \n","                    \n","                start = time.time()\n","                                            \n","            step += 1\n","                        \n","    \n","    return best_val_rmse"],"id":"impressed-minnesota","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-07-10T16:35:14.053687Z","iopub.status.busy":"2021-07-10T16:35:14.052560Z","iopub.status.idle":"2021-07-10T16:35:14.060346Z","shell.execute_reply":"2021-07-10T16:35:14.062050Z","shell.execute_reply.started":"2021-06-30T12:51:59.879987Z"},"papermill":{"duration":0.054887,"end_time":"2021-07-10T16:35:14.062272","exception":false,"start_time":"2021-07-10T16:35:14.007385","status":"completed"},"tags":[],"id":"handled-trouble"},"source":["#怪しい\n","def create_optimizer(model):\n","    #model.named_parameters():\n","    #Base -> 205\n","    #Large -> 397\n","    named_parameters = list(model.named_parameters())    \n","\n","    #Base\n","    # roberta_parameters = named_parameters[:197]    \n","    # attention_parameters = named_parameters[199:203]\n","    # regressor_parameters = named_parameters[203:]\n","    \n","    #Large\n","    roberta_parameters = named_parameters[:389]    \n","    attention_parameters = named_parameters[391:395]\n","    regressor_parameters = named_parameters[395:]\n","        \n","    attention_group = [params for (name, params) in attention_parameters]\n","    regressor_group = [params for (name, params) in regressor_parameters]\n","\n","    parameters = []\n","    parameters.append({\"params\": attention_group})\n","    parameters.append({\"params\": regressor_group})\n","\n","    for layer_num, (name, params) in enumerate(roberta_parameters):\n","        weight_decay = 0.00 if \"bias\" in name else 0.01#0\n","        lr = 2e-5\n","        #roberta-base: \n","        # if layer_num >= 69: #4/12layers       \n","        #     lr = 5e-5\n","        # if layer_num >= 1f33: #8/12layers\n","        #     lr = 1e-4\n","        #roberta-large\n","        if layer_num >= 133: #8/24layers     \n","            lr = 5e-5\n","        if layer_num >= 261: #16/24layers\n","            lr = 1e-4\n","\n","\n","        parameters.append({\"params\": params,\n","                           \"weight_decay\": weight_decay,\n","                           \"lr\": lr})\n","\n","    return AdamW(parameters)"],"id":"handled-trouble","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9SjNJPllCHBG"},"source":["# SEED = 1000\n","# kfold = KFold(n_splits=NUM_FOLDS, random_state=SEED, shuffle=True)\n","# for fold, (train_indices, val_indices) in enumerate(kfold.split(train_df)):\n","#     print(fold)\n","#     print('------------')\n","#     print(val_indices)"],"id":"9SjNJPllCHBG","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KcrbAtOaJaVS"},"source":["# Debug"],"id":"KcrbAtOaJaVS"},{"cell_type":"code","metadata":{"id":"vhG0dfPfJZpl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626859902087,"user_tz":-540,"elapsed":24743,"user":{"displayName":"小林健史","photoUrl":"","userId":"11137570562930914237"}},"outputId":"5b931516-28b5-4d23-909e-1a7cd16b68c2"},"source":["model = LitModel().to(DEVICE)"],"id":"vhG0dfPfJZpl","execution_count":null,"outputs":[{"output_type":"stream","text":["Some weights of RobertaModel were not initialized from the model checkpoint at /content/drive/MyDrive/CommonLit/input/commonlitreadabilityprize/roberta-large-mlm/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"r9ogBaNfODQ8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626859902087,"user_tz":-540,"elapsed":11,"user":{"displayName":"小林健史","photoUrl":"","userId":"11137570562930914237"}},"outputId":"40d73128-ae77-4c04-f0d7-dceee09721b6"},"source":["named_parameters = list(model.named_parameters())\n","len(named_parameters)"],"id":"r9ogBaNfODQ8","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["397"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"id":"XtGlz9UyNEPt"},"source":["# for name, param in model.named_parameters():\n","#     print(name)"],"id":"XtGlz9UyNEPt","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jtUUBp-MJXoa"},"source":["# Train\n"],"id":"jtUUBp-MJXoa"},{"cell_type":"code","metadata":{"id":"ibNIM2txGvBf"},"source":["# output にフォルダ作成\n","!mkdir /content/drive/MyDrive/CommonLit/output/roberta-large-pretrain-original_linerwarmupsteps220"],"id":"ibNIM2txGvBf","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-07-10T16:35:14.144962Z","iopub.status.busy":"2021-07-10T16:35:14.141009Z","iopub.status.idle":"2021-07-10T17:19:19.916633Z","shell.execute_reply":"2021-07-10T17:19:19.915678Z"},"papermill":{"duration":2645.82007,"end_time":"2021-07-10T17:19:19.916828","exception":false,"start_time":"2021-07-10T16:35:14.096758","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"internal-filename","outputId":"d4959a52-3554-4a1f-8dcb-824727e529a2"},"source":["gc.collect()\n","\n","SEED = 1000\n","list_val_rmse = []\n","\n","PATH = \"/content/drive/MyDrive/CommonLit/output/roberta-large-pretrain-original_linerwarmupsteps220/\"\n","\n","kfold = KFold(n_splits=NUM_FOLDS, random_state=SEED, shuffle=True)\n","\n","for fold, (train_indices, val_indices) in enumerate(kfold.split(train_df)):    \n","    print(f\"\\nFold {fold + 1}/{NUM_FOLDS}\")\n","    model_path = PATH + f\"model_{fold + 1}.pth\"\n","    # model_path = f\"/content/drive/MyDrive/CommonLit/output/roberta-large-pretrain-original/model_{fold + 1}.pth\"\n","        \n","    set_random_seed(SEED + fold)\n","    \n","    train_dataset = LitDataset(train_df.loc[train_indices])    \n","    val_dataset = LitDataset(train_df.loc[val_indices])    \n","        \n","    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n","                              drop_last=True, shuffle=True, num_workers=2)    \n","    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,\n","                            drop_last=False, shuffle=False, num_workers=2)    \n","        \n","    set_random_seed(SEED + fold)    \n","    \n","    model = LitModel().to(DEVICE)\n","    \n","    optimizer = create_optimizer(model)                        \n","    scheduler = get_linear_schedule_with_warmup(#get_cosine_schedule_with_warmup\n","        optimizer,\n","        num_training_steps=NUM_EPOCHS * len(train_loader),\n","        num_warmup_steps=220) #baseline = 50\n","    \n","    list_val_rmse.append(train(model, model_path, train_loader,\n","                               val_loader, optimizer, scheduler=scheduler))\n","\n","    del model\n","    gc.collect()\n","    \n","    print(\"\\nPerformance estimates:\")\n","    print(list_val_rmse)\n","    print(\"Mean:\", np.array(list_val_rmse).mean())\n","    \n","\n","hoge = np.array(list_val_rmse)\n","np.savetxt('/content/drive/MyDrive/CommonLit/output/test.csv',hoge)"],"id":"internal-filename","execution_count":null,"outputs":[{"output_type":"stream","text":["\n","Fold 1/5\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of RobertaModel were not initialized from the model checkpoint at /content/drive/MyDrive/CommonLit/input/commonlitreadabilityprize/roberta-large-mlm/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["\n","16 steps took 12.7 seconds\n","Epoch: 0 batch_num: 16 val_rmse: 0.991\n","New best_val_rmse: 0.991\n","\n","16 steps took 11.8 seconds\n","Epoch: 0 batch_num: 32 val_rmse: 0.7195\n","New best_val_rmse: 0.7195\n","\n","16 steps took 11.8 seconds\n","Epoch: 0 batch_num: 48 val_rmse: 0.7489\n","Still best_val_rmse: 0.7195 (from epoch 0)\n","\n","16 steps took 11.8 seconds\n","Epoch: 0 batch_num: 64 val_rmse: 0.6194\n","New best_val_rmse: 0.6194\n","\n","16 steps took 11.8 seconds\n","Epoch: 0 batch_num: 80 val_rmse: 0.5993\n","New best_val_rmse: 0.5993\n","\n","16 steps took 11.8 seconds\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YQCoNWPvJYAc"},"source":[""],"id":"YQCoNWPvJYAc","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"papermill":{"duration":0.256017,"end_time":"2021-07-10T17:19:20.427150","exception":false,"start_time":"2021-07-10T17:19:20.171133","status":"completed"},"tags":[],"id":"exposed-cornell"},"source":["# Inference"],"id":"exposed-cornell"},{"cell_type":"code","metadata":{"papermill":{"duration":0.254779,"end_time":"2021-07-10T17:19:20.959408","exception":false,"start_time":"2021-07-10T17:19:20.704629","status":"completed"},"tags":[],"id":"pacific-explanation"},"source":[""],"id":"pacific-explanation","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-07-10T17:19:21.485458Z","iopub.status.busy":"2021-07-10T17:19:21.484260Z","iopub.status.idle":"2021-07-10T17:19:21.509269Z","shell.execute_reply":"2021-07-10T17:19:21.508604Z"},"papermill":{"duration":0.29265,"end_time":"2021-07-10T17:19:21.509426","exception":false,"start_time":"2021-07-10T17:19:21.216776","status":"completed"},"tags":[],"id":"speaking-authority"},"source":["test_dataset = LitDataset(test_df, inference_only=True)"],"id":"speaking-authority","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-07-10T17:19:22.019508Z","iopub.status.busy":"2021-07-10T17:19:22.018338Z","iopub.status.idle":"2021-07-10T17:19:51.285549Z","shell.execute_reply":"2021-07-10T17:19:51.284997Z"},"papermill":{"duration":29.52399,"end_time":"2021-07-10T17:19:51.285732","exception":false,"start_time":"2021-07-10T17:19:21.761742","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"destroyed-interval","executionInfo":{"status":"ok","timestamp":1626833556447,"user_tz":-540,"elapsed":156448,"user":{"displayName":"小林健史","photoUrl":"","userId":"11137570562930914237"}},"outputId":"67b944ee-6e6a-43c6-9c5b-1029177076f8"},"source":["all_predictions = np.zeros((len(list_val_rmse), len(test_df)))\n","\n","test_dataset = LitDataset(test_df, inference_only=True)\n","test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n","                         drop_last=False, shuffle=False, num_workers=2)\n","\n","for index in range(len(list_val_rmse)):\n","    model_path = PATH + f\"model_{index + 1}.pth\"\n","    # model_path = f\"/content/drive/MyDrive/CommonLit/output/roberta-large-pretrain-original/model_{index + 1}.pth\"\n","    print(f\"\\nUsing {model_path}\")\n","                        \n","    model = LitModel()\n","    model.load_state_dict(torch.load(model_path))    \n","    model.to(DEVICE)\n","    \n","    all_predictions[index] = predict(model, test_loader)\n","    \n","    del model\n","    gc.collect()"],"id":"destroyed-interval","execution_count":null,"outputs":[{"output_type":"stream","text":["\n","Using /content/drive/MyDrive/CommonLit/output/roberta-large-pretrain-original_linerwarmupsteps250/model_1.pth\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of RobertaModel were not initialized from the model checkpoint at /content/drive/MyDrive/CommonLit/input/commonlitreadabilityprize/roberta-large-mlm/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Using /content/drive/MyDrive/CommonLit/output/roberta-large-pretrain-original_linerwarmupsteps250/model_2.pth\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of RobertaModel were not initialized from the model checkpoint at /content/drive/MyDrive/CommonLit/input/commonlitreadabilityprize/roberta-large-mlm/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Using /content/drive/MyDrive/CommonLit/output/roberta-large-pretrain-original_linerwarmupsteps250/model_3.pth\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of RobertaModel were not initialized from the model checkpoint at /content/drive/MyDrive/CommonLit/input/commonlitreadabilityprize/roberta-large-mlm/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Using /content/drive/MyDrive/CommonLit/output/roberta-large-pretrain-original_linerwarmupsteps250/model_4.pth\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of RobertaModel were not initialized from the model checkpoint at /content/drive/MyDrive/CommonLit/input/commonlitreadabilityprize/roberta-large-mlm/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Using /content/drive/MyDrive/CommonLit/output/roberta-large-pretrain-original_linerwarmupsteps250/model_5.pth\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of RobertaModel were not initialized from the model checkpoint at /content/drive/MyDrive/CommonLit/input/commonlitreadabilityprize/roberta-large-mlm/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"execution":{"iopub.execute_input":"2021-07-10T17:19:51.815506Z","iopub.status.busy":"2021-07-10T17:19:51.814838Z","iopub.status.idle":"2021-07-10T17:19:52.977737Z","shell.execute_reply":"2021-07-10T17:19:52.977091Z"},"papermill":{"duration":1.434043,"end_time":"2021-07-10T17:19:52.977904","exception":false,"start_time":"2021-07-10T17:19:51.543861","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"meaningful-petersburg","executionInfo":{"status":"ok","timestamp":1626807726056,"user_tz":-540,"elapsed":17,"user":{"displayName":"小林健史","photoUrl":"","userId":"11137570562930914237"}},"outputId":"e048bb88-390f-4989-8aa7-33ab5ba0e696"},"source":["predictions = all_predictions.mean(axis=0)\n","submission_df.target = predictions\n","print(submission_df)\n","#submission_df.to_csv(\"submission.csv\", index=False)"],"id":"meaningful-petersburg","execution_count":null,"outputs":[{"output_type":"stream","text":["          id    target\n","0  c0f722661 -0.336575\n","1  f0953f0a5 -0.407154\n","2  0df072751 -0.384847\n","3  04caf4e0c -2.246874\n","4  0e63f8bea -1.797387\n","5  12537fe78 -1.231166\n","6  965e592c0  0.256258\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KS-MlzVLkY1i"},"source":["## Upload model"],"id":"KS-MlzVLkY1i"},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x_28uXt0kYSU","executionInfo":{"status":"ok","timestamp":1626626008996,"user_tz":-540,"elapsed":214622,"user":{"displayName":"N T","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjrn_DiNuakDWBVaNxXi-bgTkRs3Cpibuo1YYo2_g=s64","userId":"09409365885492369369"}},"outputId":"47e9b643-6358-4e52-bf39-6e66f54a2a88"},"source":["!mkdir -p ./output/\n","!cp -f ./model* ./output/\n","#CHANGEME\n","!cp -f ./drive/MyDrive/kaggle/commonlit/pretrained-roberta-base/dataset-metadata.json ./output/dataset-metadata.json\n","!sed -i -e \"s/roberta-base/roberta-large-`TZ=JST-9 date +\"%Y%m%d%H%M%S\"`/\" ./output/dataset-metadata.json\n","!sed -i -e \"s/Roberta-base/Roberta-large-`TZ=JST-9 date +\"%m%d%H%M%S\"`/\" ./output/dataset-metadata.json\n","!kaggle datasets create -p ./output/"],"id":"x_28uXt0kYSU","execution_count":null,"outputs":[{"output_type":"stream","text":["Starting upload for file model_3.pth\n","Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n","100% 1.33G/1.33G [00:26<00:00, 53.2MB/s]\n","Upload successful: model_3.pth (1GB)\n","Starting upload for file model_2.pth\n","100% 1.33G/1.33G [00:25<00:00, 55.1MB/s]\n","Upload successful: model_2.pth (1GB)\n","Starting upload for file model_1.pth\n","100% 1.33G/1.33G [00:24<00:00, 57.5MB/s]\n","Upload successful: model_1.pth (1GB)\n","Starting upload for file model_4.pth\n","100% 1.33G/1.33G [00:27<00:00, 52.2MB/s]\n","Upload successful: model_4.pth (1GB)\n","Starting upload for file model_5.pth\n","100% 1.33G/1.33G [00:25<00:00, 56.0MB/s]\n","Upload successful: model_5.pth (1GB)\n","Your private Dataset is being created. Please check progress at /api/v1/datasets/status//iamnishipy/roberta-large-20210719013114\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iD1Cot2sk7A_","executionInfo":{"status":"ok","timestamp":1626626008997,"user_tz":-540,"elapsed":10,"user":{"displayName":"N T","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjrn_DiNuakDWBVaNxXi-bgTkRs3Cpibuo1YYo2_g=s64","userId":"09409365885492369369"}},"outputId":"6a9e8634-ee92-40ac-e37e-6538c256f151"},"source":["!cat ./output/dataset-metadata.json"],"id":"iD1Cot2sk7A_","execution_count":null,"outputs":[{"output_type":"stream","text":["{\n","  \"licenses\": [\n","    {\n","      \"name\": \"CC0-1.0\"\n","    }\n","  ], \n","  \"id\": \"iamnishipy/roberta-large-20210719013114\", \n","  \"title\": \"Roberta-large-0719013114\"\n","}"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uggb4d7eGsdM"},"source":[""],"id":"uggb4d7eGsdM","execution_count":null,"outputs":[]}]}