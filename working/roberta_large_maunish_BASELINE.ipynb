{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 2702.979398,
      "end_time": "2021-07-10T17:19:56.452391",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2021-07-10T16:34:53.472993",
      "version": "2.3.3"
    },
    "colab": {
      "name": "roberta-large-maunish-BASELINE.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "KS-MlzVLkY1i"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cpptake/CommonLit/blob/main/roberta_large_maunish_BASELINE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrbX1a6A8Doq"
      },
      "source": [
        "# Prerequisite"
      ],
      "id": "UrbX1a6A8Doq"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdDxPgc_8SBn",
        "outputId": "92cf80e2-96e4-4a01-d925-3277c9a06c22"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "YdDxPgc_8SBn",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-21rfg88O1A",
        "outputId": "bb7fa032-5e5e-4f65-a731-66080606ef36"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "id": "Q-21rfg88O1A",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Jul 19 12:56:41 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P0    56W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMaOL7upRq2w"
      },
      "source": [
        "## Install same version of library as Kaggle Notebook"
      ],
      "id": "KMaOL7upRq2w"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBS2Zv7DR2tk"
      },
      "source": [
        "# # cp でrequirementsをカレントdirにコピー\n",
        "# !cp -f /content/drive/MyDrive/kaggle/commonlit/roberta-large/requirements.txt ./\n",
        "# !cat ./requirements.txt"
      ],
      "id": "LBS2Zv7DR2tk",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7y1hmfoS-Q1",
        "outputId": "f5d418b6-a9f6-4be6-8675-a47e69ac508f"
      },
      "source": [
        "!pip uninstall -r /content/drive/MyDrive/CommonLit/working/requirements.txt -y"
      ],
      "id": "k7y1hmfoS-Q1",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling pandas-1.2.3:\n",
            "  Successfully uninstalled pandas-1.2.3\n",
            "Uninstalling sklearn-0.0:\n",
            "  Successfully uninstalled sklearn-0.0\n",
            "Uninstalling sklearn-pandas-2.1.0:\n",
            "  Successfully uninstalled sklearn-pandas-2.1.0\n",
            "Uninstalling torch-1.7.0:\n",
            "  Successfully uninstalled torch-1.7.0\n",
            "Uninstalling torchmetrics-0.2.0:\n",
            "  Successfully uninstalled torchmetrics-0.2.0\n",
            "Uninstalling torchvision-0.8.1:\n",
            "  Successfully uninstalled torchvision-0.8.1\n",
            "Uninstalling transformers-4.5.1:\n",
            "  Successfully uninstalled transformers-4.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 875
        },
        "id": "DhM67RrVQYws",
        "outputId": "6202d263-6bf5-470d-b423-b773cceb58c2"
      },
      "source": [
        "!pip install -r /content/drive/MyDrive/CommonLit/working/requirements.txt"
      ],
      "id": "DhM67RrVQYws",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pandas==1.2.3\n",
            "  Using cached https://files.pythonhosted.org/packages/f3/d4/3fe3b5bf9886912b64ef040040aec356fa48825e5a829a84c2667afdf952/pandas-1.2.3-cp37-cp37m-manylinux1_x86_64.whl\n",
            "Processing /root/.cache/pip/wheels/76/03/bb/589d421d27431bcd2c6da284d5f2286c8e3b2ea3cf1594c074/sklearn-0.0-py2.py3-none-any.whl\n",
            "Collecting sklearn-pandas==2.1.0\n",
            "  Using cached https://files.pythonhosted.org/packages/35/52/bb537702967ad30ed17bcda71c9e163777a696c1a982da421dd2489b4ff7/sklearn_pandas-2.1.0-py2.py3-none-any.whl\n",
            "Collecting torch==1.7.0\n",
            "  Using cached https://files.pythonhosted.org/packages/d9/74/d52c014fbfb50aefc084d2bf5ffaa0a8456f69c586782b59f93ef45e2da9/torch-1.7.0-cp37-cp37m-manylinux1_x86_64.whl\n",
            "Collecting torchmetrics==0.2.0\n",
            "  Using cached https://files.pythonhosted.org/packages/3a/42/d984612cabf005a265aa99c8d4ab2958e37b753aafb12f31c81df38751c8/torchmetrics-0.2.0-py3-none-any.whl\n",
            "Collecting torchvision==0.8.1\n",
            "  Using cached https://files.pythonhosted.org/packages/a3/39/a9caac0deb027feec2cdd7cc40b2a598256d3f50050c80f349c030f915f2/torchvision-0.8.1-cp37-cp37m-manylinux1_x86_64.whl\n",
            "Collecting transformers==4.5.1\n",
            "  Using cached https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from pandas==1.2.3->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 1)) (1.19.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.2.3->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 1)) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.2.3->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 1)) (2.8.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn==0.0->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 2)) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=1.5.1 in /usr/local/lib/python3.7/dist-packages (from sklearn-pandas==2.1.0->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 3)) (1.7.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 4)) (0.6)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 4)) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 4)) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.8.1->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 7)) (7.1.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 8)) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 8)) (0.0.45)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 8)) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 8)) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 8)) (4.6.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 8)) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 8)) (21.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 8)) (0.10.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas==1.2.3->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn==0.0->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 2)) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.1->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 8)) (7.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 8)) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 8)) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 8)) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 8)) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers==4.5.1->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 8)) (3.5.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.5.1->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 8)) (2.4.7)\n",
            "\u001b[31mERROR: torchtext 0.10.0 has requirement torch==1.9.0, but you'll have torch 1.7.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=1.1.0; python_version >= \"3.0\", but you'll have pandas 1.2.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: sklearn-pandas 2.1.0 has requirement scikit-learn>=0.23.0, but you'll have scikit-learn 0.22.2.post1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pandas, sklearn, sklearn-pandas, torch, torchmetrics, torchvision, transformers\n",
            "Successfully installed pandas-1.2.3 sklearn-0.0 sklearn-pandas-2.1.0 torch-1.7.0 torchmetrics-0.2.0 torchvision-0.8.1 transformers-4.5.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pandas"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBMkIBcxUIb2",
        "outputId": "0ef86003-6c69-4796-ba8e-f114ac4f7882"
      },
      "source": [
        "!pip freeze |grep -e random -e math -e numpy -e pandas -e torch -e transformers -e sklearn -e gc"
      ],
      "id": "HBMkIBcxUIb2",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mpmath==1.2.1\n",
            "numpy==1.19.5\n",
            "pandas==1.2.3\n",
            "pandas-datareader==0.9.0\n",
            "pandas-gbq==0.13.3\n",
            "pandas-profiling==1.4.1\n",
            "sklearn==0.0\n",
            "sklearn-pandas==2.1.0\n",
            "tensorflow-gcs-config==2.5.0\n",
            "torch==1.7.0\n",
            "torchmetrics==0.2.0\n",
            "torchsummary==1.5.1\n",
            "torchtext==0.10.0\n",
            "torchvision==0.8.1\n",
            "transformers==4.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "De2yfzg48VPx"
      },
      "source": [
        "## Prepare dataset"
      ],
      "id": "De2yfzg48VPx"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YZk0yJH8mWO"
      },
      "source": [
        "### kaggle.json"
      ],
      "id": "_YZk0yJH8mWO"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEuB9fOD8j8l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f528306-a670-46a5-b54f-5014b3d7749d"
      },
      "source": [
        "# !mkdir -p /root/.kaggle/\n",
        "# !cp ./drive/MyDrive/kaggle/commonlit/kaggle.json ~/.kaggle/kaggle.json\n",
        "# !chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "!pip install -q kaggle\n",
        "!mkdir /root/.kaggle\n",
        "!cp /content/drive/MyDrive/Colab\\ Notebooks/kaggle.json /root/.kaggle/"
      ],
      "id": "DEuB9fOD8j8l",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oKBPd9H8q0L"
      },
      "source": [
        "### Competition dataset"
      ],
      "id": "4oKBPd9H8q0L"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lv4fLHsg8vsO"
      },
      "source": [
        "# !mkdir -p /content/drive/MyDrive/CommonLit/input/commonlitreadabilityprize/\n",
        "# !kaggle competitions download -c commonlitreadabilityprize -p /content/drive/MyDrive/CommonLit/input/commonlitreadabilityprize/\n",
        "# !cp -f ./drive/MyDrive/kaggle/commonlit/train_stratiKfold.csv.zip /content/drive/MyDrive/CommonLit/input/commonlitreadabilityprize/"
      ],
      "id": "lv4fLHsg8vsO",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1QcZXR79EO2"
      },
      "source": [
        "# !unzip -o ../input/commonlitreadabilityprize/train.csv.zip -d ../input/commonlitreadabilityprize/\n",
        "# !unzip -o ../input/commonlitreadabilityprize/train_stratiKfold.csv.zip -d ../input/commonlitreadabilityprize/"
      ],
      "id": "R1QcZXR79EO2",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAJaSyc89GiB"
      },
      "source": [
        "!ls ../input/commonlitreadabilityprize/"
      ],
      "id": "FAJaSyc89GiB",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03O1cJpp9HAO"
      },
      "source": [
        "### Pretrained RoBERTa Large \n",
        "- Pretrain RoBERTa Large in the same way as this notebook\n",
        "  - https://www.kaggle.com/maunish/clrp-pytorch-roberta-pretrain\n",
        "- Dataset:\n",
        "  - https://www.kaggle.com/iamnishipy/roberta-large-20210712191259-mlm"
      ],
      "id": "03O1cJpp9HAO"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DFyAMzl9Rs_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a14aeb5-2bf9-46f9-8ffe-c8affcd4998a"
      },
      "source": [
        "!mkdir -p /content/drive/MyDrive/CommonLit/input/commonlitreadabilityprize/pretrained-model/\n",
        "!kaggle datasets download iamnishipy/roberta-large-20210712191259-mlm"
      ],
      "id": "7DFyAMzl9Rs_",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "401 - Unauthorized\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Izsn9ikg99Bn",
        "outputId": "bef0f96a-988c-4c5e-8549-da5885fb022e"
      },
      "source": [
        "!unzip -o ./roberta-large-20210712191259-mlm.zip -d ../input/commonlitreadabilityprize/pretrained-model/"
      ],
      "id": "Izsn9ikg99Bn",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unzip:  cannot find or open ./roberta-large-20210712191259-mlm.zip, ./roberta-large-20210712191259-mlm.zip.zip or ./roberta-large-20210712191259-mlm.zip.ZIP.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWsjc3K2UJrO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3d529ca-a191-4f95-a36d-c80485968025"
      },
      "source": [
        "!ls ../input/commonlitreadabilityprize/pretrained-model/"
      ],
      "id": "WWsjc3K2UJrO",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ls: cannot access '../input/commonlitreadabilityprize/pretrained-model/': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEGf6ny98zoo"
      },
      "source": [
        ""
      ],
      "id": "AEGf6ny98zoo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.020873,
          "end_time": "2021-07-10T16:35:02.554689",
          "exception": false,
          "start_time": "2021-07-10T16:35:02.533816",
          "status": "completed"
        },
        "tags": [],
        "id": "central-liberia"
      },
      "source": [
        "# Overview\n",
        "This is kernel is almost the same as [Lightweight Roberta solution in PyTorch](https://www.kaggle.com/andretugan/lightweight-roberta-solution-in-pytorch), but instead of \"roberta-base\", it starts from [Maunish's pre-trained model](https://www.kaggle.com/maunish/clrp-roberta-base).\n",
        "\n",
        "Acknowledgments: some ideas were taken from kernels by [Torch](https://www.kaggle.com/rhtsingh) and [Maunish](https://www.kaggle.com/maunish)."
      ],
      "id": "central-liberia"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dsgr4s1G-m73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eca5b7fa-ee33-4409-9a68-2fb7dfd408c2"
      },
      "source": [
        "!pip install transformers accelerate datasets"
      ],
      "id": "Dsgr4s1G-m73",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.5.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.7/dist-packages (0.3.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.9.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from accelerate) (1.7.0)\n",
            "Requirement already satisfied: pyaml>=20.4.0 in /usr/local/lib/python3.7/dist-packages (from accelerate) (20.4.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.2.3)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2021.7.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: huggingface-hub<0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.0.14)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.5.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->accelerate) (0.6)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->accelerate) (0.16.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyaml>=20.4.0->accelerate) (3.13)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2021-07-10T16:35:02.605794Z",
          "iopub.status.busy": "2021-07-10T16:35:02.602341Z",
          "iopub.status.idle": "2021-07-10T16:35:11.998468Z",
          "shell.execute_reply": "2021-07-10T16:35:11.999041Z",
          "shell.execute_reply.started": "2021-07-10T16:33:36.630414Z"
        },
        "papermill": {
          "duration": 9.425549,
          "end_time": "2021-07-10T16:35:11.999387",
          "exception": false,
          "start_time": "2021-07-10T16:35:02.573838",
          "status": "completed"
        },
        "tags": [],
        "id": "classical-garage"
      },
      "source": [
        "import os\n",
        "import math\n",
        "import random\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from transformers import AdamW\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModel\n",
        "from transformers import AutoConfig\n",
        "from transformers import get_cosine_schedule_with_warmup\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "import gc\n",
        "gc.enable()"
      ],
      "id": "classical-garage",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.017537,
          "end_time": "2021-07-10T16:35:12.036899",
          "exception": false,
          "start_time": "2021-07-10T16:35:12.019362",
          "status": "completed"
        },
        "tags": [],
        "id": "challenging-bottle"
      },
      "source": [
        "## Prepare dataset"
      ],
      "id": "challenging-bottle"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-10T16:35:12.966641Z",
          "iopub.status.busy": "2021-07-10T16:35:12.965516Z",
          "iopub.status.idle": "2021-07-10T16:35:12.969362Z",
          "shell.execute_reply": "2021-07-10T16:35:12.968739Z",
          "shell.execute_reply.started": "2021-06-30T12:51:59.456435Z"
        },
        "papermill": {
          "duration": 0.076388,
          "end_time": "2021-07-10T16:35:12.969525",
          "exception": false,
          "start_time": "2021-07-10T16:35:12.893137",
          "status": "completed"
        },
        "tags": [],
        "id": "measured-cornwall"
      },
      "source": [
        "NUM_FOLDS = 5\n",
        "NUM_EPOCHS = 3\n",
        "BATCH_SIZE = 8\n",
        "MAX_LEN = 248\n",
        "EVAL_SCHEDULE = [(0.50, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1., 1)]\n",
        "ROBERTA_PATH = \"/content/drive/MyDrive/CommonLit/input/commonlitreadabilityprize/roberta-large\"\n",
        "TOKENIZER_PATH = \"/content/drive/MyDrive/CommonLit/input/commonlitreadabilityprize/roberta-large\"\n",
        "# ROBERTA_PATH = \"/content/drive/MyDrive/CommonLit/input/commonlitreadabilityprize/pretrained-model/clrp_roberta_large\"\n",
        "# TOKENIZER_PATH = \"/content/drive/MyDrive/CommonLit/input/commonlitreadabilityprize/pretrained-model/clrp_roberta_large\"\n",
        "# ROBERTA_PATH = \"../input/clrp-roberta-base/clrp_roberta_base\"\n",
        "# TOKENIZER_PATH = \"../input/clrp-roberta-base/clrp_roberta_base\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "id": "measured-cornwall",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-10T16:35:13.013652Z",
          "iopub.status.busy": "2021-07-10T16:35:13.012651Z",
          "iopub.status.idle": "2021-07-10T16:35:13.016079Z",
          "shell.execute_reply": "2021-07-10T16:35:13.015529Z",
          "shell.execute_reply.started": "2021-06-30T12:51:59.470504Z"
        },
        "papermill": {
          "duration": 0.028052,
          "end_time": "2021-07-10T16:35:13.016225",
          "exception": false,
          "start_time": "2021-07-10T16:35:12.988173",
          "status": "completed"
        },
        "tags": [],
        "id": "sitting-brook"
      },
      "source": [
        "def set_random_seed(random_seed):\n",
        "    random.seed(random_seed)\n",
        "    np.random.seed(random_seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n",
        "\n",
        "    torch.manual_seed(random_seed)\n",
        "    torch.cuda.manual_seed(random_seed)\n",
        "    torch.cuda.manual_seed_all(random_seed)\n",
        "\n",
        "    torch.backends.cudnn.deterministic = True"
      ],
      "id": "sitting-brook",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-10T16:35:13.059695Z",
          "iopub.status.busy": "2021-07-10T16:35:13.059033Z",
          "iopub.status.idle": "2021-07-10T16:35:13.181860Z",
          "shell.execute_reply": "2021-07-10T16:35:13.181008Z",
          "shell.execute_reply.started": "2021-06-30T12:51:59.485325Z"
        },
        "papermill": {
          "duration": 0.147365,
          "end_time": "2021-07-10T16:35:13.182094",
          "exception": false,
          "start_time": "2021-07-10T16:35:13.034729",
          "status": "completed"
        },
        "tags": [],
        "id": "banner-plastic"
      },
      "source": [
        "train_df = pd.read_csv(\"/content/drive/MyDrive/CommonLit/input/commonlitreadabilityprize/train.csv\")\n",
        "\n",
        "# Remove incomplete entries if any.\n",
        "train_df.drop(train_df[(train_df.target == 0) & (train_df.standard_error == 0)].index,\n",
        "              inplace=True)\n",
        "train_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "test_df = pd.read_csv(\"/content/drive/MyDrive/CommonLit/input/commonlitreadabilityprize/test.csv\")\n",
        "submission_df = pd.read_csv(\"/content/drive/MyDrive/CommonLit/input/commonlitreadabilityprize/sample_submission.csv\")"
      ],
      "id": "banner-plastic",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-10T16:35:13.223654Z",
          "iopub.status.busy": "2021-07-10T16:35:13.222955Z",
          "iopub.status.idle": "2021-07-10T16:35:13.465852Z",
          "shell.execute_reply": "2021-07-10T16:35:13.464700Z",
          "shell.execute_reply.started": "2021-06-30T12:51:59.537207Z"
        },
        "papermill": {
          "duration": 0.265264,
          "end_time": "2021-07-10T16:35:13.466048",
          "exception": false,
          "start_time": "2021-07-10T16:35:13.200784",
          "status": "completed"
        },
        "tags": [],
        "id": "unavailable-philadelphia"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)"
      ],
      "id": "unavailable-philadelphia",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.018281,
          "end_time": "2021-07-10T16:35:13.502997",
          "exception": false,
          "start_time": "2021-07-10T16:35:13.484716",
          "status": "completed"
        },
        "tags": [],
        "id": "intermediate-brand"
      },
      "source": [
        "# Dataset"
      ],
      "id": "intermediate-brand"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-10T16:35:13.549331Z",
          "iopub.status.busy": "2021-07-10T16:35:13.548372Z",
          "iopub.status.idle": "2021-07-10T16:35:13.552476Z",
          "shell.execute_reply": "2021-07-10T16:35:13.551942Z",
          "shell.execute_reply.started": "2021-06-30T12:51:59.797452Z"
        },
        "papermill": {
          "duration": 0.031082,
          "end_time": "2021-07-10T16:35:13.552607",
          "exception": false,
          "start_time": "2021-07-10T16:35:13.521525",
          "status": "completed"
        },
        "tags": [],
        "id": "adopted-prayer"
      },
      "source": [
        "class LitDataset(Dataset):\n",
        "    def __init__(self, df, inference_only=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.df = df        \n",
        "        self.inference_only = inference_only\n",
        "        self.text = df.excerpt.tolist()\n",
        "        #self.text = [text.replace(\"\\n\", \" \") for text in self.text]\n",
        "        \n",
        "        if not self.inference_only:\n",
        "            self.target = torch.tensor(df.target.values, dtype=torch.float32)        \n",
        "    \n",
        "        self.encoded = tokenizer.batch_encode_plus(\n",
        "            self.text,\n",
        "            padding = 'max_length',            \n",
        "            max_length = MAX_LEN,\n",
        "            truncation = True,\n",
        "            return_attention_mask=True\n",
        "        )        \n",
        " \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    \n",
        "    def __getitem__(self, index):        \n",
        "        input_ids = torch.tensor(self.encoded['input_ids'][index])\n",
        "        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n",
        "        \n",
        "        if self.inference_only:\n",
        "            return (input_ids, attention_mask)            \n",
        "        else:\n",
        "            target = self.target[index]\n",
        "            return (input_ids, attention_mask, target)"
      ],
      "id": "adopted-prayer",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.018016,
          "end_time": "2021-07-10T16:35:13.588706",
          "exception": false,
          "start_time": "2021-07-10T16:35:13.570690",
          "status": "completed"
        },
        "tags": [],
        "id": "sonic-cooperative"
      },
      "source": [
        "# Model\n",
        "The model is inspired by the one from [Maunish](https://www.kaggle.com/maunish/clrp-roberta-svm)."
      ],
      "id": "sonic-cooperative"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-10T16:35:13.662281Z",
          "iopub.status.busy": "2021-07-10T16:35:13.661368Z",
          "iopub.status.idle": "2021-07-10T16:35:13.679333Z",
          "shell.execute_reply": "2021-07-10T16:35:13.680266Z",
          "shell.execute_reply.started": "2021-06-30T12:51:59.811644Z"
        },
        "papermill": {
          "duration": 0.063207,
          "end_time": "2021-07-10T16:35:13.680545",
          "exception": false,
          "start_time": "2021-07-10T16:35:13.617338",
          "status": "completed"
        },
        "tags": [],
        "id": "listed-coordinate"
      },
      "source": [
        "class LitModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n",
        "        config.update({\"output_hidden_states\":True, \n",
        "                       \"hidden_dropout_prob\": 0.0,\n",
        "                       \"layer_norm_eps\": 1e-7})                       \n",
        "        \n",
        "        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config)\n",
        "        #https://towardsdatascience.com/attention-based-deep-multiple-instance-learning-1bb3df857e24\n",
        "        # 768: node fully connected layer\n",
        "        # 512: node attention layer\n",
        "        # self.attention = nn.Sequential(            \n",
        "        #     nn.Linear(768, 512),            \n",
        "        #     nn.Tanh(),                       \n",
        "        #     nn.Linear(512, 1),\n",
        "        #     nn.Softmax(dim=1)\n",
        "        # )        \n",
        "\n",
        "        # self.regressor = nn.Sequential(                        \n",
        "        #     nn.Linear(768, 1)                        \n",
        "        # )\n",
        "\n",
        "\n",
        "        #768 -> 1024\n",
        "        #512 -> 768\n",
        "        self.attention = nn.Sequential(            \n",
        "            nn.Linear(1024, 768),            \n",
        "            nn.Tanh(),                       \n",
        "            nn.Linear(768, 1),\n",
        "            nn.Softmax(dim=1)\n",
        "        )        \n",
        "\n",
        "        self.regressor = nn.Sequential(                        \n",
        "            nn.Linear(1024, 1)                        \n",
        "        )\n",
        "        \n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # roberta_output = self.roberta(input_ids=input_ids,\n",
        "        #                               attention_mask=attention_mask)\n",
        "        \n",
        "        roberta_output = self.roberta(input_ids=input_ids,\n",
        "                                      attention_mask=attention_mask)\n",
        "        \n",
        "        last_hidden_state = roberta_output[0]\n",
        "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
        "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
        "        sum_mask = input_mask_expanded.sum(1)\n",
        "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
        "        mean_embeddings = sum_embeddings / sum_mask\n",
        "\n",
        "        # print(mean_embeddings.shape)\n",
        "\n",
        "        logits = self.regressor(mean_embeddings)\n",
        "        \n",
        "        preds = logits.squeeze(-1).squeeze(-1)\n",
        "\n",
        "        return preds\n",
        "\n",
        "        #### nishipy original ####\n",
        "        # # There are a total of 13 layers of hidden states.\n",
        "        # # 1 for the embedding layer, and 12 for the 12 Roberta layers.\n",
        "        # # We take the hidden states from the last Roberta layer.\n",
        "        # last_layer_hidden_states = roberta_output.hidden_states[-1]\n",
        "\n",
        "        # # The number of cells is MAX_LEN.\n",
        "        # # The size of the hidden state of each cell is 768 (for roberta-base).\n",
        "        # # In order to condense hidden states of all cells to a context vector,\n",
        "        # # we compute a weighted average of the hidden states of all cells.\n",
        "        # # We compute the weight of each cell, using the attention neural network.\n",
        "        # weights = self.attention(last_layer_hidden_states)\n",
        "                \n",
        "        # # weights.shape is BATCH_SIZE x MAX_LEN x 1\n",
        "        # # last_layer_hidden_states.shape is BATCH_SIZE x MAX_LEN x 768        \n",
        "        # # Now we compute context_vector as the weighted average.\n",
        "        # # context_vector.shape is BATCH_SIZE x 768\n",
        "        # context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n",
        "        \n",
        "        # # Now we reduce the context vector to the prediction score.\n",
        "        # return self.regressor(context_vector)"
      ],
      "id": "listed-coordinate",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-10T16:35:13.776053Z",
          "iopub.status.busy": "2021-07-10T16:35:13.774791Z",
          "iopub.status.idle": "2021-07-10T16:35:13.782919Z",
          "shell.execute_reply": "2021-07-10T16:35:13.784211Z",
          "shell.execute_reply.started": "2021-06-30T12:51:59.831662Z"
        },
        "papermill": {
          "duration": 0.069033,
          "end_time": "2021-07-10T16:35:13.784378",
          "exception": false,
          "start_time": "2021-07-10T16:35:13.715345",
          "status": "completed"
        },
        "tags": [],
        "id": "marked-citation"
      },
      "source": [
        "def eval_mse(model, data_loader):\n",
        "    \"\"\"Evaluates the mean squared error of the |model| on |data_loader|\"\"\"\n",
        "    model.eval()            \n",
        "    mse_sum = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_num, (input_ids, attention_mask, target) in enumerate(data_loader):\n",
        "            input_ids = input_ids.to(DEVICE)\n",
        "            attention_mask = attention_mask.to(DEVICE)                        \n",
        "            target = target.to(DEVICE)           \n",
        "            \n",
        "            pred = model(input_ids, attention_mask)                       \n",
        "\n",
        "            mse_sum += nn.MSELoss(reduction=\"sum\")(pred.flatten(), target).item()\n",
        "                \n",
        "\n",
        "    return mse_sum / len(data_loader.dataset)"
      ],
      "id": "marked-citation",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-10T16:35:13.860639Z",
          "iopub.status.busy": "2021-07-10T16:35:13.859518Z",
          "iopub.status.idle": "2021-07-10T16:35:13.865246Z",
          "shell.execute_reply": "2021-07-10T16:35:13.867010Z",
          "shell.execute_reply.started": "2021-06-30T12:51:59.844758Z"
        },
        "papermill": {
          "duration": 0.049393,
          "end_time": "2021-07-10T16:35:13.867227",
          "exception": false,
          "start_time": "2021-07-10T16:35:13.817834",
          "status": "completed"
        },
        "tags": [],
        "id": "associate-astrology"
      },
      "source": [
        "def predict(model, data_loader):\n",
        "    \"\"\"Returns an np.array with predictions of the |model| on |data_loader|\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    result = np.zeros(len(data_loader.dataset))    \n",
        "    index = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch_num, (input_ids, attention_mask) in enumerate(data_loader):\n",
        "            input_ids = input_ids.to(DEVICE)\n",
        "            attention_mask = attention_mask.to(DEVICE)\n",
        "                        \n",
        "            pred = model(input_ids, attention_mask)                        \n",
        "\n",
        "            result[index : index + pred.shape[0]] = pred.flatten().to(\"cpu\")\n",
        "            index += pred.shape[0]\n",
        "\n",
        "    return result"
      ],
      "id": "associate-astrology",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-10T16:35:13.957154Z",
          "iopub.status.busy": "2021-07-10T16:35:13.955894Z",
          "iopub.status.idle": "2021-07-10T16:35:13.970258Z",
          "shell.execute_reply": "2021-07-10T16:35:13.971515Z",
          "shell.execute_reply.started": "2021-06-30T12:51:59.863562Z"
        },
        "papermill": {
          "duration": 0.064065,
          "end_time": "2021-07-10T16:35:13.971767",
          "exception": false,
          "start_time": "2021-07-10T16:35:13.907702",
          "status": "completed"
        },
        "tags": [],
        "id": "impressed-minnesota"
      },
      "source": [
        "def train(model, model_path, train_loader, val_loader,\n",
        "          optimizer, scheduler=None, num_epochs=NUM_EPOCHS):    \n",
        "    best_val_rmse = None\n",
        "    best_epoch = 0\n",
        "    step = 0\n",
        "    last_eval_step = 0\n",
        "    eval_period = EVAL_SCHEDULE[0][1]    \n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):                           \n",
        "        val_rmse = None         \n",
        "\n",
        "        for batch_num, (input_ids, attention_mask, target) in enumerate(train_loader):\n",
        "            input_ids = input_ids.to(DEVICE)\n",
        "            attention_mask = attention_mask.to(DEVICE)            \n",
        "            target = target.to(DEVICE)                        \n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            model.train()\n",
        "\n",
        "            pred = model(input_ids, attention_mask)\n",
        "                                                        \n",
        "            mse = nn.MSELoss(reduction=\"mean\")(pred.flatten(), target)\n",
        "                        \n",
        "            mse.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "            if scheduler:\n",
        "                scheduler.step()\n",
        "            \n",
        "            if step >= last_eval_step + eval_period:\n",
        "                # Evaluate the model on val_loader.\n",
        "                elapsed_seconds = time.time() - start\n",
        "                num_steps = step - last_eval_step\n",
        "                print(f\"\\n{num_steps} steps took {elapsed_seconds:0.3} seconds\")\n",
        "                last_eval_step = step\n",
        "                \n",
        "                val_rmse = math.sqrt(eval_mse(model, val_loader))                            \n",
        "\n",
        "                print(f\"Epoch: {epoch} batch_num: {batch_num}\", \n",
        "                      f\"val_rmse: {val_rmse:0.4}\")\n",
        "\n",
        "                for rmse, period in EVAL_SCHEDULE:\n",
        "                    if val_rmse >= rmse:\n",
        "                        eval_period = period\n",
        "                        break                               \n",
        "                \n",
        "                if not best_val_rmse or val_rmse < best_val_rmse:                    \n",
        "                    best_val_rmse = val_rmse\n",
        "                    best_epoch = epoch\n",
        "                    torch.save(model.state_dict(), model_path)\n",
        "                    print(f\"New best_val_rmse: {best_val_rmse:0.4}\")\n",
        "                else:       \n",
        "                    print(f\"Still best_val_rmse: {best_val_rmse:0.4}\",\n",
        "                          f\"(from epoch {best_epoch})\")                                    \n",
        "                    \n",
        "                start = time.time()\n",
        "                                            \n",
        "            step += 1\n",
        "                        \n",
        "    \n",
        "    return best_val_rmse"
      ],
      "id": "impressed-minnesota",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-10T16:35:14.053687Z",
          "iopub.status.busy": "2021-07-10T16:35:14.052560Z",
          "iopub.status.idle": "2021-07-10T16:35:14.060346Z",
          "shell.execute_reply": "2021-07-10T16:35:14.062050Z",
          "shell.execute_reply.started": "2021-06-30T12:51:59.879987Z"
        },
        "papermill": {
          "duration": 0.054887,
          "end_time": "2021-07-10T16:35:14.062272",
          "exception": false,
          "start_time": "2021-07-10T16:35:14.007385",
          "status": "completed"
        },
        "tags": [],
        "id": "handled-trouble"
      },
      "source": [
        "#怪しい\n",
        "def create_optimizer(model):\n",
        "    #model.named_parameters():\n",
        "    #Base -> 205\n",
        "    #Large -> 397\n",
        "    named_parameters = list(model.named_parameters())    \n",
        "\n",
        "    #Base\n",
        "    # roberta_parameters = named_parameters[:197]    \n",
        "    # attention_parameters = named_parameters[199:203]\n",
        "    # regressor_parameters = named_parameters[203:]\n",
        "    \n",
        "    #Large\n",
        "    roberta_parameters = named_parameters[:389]    \n",
        "    attention_parameters = named_parameters[391:395]\n",
        "    regressor_parameters = named_parameters[395:]\n",
        "        \n",
        "    attention_group = [params for (name, params) in attention_parameters]\n",
        "    regressor_group = [params for (name, params) in regressor_parameters]\n",
        "\n",
        "    parameters = []\n",
        "    parameters.append({\"params\": attention_group})\n",
        "    parameters.append({\"params\": regressor_group})\n",
        "\n",
        "    for layer_num, (name, params) in enumerate(roberta_parameters):\n",
        "        weight_decay = 0.0 if \"bias\" in name else 0.01\n",
        "        lr = 2e-5\n",
        "        #roberta-base: \n",
        "        # if layer_num >= 69: #4/12layers       \n",
        "        #     lr = 5e-5\n",
        "        # if layer_num >= 1f33: #8/12layers\n",
        "        #     lr = 1e-4\n",
        "        #roberta-large\n",
        "        if layer_num >= 133: #8/24layers     \n",
        "            lr = 5e-5\n",
        "        if layer_num >= 261: #16/24layers\n",
        "            lr = 1e-4\n",
        "\n",
        "\n",
        "        parameters.append({\"params\": params,\n",
        "                           \"weight_decay\": weight_decay,\n",
        "                           \"lr\": lr})\n",
        "\n",
        "    return AdamW(parameters)"
      ],
      "id": "handled-trouble",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SjNJPllCHBG"
      },
      "source": [
        "# SEED = 1000\n",
        "# kfold = KFold(n_splits=NUM_FOLDS, random_state=SEED, shuffle=True)\n",
        "# for fold, (train_indices, val_indices) in enumerate(kfold.split(train_df)):\n",
        "#     print(fold)\n",
        "#     print('------------')\n",
        "#     print(val_indices)"
      ],
      "id": "9SjNJPllCHBG",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcrbAtOaJaVS"
      },
      "source": [
        "# Debug"
      ],
      "id": "KcrbAtOaJaVS"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhG0dfPfJZpl"
      },
      "source": [
        "model = LitModel().to(DEVICE)"
      ],
      "id": "vhG0dfPfJZpl",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9ogBaNfODQ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a40c719-3832-4594-fc76-c04ba7f042e2"
      },
      "source": [
        "named_parameters = list(model.named_parameters())\n",
        "len(named_parameters)"
      ],
      "id": "r9ogBaNfODQ8",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "397"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtGlz9UyNEPt"
      },
      "source": [
        "# for name, param in model.named_parameters():\n",
        "#     print(name)"
      ],
      "id": "XtGlz9UyNEPt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxgwubuBJcY1"
      },
      "source": [
        ""
      ],
      "id": "MxgwubuBJcY1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMrGZvBFJcOe"
      },
      "source": [
        ""
      ],
      "id": "nMrGZvBFJcOe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtUUBp-MJXoa"
      },
      "source": [
        "# Train\n"
      ],
      "id": "jtUUBp-MJXoa"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-10T16:35:14.144962Z",
          "iopub.status.busy": "2021-07-10T16:35:14.141009Z",
          "iopub.status.idle": "2021-07-10T17:19:19.916633Z",
          "shell.execute_reply": "2021-07-10T17:19:19.915678Z"
        },
        "papermill": {
          "duration": 2645.82007,
          "end_time": "2021-07-10T17:19:19.916828",
          "exception": false,
          "start_time": "2021-07-10T16:35:14.096758",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "internal-filename",
        "outputId": "c622913c-fc8b-4add-c578-a8af6c3f8769"
      },
      "source": [
        "gc.collect()\n",
        "\n",
        "SEED = 1000\n",
        "list_val_rmse = []\n",
        "\n",
        "kfold = KFold(n_splits=NUM_FOLDS, random_state=SEED, shuffle=True)\n",
        "\n",
        "for fold, (train_indices, val_indices) in enumerate(kfold.split(train_df)):    \n",
        "    print(f\"\\nFold {fold + 1}/{NUM_FOLDS}\")\n",
        "    model_path = f\"model_{fold + 1}.pth\"\n",
        "        \n",
        "    set_random_seed(SEED + fold)\n",
        "    \n",
        "    train_dataset = LitDataset(train_df.loc[train_indices])    \n",
        "    val_dataset = LitDataset(train_df.loc[val_indices])    \n",
        "        \n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                              drop_last=True, shuffle=True, num_workers=2)    \n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,\n",
        "                            drop_last=False, shuffle=False, num_workers=2)    \n",
        "        \n",
        "    set_random_seed(SEED + fold)    \n",
        "    \n",
        "    model = LitModel().to(DEVICE)\n",
        "    \n",
        "    optimizer = create_optimizer(model)                        \n",
        "    scheduler = get_cosine_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_training_steps=NUM_EPOCHS * len(train_loader),\n",
        "        num_warmup_steps=50)    \n",
        "    \n",
        "    list_val_rmse.append(train(model, model_path, train_loader,\n",
        "                               val_loader, optimizer, scheduler=scheduler))\n",
        "\n",
        "    del model\n",
        "    gc.collect()\n",
        "    \n",
        "    print(\"\\nPerformance estimates:\")\n",
        "    print(list_val_rmse)\n",
        "    print(\"Mean:\", np.array(list_val_rmse).mean())\n",
        "    "
      ],
      "id": "internal-filename",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Fold 1/5\n",
            "\n",
            "16 steps took 6.91 seconds\n",
            "Epoch: 0 batch_num: 16 val_rmse: 0.9267\n",
            "New best_val_rmse: 0.9267\n",
            "\n",
            "16 steps took 6.38 seconds\n",
            "Epoch: 0 batch_num: 32 val_rmse: 0.7674\n",
            "New best_val_rmse: 0.7674\n",
            "\n",
            "16 steps took 6.44 seconds\n",
            "Epoch: 0 batch_num: 48 val_rmse: 0.7594\n",
            "New best_val_rmse: 0.7594\n",
            "\n",
            "16 steps took 6.43 seconds\n",
            "Epoch: 0 batch_num: 64 val_rmse: 0.6386\n",
            "New best_val_rmse: 0.6386\n",
            "\n",
            "16 steps took 6.39 seconds\n",
            "Epoch: 0 batch_num: 80 val_rmse: 0.7786\n",
            "Still best_val_rmse: 0.6386 (from epoch 0)\n",
            "\n",
            "16 steps took 6.41 seconds\n",
            "Epoch: 0 batch_num: 96 val_rmse: 0.6306\n",
            "New best_val_rmse: 0.6306\n",
            "\n",
            "16 steps took 6.41 seconds\n",
            "Epoch: 0 batch_num: 112 val_rmse: 0.5816\n",
            "New best_val_rmse: 0.5816\n",
            "\n",
            "16 steps took 6.42 seconds\n",
            "Epoch: 0 batch_num: 128 val_rmse: 0.6318\n",
            "Still best_val_rmse: 0.5816 (from epoch 0)\n",
            "\n",
            "16 steps took 6.38 seconds\n",
            "Epoch: 0 batch_num: 144 val_rmse: 0.6653\n",
            "Still best_val_rmse: 0.5816 (from epoch 0)\n",
            "\n",
            "16 steps took 6.45 seconds\n",
            "Epoch: 0 batch_num: 160 val_rmse: 0.8283\n",
            "Still best_val_rmse: 0.5816 (from epoch 0)\n",
            "\n",
            "16 steps took 6.43 seconds\n",
            "Epoch: 0 batch_num: 176 val_rmse: 0.7193\n",
            "Still best_val_rmse: 0.5816 (from epoch 0)\n",
            "\n",
            "16 steps took 6.44 seconds\n",
            "Epoch: 0 batch_num: 192 val_rmse: 0.7484\n",
            "Still best_val_rmse: 0.5816 (from epoch 0)\n",
            "\n",
            "16 steps took 6.45 seconds\n",
            "Epoch: 0 batch_num: 208 val_rmse: 0.9798\n",
            "Still best_val_rmse: 0.5816 (from epoch 0)\n",
            "\n",
            "16 steps took 6.39 seconds\n",
            "Epoch: 0 batch_num: 224 val_rmse: 1.006\n",
            "Still best_val_rmse: 0.5816 (from epoch 0)\n",
            "\n",
            "16 steps took 6.41 seconds\n",
            "Epoch: 0 batch_num: 240 val_rmse: 1.017\n",
            "Still best_val_rmse: 0.5816 (from epoch 0)\n",
            "\n",
            "16 steps took 6.42 seconds\n",
            "Epoch: 0 batch_num: 256 val_rmse: 0.9447\n",
            "Still best_val_rmse: 0.5816 (from epoch 0)\n",
            "\n",
            "16 steps took 6.45 seconds\n",
            "Epoch: 0 batch_num: 272 val_rmse: 0.9051\n",
            "Still best_val_rmse: 0.5816 (from epoch 0)\n",
            "\n",
            "16 steps took 6.65 seconds\n",
            "Epoch: 1 batch_num: 5 val_rmse: 0.9139\n",
            "Still best_val_rmse: 0.5816 (from epoch 0)\n",
            "\n",
            "16 steps took 6.39 seconds\n",
            "Epoch: 1 batch_num: 21 val_rmse: 0.9843\n",
            "Still best_val_rmse: 0.5816 (from epoch 0)\n",
            "\n",
            "16 steps took 6.41 seconds\n",
            "Epoch: 1 batch_num: 37 val_rmse: 1.091\n",
            "Still best_val_rmse: 0.5816 (from epoch 0)\n",
            "\n",
            "16 steps took 6.44 seconds\n",
            "Epoch: 1 batch_num: 53 val_rmse: 0.8277\n",
            "Still best_val_rmse: 0.5816 (from epoch 0)\n",
            "\n",
            "16 steps took 6.44 seconds\n",
            "Epoch: 1 batch_num: 69 val_rmse: 0.7525\n",
            "Still best_val_rmse: 0.5816 (from epoch 0)\n",
            "\n",
            "16 steps took 6.41 seconds\n",
            "Epoch: 1 batch_num: 85 val_rmse: 0.7849\n",
            "Still best_val_rmse: 0.5816 (from epoch 0)\n",
            "\n",
            "16 steps took 6.42 seconds\n",
            "Epoch: 1 batch_num: 101 val_rmse: 0.7085\n",
            "Still best_val_rmse: 0.5816 (from epoch 0)\n",
            "\n",
            "16 steps took 6.47 seconds\n",
            "Epoch: 1 batch_num: 117 val_rmse: 0.7039\n",
            "Still best_val_rmse: 0.5816 (from epoch 0)\n",
            "\n",
            "16 steps took 6.52 seconds\n",
            "Epoch: 1 batch_num: 133 val_rmse: 0.7276\n",
            "Still best_val_rmse: 0.5816 (from epoch 0)\n",
            "\n",
            "16 steps took 6.43 seconds\n",
            "Epoch: 1 batch_num: 149 val_rmse: 0.6533\n",
            "Still best_val_rmse: 0.5816 (from epoch 0)\n",
            "\n",
            "16 steps took 6.43 seconds\n",
            "Epoch: 1 batch_num: 165 val_rmse: 0.6516\n",
            "Still best_val_rmse: 0.5816 (from epoch 0)\n",
            "\n",
            "16 steps took 6.47 seconds\n",
            "Epoch: 1 batch_num: 181 val_rmse: 0.7601\n",
            "Still best_val_rmse: 0.5816 (from epoch 0)\n",
            "\n",
            "16 steps took 6.47 seconds\n",
            "Epoch: 1 batch_num: 197 val_rmse: 0.6324\n",
            "Still best_val_rmse: 0.5816 (from epoch 0)\n",
            "\n",
            "16 steps took 6.42 seconds\n",
            "Epoch: 1 batch_num: 213 val_rmse: 0.6061\n",
            "Still best_val_rmse: 0.5816 (from epoch 0)\n",
            "\n",
            "16 steps took 6.44 seconds\n",
            "Epoch: 1 batch_num: 229 val_rmse: 0.5941\n",
            "Still best_val_rmse: 0.5816 (from epoch 0)\n",
            "\n",
            "16 steps took 6.45 seconds\n",
            "Epoch: 1 batch_num: 245 val_rmse: 0.6309\n",
            "Still best_val_rmse: 0.5816 (from epoch 0)\n",
            "\n",
            "16 steps took 6.49 seconds\n",
            "Epoch: 1 batch_num: 261 val_rmse: 0.6184\n",
            "Still best_val_rmse: 0.5816 (from epoch 0)\n",
            "\n",
            "16 steps took 6.44 seconds\n",
            "Epoch: 1 batch_num: 277 val_rmse: 0.5883\n",
            "Still best_val_rmse: 0.5816 (from epoch 0)\n",
            "\n",
            "16 steps took 6.7 seconds\n",
            "Epoch: 2 batch_num: 10 val_rmse: 0.587\n",
            "Still best_val_rmse: 0.5816 (from epoch 0)\n",
            "\n",
            "16 steps took 6.46 seconds\n",
            "Epoch: 2 batch_num: 26 val_rmse: 0.6085\n",
            "Still best_val_rmse: 0.5816 (from epoch 0)\n",
            "\n",
            "16 steps took 6.43 seconds\n",
            "Epoch: 2 batch_num: 42 val_rmse: 0.5737\n",
            "New best_val_rmse: 0.5737\n",
            "\n",
            "16 steps took 6.44 seconds\n",
            "Epoch: 2 batch_num: 58 val_rmse: 0.6015\n",
            "Still best_val_rmse: 0.5737 (from epoch 2)\n",
            "\n",
            "16 steps took 6.46 seconds\n",
            "Epoch: 2 batch_num: 74 val_rmse: 0.5674\n",
            "New best_val_rmse: 0.5674\n",
            "\n",
            "16 steps took 6.41 seconds\n",
            "Epoch: 2 batch_num: 90 val_rmse: 0.5668\n",
            "New best_val_rmse: 0.5668\n",
            "\n",
            "16 steps took 6.42 seconds\n",
            "Epoch: 2 batch_num: 106 val_rmse: 0.5741\n",
            "Still best_val_rmse: 0.5668 (from epoch 2)\n",
            "\n",
            "16 steps took 6.41 seconds\n",
            "Epoch: 2 batch_num: 122 val_rmse: 0.5682\n",
            "Still best_val_rmse: 0.5668 (from epoch 2)\n",
            "\n",
            "16 steps took 6.44 seconds\n",
            "Epoch: 2 batch_num: 138 val_rmse: 0.5661\n",
            "New best_val_rmse: 0.5661\n",
            "\n",
            "16 steps took 6.43 seconds\n",
            "Epoch: 2 batch_num: 154 val_rmse: 0.5546\n",
            "New best_val_rmse: 0.5546\n",
            "\n",
            "16 steps took 6.39 seconds\n",
            "Epoch: 2 batch_num: 170 val_rmse: 0.5726\n",
            "Still best_val_rmse: 0.5546 (from epoch 2)\n",
            "\n",
            "16 steps took 6.45 seconds\n",
            "Epoch: 2 batch_num: 186 val_rmse: 0.5551\n",
            "Still best_val_rmse: 0.5546 (from epoch 2)\n",
            "\n",
            "16 steps took 6.47 seconds\n",
            "Epoch: 2 batch_num: 202 val_rmse: 0.5897\n",
            "Still best_val_rmse: 0.5546 (from epoch 2)\n",
            "\n",
            "16 steps took 6.41 seconds\n",
            "Epoch: 2 batch_num: 218 val_rmse: 0.5634\n",
            "Still best_val_rmse: 0.5546 (from epoch 2)\n",
            "\n",
            "16 steps took 6.41 seconds\n",
            "Epoch: 2 batch_num: 234 val_rmse: 0.5538\n",
            "New best_val_rmse: 0.5538\n",
            "\n",
            "16 steps took 6.47 seconds\n",
            "Epoch: 2 batch_num: 250 val_rmse: 0.5519\n",
            "New best_val_rmse: 0.5519\n",
            "\n",
            "16 steps took 6.42 seconds\n",
            "Epoch: 2 batch_num: 266 val_rmse: 0.5518\n",
            "New best_val_rmse: 0.5518\n",
            "\n",
            "16 steps took 6.44 seconds\n",
            "Epoch: 2 batch_num: 282 val_rmse: 0.5518\n",
            "New best_val_rmse: 0.5518\n",
            "\n",
            "Performance estimates:\n",
            "[0.5517595301857675]\n",
            "Mean: 0.5517595301857675\n",
            "\n",
            "Fold 2/5\n",
            "\n",
            "16 steps took 6.92 seconds\n",
            "Epoch: 0 batch_num: 16 val_rmse: 1.014\n",
            "New best_val_rmse: 1.014\n",
            "\n",
            "16 steps took 6.41 seconds\n",
            "Epoch: 0 batch_num: 32 val_rmse: 0.7498\n",
            "New best_val_rmse: 0.7498\n",
            "\n",
            "16 steps took 6.46 seconds\n",
            "Epoch: 0 batch_num: 48 val_rmse: 0.7408\n",
            "New best_val_rmse: 0.7408\n",
            "\n",
            "16 steps took 6.36 seconds\n",
            "Epoch: 0 batch_num: 64 val_rmse: 0.6712\n",
            "New best_val_rmse: 0.6712\n",
            "\n",
            "16 steps took 6.37 seconds\n",
            "Epoch: 0 batch_num: 80 val_rmse: 0.9155\n",
            "Still best_val_rmse: 0.6712 (from epoch 0)\n",
            "\n",
            "16 steps took 6.43 seconds\n",
            "Epoch: 0 batch_num: 96 val_rmse: 0.8335\n",
            "Still best_val_rmse: 0.6712 (from epoch 0)\n",
            "\n",
            "16 steps took 6.44 seconds\n",
            "Epoch: 0 batch_num: 112 val_rmse: 0.7117\n",
            "Still best_val_rmse: 0.6712 (from epoch 0)\n",
            "\n",
            "16 steps took 6.43 seconds\n",
            "Epoch: 0 batch_num: 128 val_rmse: 0.5806\n",
            "New best_val_rmse: 0.5806\n",
            "\n",
            "16 steps took 6.4 seconds\n",
            "Epoch: 0 batch_num: 144 val_rmse: 0.5855\n",
            "Still best_val_rmse: 0.5806 (from epoch 0)\n",
            "\n",
            "16 steps took 6.46 seconds\n",
            "Epoch: 0 batch_num: 160 val_rmse: 0.5934\n",
            "Still best_val_rmse: 0.5806 (from epoch 0)\n",
            "\n",
            "16 steps took 6.42 seconds\n",
            "Epoch: 0 batch_num: 176 val_rmse: 0.5684\n",
            "New best_val_rmse: 0.5684\n",
            "\n",
            "16 steps took 6.42 seconds\n",
            "Epoch: 0 batch_num: 192 val_rmse: 0.6822\n",
            "Still best_val_rmse: 0.5684 (from epoch 0)\n",
            "\n",
            "16 steps took 6.38 seconds\n",
            "Epoch: 0 batch_num: 208 val_rmse: 0.566\n",
            "New best_val_rmse: 0.566\n",
            "\n",
            "16 steps took 6.44 seconds\n",
            "Epoch: 0 batch_num: 224 val_rmse: 0.5744\n",
            "Still best_val_rmse: 0.566 (from epoch 0)\n",
            "\n",
            "16 steps took 6.4 seconds\n",
            "Epoch: 0 batch_num: 240 val_rmse: 0.64\n",
            "Still best_val_rmse: 0.566 (from epoch 0)\n",
            "\n",
            "16 steps took 6.41 seconds\n",
            "Epoch: 0 batch_num: 256 val_rmse: 0.5421\n",
            "New best_val_rmse: 0.5421\n",
            "\n",
            "16 steps took 6.42 seconds\n",
            "Epoch: 0 batch_num: 272 val_rmse: 0.5551\n",
            "Still best_val_rmse: 0.5421 (from epoch 0)\n",
            "\n",
            "16 steps took 6.66 seconds\n",
            "Epoch: 1 batch_num: 5 val_rmse: 0.6004\n",
            "Still best_val_rmse: 0.5421 (from epoch 0)\n",
            "\n",
            "16 steps took 6.41 seconds\n",
            "Epoch: 1 batch_num: 21 val_rmse: 0.5526\n",
            "Still best_val_rmse: 0.5421 (from epoch 0)\n",
            "\n",
            "16 steps took 6.45 seconds\n",
            "Epoch: 1 batch_num: 37 val_rmse: 0.537\n",
            "New best_val_rmse: 0.537\n",
            "\n",
            "16 steps took 6.44 seconds\n",
            "Epoch: 1 batch_num: 53 val_rmse: 0.5235\n",
            "New best_val_rmse: 0.5235\n",
            "\n",
            "16 steps took 6.41 seconds\n",
            "Epoch: 1 batch_num: 69 val_rmse: 0.5215\n",
            "New best_val_rmse: 0.5215\n",
            "\n",
            "16 steps took 6.41 seconds\n",
            "Epoch: 1 batch_num: 85 val_rmse: 0.523\n",
            "Still best_val_rmse: 0.5215 (from epoch 1)\n",
            "\n",
            "16 steps took 6.42 seconds\n",
            "Epoch: 1 batch_num: 101 val_rmse: 0.5095\n",
            "New best_val_rmse: 0.5095\n",
            "\n",
            "16 steps took 6.43 seconds\n",
            "Epoch: 1 batch_num: 117 val_rmse: 0.5088\n",
            "New best_val_rmse: 0.5088\n",
            "\n",
            "16 steps took 6.41 seconds\n",
            "Epoch: 1 batch_num: 133 val_rmse: 0.5161\n",
            "Still best_val_rmse: 0.5088 (from epoch 1)\n",
            "\n",
            "16 steps took 6.45 seconds\n",
            "Epoch: 1 batch_num: 149 val_rmse: 0.4938\n",
            "New best_val_rmse: 0.4938\n",
            "\n",
            "8 steps took 3.21 seconds\n",
            "Epoch: 1 batch_num: 157 val_rmse: 0.5123\n",
            "Still best_val_rmse: 0.4938 (from epoch 1)\n",
            "\n",
            "16 steps took 6.4 seconds\n",
            "Epoch: 1 batch_num: 173 val_rmse: 0.4996\n",
            "Still best_val_rmse: 0.4938 (from epoch 1)\n",
            "\n",
            "8 steps took 3.2 seconds\n",
            "Epoch: 1 batch_num: 181 val_rmse: 0.5101\n",
            "Still best_val_rmse: 0.4938 (from epoch 1)\n",
            "\n",
            "16 steps took 6.44 seconds\n",
            "Epoch: 1 batch_num: 197 val_rmse: 0.5413\n",
            "Still best_val_rmse: 0.4938 (from epoch 1)\n",
            "\n",
            "16 steps took 6.46 seconds\n",
            "Epoch: 1 batch_num: 213 val_rmse: 0.6467\n",
            "Still best_val_rmse: 0.4938 (from epoch 1)\n",
            "\n",
            "16 steps took 6.42 seconds\n",
            "Epoch: 1 batch_num: 229 val_rmse: 0.505\n",
            "Still best_val_rmse: 0.4938 (from epoch 1)\n",
            "\n",
            "16 steps took 6.44 seconds\n",
            "Epoch: 1 batch_num: 245 val_rmse: 0.5568\n",
            "Still best_val_rmse: 0.4938 (from epoch 1)\n",
            "\n",
            "16 steps took 6.44 seconds\n",
            "Epoch: 1 batch_num: 261 val_rmse: 0.4812\n",
            "New best_val_rmse: 0.4812\n",
            "\n",
            "4 steps took 1.64 seconds\n",
            "Epoch: 1 batch_num: 265 val_rmse: 0.4875\n",
            "Still best_val_rmse: 0.4812 (from epoch 1)\n",
            "\n",
            "4 steps took 1.62 seconds\n",
            "Epoch: 1 batch_num: 269 val_rmse: 0.4911\n",
            "Still best_val_rmse: 0.4812 (from epoch 1)\n",
            "\n",
            "8 steps took 3.23 seconds\n",
            "Epoch: 1 batch_num: 277 val_rmse: 0.4869\n",
            "Still best_val_rmse: 0.4812 (from epoch 1)\n",
            "\n",
            "4 steps took 1.61 seconds\n",
            "Epoch: 1 batch_num: 281 val_rmse: 0.4853\n",
            "Still best_val_rmse: 0.4812 (from epoch 1)\n",
            "\n",
            "4 steps took 1.81 seconds\n",
            "Epoch: 2 batch_num: 2 val_rmse: 0.497\n",
            "Still best_val_rmse: 0.4812 (from epoch 1)\n",
            "\n",
            "8 steps took 3.22 seconds\n",
            "Epoch: 2 batch_num: 10 val_rmse: 0.4821\n",
            "Still best_val_rmse: 0.4812 (from epoch 1)\n",
            "\n",
            "4 steps took 1.6 seconds\n",
            "Epoch: 2 batch_num: 14 val_rmse: 0.493\n",
            "Still best_val_rmse: 0.4812 (from epoch 1)\n",
            "\n",
            "8 steps took 3.19 seconds\n",
            "Epoch: 2 batch_num: 22 val_rmse: 0.5057\n",
            "Still best_val_rmse: 0.4812 (from epoch 1)\n",
            "\n",
            "16 steps took 6.47 seconds\n",
            "Epoch: 2 batch_num: 38 val_rmse: 0.4859\n",
            "Still best_val_rmse: 0.4812 (from epoch 1)\n",
            "\n",
            "4 steps took 1.62 seconds\n",
            "Epoch: 2 batch_num: 42 val_rmse: 0.5\n",
            "Still best_val_rmse: 0.4812 (from epoch 1)\n",
            "\n",
            "16 steps took 6.4 seconds\n",
            "Epoch: 2 batch_num: 58 val_rmse: 0.4787\n",
            "New best_val_rmse: 0.4787\n",
            "\n",
            "2 steps took 0.826 seconds\n",
            "Epoch: 2 batch_num: 60 val_rmse: 0.4814\n",
            "Still best_val_rmse: 0.4787 (from epoch 2)\n",
            "\n",
            "4 steps took 1.6 seconds\n",
            "Epoch: 2 batch_num: 64 val_rmse: 0.4763\n",
            "New best_val_rmse: 0.4763\n",
            "\n",
            "2 steps took 0.812 seconds\n",
            "Epoch: 2 batch_num: 66 val_rmse: 0.4765\n",
            "Still best_val_rmse: 0.4763 (from epoch 2)\n",
            "\n",
            "2 steps took 0.801 seconds\n",
            "Epoch: 2 batch_num: 68 val_rmse: 0.476\n",
            "New best_val_rmse: 0.476\n",
            "\n",
            "2 steps took 0.82 seconds\n",
            "Epoch: 2 batch_num: 70 val_rmse: 0.4765\n",
            "Still best_val_rmse: 0.476 (from epoch 2)\n",
            "\n",
            "2 steps took 0.799 seconds\n",
            "Epoch: 2 batch_num: 72 val_rmse: 0.4766\n",
            "Still best_val_rmse: 0.476 (from epoch 2)\n",
            "\n",
            "2 steps took 0.799 seconds\n",
            "Epoch: 2 batch_num: 74 val_rmse: 0.4763\n",
            "Still best_val_rmse: 0.476 (from epoch 2)\n",
            "\n",
            "2 steps took 0.798 seconds\n",
            "Epoch: 2 batch_num: 76 val_rmse: 0.4767\n",
            "Still best_val_rmse: 0.476 (from epoch 2)\n",
            "\n",
            "2 steps took 0.801 seconds\n",
            "Epoch: 2 batch_num: 78 val_rmse: 0.476\n",
            "Still best_val_rmse: 0.476 (from epoch 2)\n",
            "\n",
            "2 steps took 0.804 seconds\n",
            "Epoch: 2 batch_num: 80 val_rmse: 0.4764\n",
            "Still best_val_rmse: 0.476 (from epoch 2)\n",
            "\n",
            "2 steps took 0.805 seconds\n",
            "Epoch: 2 batch_num: 82 val_rmse: 0.479\n",
            "Still best_val_rmse: 0.476 (from epoch 2)\n",
            "\n",
            "2 steps took 0.799 seconds\n",
            "Epoch: 2 batch_num: 84 val_rmse: 0.4853\n",
            "Still best_val_rmse: 0.476 (from epoch 2)\n",
            "\n",
            "4 steps took 1.61 seconds\n",
            "Epoch: 2 batch_num: 88 val_rmse: 0.4876\n",
            "Still best_val_rmse: 0.476 (from epoch 2)\n",
            "\n",
            "4 steps took 1.61 seconds\n",
            "Epoch: 2 batch_num: 92 val_rmse: 0.4782\n",
            "Still best_val_rmse: 0.476 (from epoch 2)\n",
            "\n",
            "2 steps took 0.804 seconds\n",
            "Epoch: 2 batch_num: 94 val_rmse: 0.4753\n",
            "New best_val_rmse: 0.4753\n",
            "\n",
            "2 steps took 0.826 seconds\n",
            "Epoch: 2 batch_num: 96 val_rmse: 0.4746\n",
            "New best_val_rmse: 0.4746\n",
            "\n",
            "2 steps took 0.807 seconds\n",
            "Epoch: 2 batch_num: 98 val_rmse: 0.4767\n",
            "Still best_val_rmse: 0.4746 (from epoch 2)\n",
            "\n",
            "2 steps took 0.806 seconds\n",
            "Epoch: 2 batch_num: 100 val_rmse: 0.4774\n",
            "Still best_val_rmse: 0.4746 (from epoch 2)\n",
            "\n",
            "2 steps took 0.815 seconds\n",
            "Epoch: 2 batch_num: 102 val_rmse: 0.4783\n",
            "Still best_val_rmse: 0.4746 (from epoch 2)\n",
            "\n",
            "2 steps took 0.811 seconds\n",
            "Epoch: 2 batch_num: 104 val_rmse: 0.4786\n",
            "Still best_val_rmse: 0.4746 (from epoch 2)\n",
            "\n",
            "2 steps took 0.8 seconds\n",
            "Epoch: 2 batch_num: 106 val_rmse: 0.4789\n",
            "Still best_val_rmse: 0.4746 (from epoch 2)\n",
            "\n",
            "2 steps took 0.804 seconds\n",
            "Epoch: 2 batch_num: 108 val_rmse: 0.477\n",
            "Still best_val_rmse: 0.4746 (from epoch 2)\n",
            "\n",
            "2 steps took 0.802 seconds\n",
            "Epoch: 2 batch_num: 110 val_rmse: 0.4773\n",
            "Still best_val_rmse: 0.4746 (from epoch 2)\n",
            "\n",
            "2 steps took 0.807 seconds\n",
            "Epoch: 2 batch_num: 112 val_rmse: 0.4769\n",
            "Still best_val_rmse: 0.4746 (from epoch 2)\n",
            "\n",
            "2 steps took 0.806 seconds\n",
            "Epoch: 2 batch_num: 114 val_rmse: 0.4762\n",
            "Still best_val_rmse: 0.4746 (from epoch 2)\n",
            "\n",
            "2 steps took 0.815 seconds\n",
            "Epoch: 2 batch_num: 116 val_rmse: 0.4747\n",
            "Still best_val_rmse: 0.4746 (from epoch 2)\n",
            "\n",
            "2 steps took 0.807 seconds\n",
            "Epoch: 2 batch_num: 118 val_rmse: 0.4737\n",
            "New best_val_rmse: 0.4737\n",
            "\n",
            "2 steps took 0.828 seconds\n",
            "Epoch: 2 batch_num: 120 val_rmse: 0.4735\n",
            "New best_val_rmse: 0.4735\n",
            "\n",
            "2 steps took 0.8 seconds\n",
            "Epoch: 2 batch_num: 122 val_rmse: 0.4737\n",
            "Still best_val_rmse: 0.4735 (from epoch 2)\n",
            "\n",
            "2 steps took 0.805 seconds\n",
            "Epoch: 2 batch_num: 124 val_rmse: 0.4748\n",
            "Still best_val_rmse: 0.4735 (from epoch 2)\n",
            "\n",
            "2 steps took 0.809 seconds\n",
            "Epoch: 2 batch_num: 126 val_rmse: 0.476\n",
            "Still best_val_rmse: 0.4735 (from epoch 2)\n",
            "\n",
            "2 steps took 0.807 seconds\n",
            "Epoch: 2 batch_num: 128 val_rmse: 0.4784\n",
            "Still best_val_rmse: 0.4735 (from epoch 2)\n",
            "\n",
            "2 steps took 0.809 seconds\n",
            "Epoch: 2 batch_num: 130 val_rmse: 0.4797\n",
            "Still best_val_rmse: 0.4735 (from epoch 2)\n",
            "\n",
            "2 steps took 0.802 seconds\n",
            "Epoch: 2 batch_num: 132 val_rmse: 0.4796\n",
            "Still best_val_rmse: 0.4735 (from epoch 2)\n",
            "\n",
            "2 steps took 0.806 seconds\n",
            "Epoch: 2 batch_num: 134 val_rmse: 0.4766\n",
            "Still best_val_rmse: 0.4735 (from epoch 2)\n",
            "\n",
            "2 steps took 0.804 seconds\n",
            "Epoch: 2 batch_num: 136 val_rmse: 0.4742\n",
            "Still best_val_rmse: 0.4735 (from epoch 2)\n",
            "\n",
            "2 steps took 0.802 seconds\n",
            "Epoch: 2 batch_num: 138 val_rmse: 0.4734\n",
            "New best_val_rmse: 0.4734\n",
            "\n",
            "2 steps took 0.822 seconds\n",
            "Epoch: 2 batch_num: 140 val_rmse: 0.4731\n",
            "New best_val_rmse: 0.4731\n",
            "\n",
            "2 steps took 0.813 seconds\n",
            "Epoch: 2 batch_num: 142 val_rmse: 0.4736\n",
            "Still best_val_rmse: 0.4731 (from epoch 2)\n",
            "\n",
            "2 steps took 0.8 seconds\n",
            "Epoch: 2 batch_num: 144 val_rmse: 0.4732\n",
            "Still best_val_rmse: 0.4731 (from epoch 2)\n",
            "\n",
            "2 steps took 0.802 seconds\n",
            "Epoch: 2 batch_num: 146 val_rmse: 0.4728\n",
            "New best_val_rmse: 0.4728\n",
            "\n",
            "2 steps took 0.819 seconds\n",
            "Epoch: 2 batch_num: 148 val_rmse: 0.4733\n",
            "Still best_val_rmse: 0.4728 (from epoch 2)\n",
            "\n",
            "2 steps took 0.81 seconds\n",
            "Epoch: 2 batch_num: 150 val_rmse: 0.4746\n",
            "Still best_val_rmse: 0.4728 (from epoch 2)\n",
            "\n",
            "2 steps took 0.805 seconds\n",
            "Epoch: 2 batch_num: 152 val_rmse: 0.4754\n",
            "Still best_val_rmse: 0.4728 (from epoch 2)\n",
            "\n",
            "2 steps took 0.803 seconds\n",
            "Epoch: 2 batch_num: 154 val_rmse: 0.4765\n",
            "Still best_val_rmse: 0.4728 (from epoch 2)\n",
            "\n",
            "2 steps took 0.801 seconds\n",
            "Epoch: 2 batch_num: 156 val_rmse: 0.4796\n",
            "Still best_val_rmse: 0.4728 (from epoch 2)\n",
            "\n",
            "2 steps took 0.809 seconds\n",
            "Epoch: 2 batch_num: 158 val_rmse: 0.484\n",
            "Still best_val_rmse: 0.4728 (from epoch 2)\n",
            "\n",
            "4 steps took 1.6 seconds\n",
            "Epoch: 2 batch_num: 162 val_rmse: 0.4886\n",
            "Still best_val_rmse: 0.4728 (from epoch 2)\n",
            "\n",
            "4 steps took 1.61 seconds\n",
            "Epoch: 2 batch_num: 166 val_rmse: 0.4825\n",
            "Still best_val_rmse: 0.4728 (from epoch 2)\n",
            "\n",
            "4 steps took 1.62 seconds\n",
            "Epoch: 2 batch_num: 170 val_rmse: 0.4759\n",
            "Still best_val_rmse: 0.4728 (from epoch 2)\n",
            "\n",
            "2 steps took 0.811 seconds\n",
            "Epoch: 2 batch_num: 172 val_rmse: 0.4736\n",
            "Still best_val_rmse: 0.4728 (from epoch 2)\n",
            "\n",
            "2 steps took 0.801 seconds\n",
            "Epoch: 2 batch_num: 174 val_rmse: 0.4727\n",
            "New best_val_rmse: 0.4727\n",
            "\n",
            "2 steps took 0.829 seconds\n",
            "Epoch: 2 batch_num: 176 val_rmse: 0.4724\n",
            "New best_val_rmse: 0.4724\n",
            "\n",
            "2 steps took 0.808 seconds\n",
            "Epoch: 2 batch_num: 178 val_rmse: 0.4724\n",
            "Still best_val_rmse: 0.4724 (from epoch 2)\n",
            "\n",
            "2 steps took 0.802 seconds\n",
            "Epoch: 2 batch_num: 180 val_rmse: 0.4728\n",
            "Still best_val_rmse: 0.4724 (from epoch 2)\n",
            "\n",
            "2 steps took 0.803 seconds\n",
            "Epoch: 2 batch_num: 182 val_rmse: 0.4734\n",
            "Still best_val_rmse: 0.4724 (from epoch 2)\n",
            "\n",
            "2 steps took 0.805 seconds\n",
            "Epoch: 2 batch_num: 184 val_rmse: 0.4742\n",
            "Still best_val_rmse: 0.4724 (from epoch 2)\n",
            "\n",
            "2 steps took 0.799 seconds\n",
            "Epoch: 2 batch_num: 186 val_rmse: 0.4748\n",
            "Still best_val_rmse: 0.4724 (from epoch 2)\n",
            "\n",
            "2 steps took 0.809 seconds\n",
            "Epoch: 2 batch_num: 188 val_rmse: 0.4752\n",
            "Still best_val_rmse: 0.4724 (from epoch 2)\n",
            "\n",
            "2 steps took 0.809 seconds\n",
            "Epoch: 2 batch_num: 190 val_rmse: 0.4752\n",
            "Still best_val_rmse: 0.4724 (from epoch 2)\n",
            "\n",
            "2 steps took 0.804 seconds\n",
            "Epoch: 2 batch_num: 192 val_rmse: 0.4754\n",
            "Still best_val_rmse: 0.4724 (from epoch 2)\n",
            "\n",
            "2 steps took 0.799 seconds\n",
            "Epoch: 2 batch_num: 194 val_rmse: 0.475\n",
            "Still best_val_rmse: 0.4724 (from epoch 2)\n",
            "\n",
            "2 steps took 0.799 seconds\n",
            "Epoch: 2 batch_num: 196 val_rmse: 0.4746\n",
            "Still best_val_rmse: 0.4724 (from epoch 2)\n",
            "\n",
            "2 steps took 0.801 seconds\n",
            "Epoch: 2 batch_num: 198 val_rmse: 0.4741\n",
            "Still best_val_rmse: 0.4724 (from epoch 2)\n",
            "\n",
            "2 steps took 0.804 seconds\n",
            "Epoch: 2 batch_num: 200 val_rmse: 0.474\n",
            "Still best_val_rmse: 0.4724 (from epoch 2)\n",
            "\n",
            "2 steps took 0.809 seconds\n",
            "Epoch: 2 batch_num: 202 val_rmse: 0.4736\n",
            "Still best_val_rmse: 0.4724 (from epoch 2)\n",
            "\n",
            "2 steps took 0.811 seconds\n",
            "Epoch: 2 batch_num: 204 val_rmse: 0.4732\n",
            "Still best_val_rmse: 0.4724 (from epoch 2)\n",
            "\n",
            "2 steps took 0.807 seconds\n",
            "Epoch: 2 batch_num: 206 val_rmse: 0.473\n",
            "Still best_val_rmse: 0.4724 (from epoch 2)\n",
            "\n",
            "2 steps took 0.801 seconds\n",
            "Epoch: 2 batch_num: 208 val_rmse: 0.4729\n",
            "Still best_val_rmse: 0.4724 (from epoch 2)\n",
            "\n",
            "2 steps took 0.805 seconds\n",
            "Epoch: 2 batch_num: 210 val_rmse: 0.4727\n",
            "Still best_val_rmse: 0.4724 (from epoch 2)\n",
            "\n",
            "2 steps took 0.804 seconds\n",
            "Epoch: 2 batch_num: 212 val_rmse: 0.4727\n",
            "Still best_val_rmse: 0.4724 (from epoch 2)\n",
            "\n",
            "2 steps took 0.81 seconds\n",
            "Epoch: 2 batch_num: 214 val_rmse: 0.4728\n",
            "Still best_val_rmse: 0.4724 (from epoch 2)\n",
            "\n",
            "2 steps took 0.809 seconds\n",
            "Epoch: 2 batch_num: 216 val_rmse: 0.473\n",
            "Still best_val_rmse: 0.4724 (from epoch 2)\n",
            "\n",
            "2 steps took 0.813 seconds\n",
            "Epoch: 2 batch_num: 218 val_rmse: 0.473\n",
            "Still best_val_rmse: 0.4724 (from epoch 2)\n",
            "\n",
            "2 steps took 0.805 seconds\n",
            "Epoch: 2 batch_num: 220 val_rmse: 0.473\n",
            "Still best_val_rmse: 0.4724 (from epoch 2)\n",
            "\n",
            "2 steps took 0.804 seconds\n",
            "Epoch: 2 batch_num: 222 val_rmse: 0.4729\n",
            "Still best_val_rmse: 0.4724 (from epoch 2)\n",
            "\n",
            "2 steps took 0.809 seconds\n",
            "Epoch: 2 batch_num: 224 val_rmse: 0.473\n",
            "Still best_val_rmse: 0.4724 (from epoch 2)\n",
            "\n",
            "2 steps took 0.802 seconds\n",
            "Epoch: 2 batch_num: 226 val_rmse: 0.473\n",
            "Still best_val_rmse: 0.4724 (from epoch 2)\n",
            "\n",
            "2 steps took 0.812 seconds\n",
            "Epoch: 2 batch_num: 228 val_rmse: 0.4731\n",
            "Still best_val_rmse: 0.4724 (from epoch 2)\n",
            "\n",
            "2 steps took 0.808 seconds\n",
            "Epoch: 2 batch_num: 230 val_rmse: 0.4731\n",
            "Still best_val_rmse: 0.4724 (from epoch 2)\n",
            "\n",
            "2 steps took 0.814 seconds\n",
            "Epoch: 2 batch_num: 232 val_rmse: 0.4732\n",
            "Still best_val_rmse: 0.4724 (from epoch 2)\n",
            "\n",
            "2 steps took 0.804 seconds\n",
            "Epoch: 2 batch_num: 234 val_rmse: 0.4733\n",
            "Still best_val_rmse: 0.4724 (from epoch 2)\n",
            "\n",
            "2 steps took 0.808 seconds\n",
            "Epoch: 2 batch_num: 236 val_rmse: 0.4735\n",
            "Still best_val_rmse: 0.4724 (from epoch 2)\n",
            "\n",
            "2 steps took 0.805 seconds\n",
            "Epoch: 2 batch_num: 238 val_rmse: 0.4735\n",
            "Still best_val_rmse: 0.4724 (from epoch 2)\n",
            "\n",
            "2 steps took 0.803 seconds\n",
            "Epoch: 2 batch_num: 240 val_rmse: 0.4736\n",
            "Still best_val_rmse: 0.4724 (from epoch 2)\n",
            "\n",
            "2 steps took 0.804 seconds\n",
            "Epoch: 2 batch_num: 242 val_rmse: 0.4738\n",
            "Still best_val_rmse: 0.4724 (from epoch 2)\n",
            "\n",
            "2 steps took 0.802 seconds\n",
            "Epoch: 2 batch_num: 244 val_rmse: 0.4739\n",
            "Still best_val_rmse: 0.4724 (from epoch 2)\n",
            "\n",
            "2 steps took 0.802 seconds\n",
            "Epoch: 2 batch_num: 246 val_rmse: 0.4738\n",
            "Still best_val_rmse: 0.4724 (from epoch 2)\n",
            "\n",
            "2 steps took 0.808 seconds\n",
            "Epoch: 2 batch_num: 248 val_rmse: 0.4738\n",
            "Still best_val_rmse: 0.4724 (from epoch 2)\n",
            "\n",
            "2 steps took 0.815 seconds\n",
            "Epoch: 2 batch_num: 250 val_rmse: 0.4738\n",
            "Still best_val_rmse: 0.4724 (from epoch 2)\n",
            "\n",
            "2 steps took 0.815 seconds\n",
            "Epoch: 2 batch_num: 252 val_rmse: 0.4738\n",
            "Still best_val_rmse: 0.4724 (from epoch 2)\n",
            "\n",
            "2 steps took 0.804 seconds\n",
            "Epoch: 2 batch_num: 254 val_rmse: 0.4738\n",
            "Still best_val_rmse: 0.4724 (from epoch 2)\n",
            "\n",
            "2 steps took 0.801 seconds\n",
            "Epoch: 2 batch_num: 256 val_rmse: 0.4739\n",
            "Still best_val_rmse: 0.4724 (from epoch 2)\n",
            "\n",
            "2 steps took 0.803 seconds\n",
            "Epoch: 2 batch_num: 258 val_rmse: 0.4738\n",
            "Still best_val_rmse: 0.4724 (from epoch 2)\n",
            "\n",
            "2 steps took 0.809 seconds\n",
            "Epoch: 2 batch_num: 260 val_rmse: 0.4738\n",
            "Still best_val_rmse: 0.4724 (from epoch 2)\n",
            "\n",
            "2 steps took 0.81 seconds\n",
            "Epoch: 2 batch_num: 262 val_rmse: 0.4737\n",
            "Still best_val_rmse: 0.4724 (from epoch 2)\n",
            "\n",
            "2 steps took 0.803 seconds\n",
            "Epoch: 2 batch_num: 264 val_rmse: 0.4737\n",
            "Still best_val_rmse: 0.4724 (from epoch 2)\n",
            "\n",
            "2 steps took 0.805 seconds\n",
            "Epoch: 2 batch_num: 266 val_rmse: 0.4736\n",
            "Still best_val_rmse: 0.4724 (from epoch 2)\n",
            "\n",
            "2 steps took 0.801 seconds\n",
            "Epoch: 2 batch_num: 268 val_rmse: 0.4736\n",
            "Still best_val_rmse: 0.4724 (from epoch 2)\n",
            "\n",
            "2 steps took 0.8 seconds\n",
            "Epoch: 2 batch_num: 270 val_rmse: 0.4736\n",
            "Still best_val_rmse: 0.4724 (from epoch 2)\n",
            "\n",
            "2 steps took 0.805 seconds\n",
            "Epoch: 2 batch_num: 272 val_rmse: 0.4736\n",
            "Still best_val_rmse: 0.4724 (from epoch 2)\n",
            "\n",
            "2 steps took 0.801 seconds\n",
            "Epoch: 2 batch_num: 274 val_rmse: 0.4736\n",
            "Still best_val_rmse: 0.4724 (from epoch 2)\n",
            "\n",
            "2 steps took 0.809 seconds\n",
            "Epoch: 2 batch_num: 276 val_rmse: 0.4736\n",
            "Still best_val_rmse: 0.4724 (from epoch 2)\n",
            "\n",
            "2 steps took 0.811 seconds\n",
            "Epoch: 2 batch_num: 278 val_rmse: 0.4736\n",
            "Still best_val_rmse: 0.4724 (from epoch 2)\n",
            "\n",
            "2 steps took 0.806 seconds\n",
            "Epoch: 2 batch_num: 280 val_rmse: 0.4736\n",
            "Still best_val_rmse: 0.4724 (from epoch 2)\n",
            "\n",
            "2 steps took 0.802 seconds\n",
            "Epoch: 2 batch_num: 282 val_rmse: 0.4736\n",
            "Still best_val_rmse: 0.4724 (from epoch 2)\n",
            "\n",
            "Performance estimates:\n",
            "[0.5517595301857675, 0.47238553319350035]\n",
            "Mean: 0.512072531689634\n",
            "\n",
            "Fold 3/5\n",
            "\n",
            "16 steps took 6.93 seconds\n",
            "Epoch: 0 batch_num: 16 val_rmse: 1.018\n",
            "New best_val_rmse: 1.018\n",
            "\n",
            "16 steps took 6.41 seconds\n",
            "Epoch: 0 batch_num: 32 val_rmse: 0.7603\n",
            "New best_val_rmse: 0.7603\n",
            "\n",
            "16 steps took 6.35 seconds\n",
            "Epoch: 0 batch_num: 48 val_rmse: 0.835\n",
            "Still best_val_rmse: 0.7603 (from epoch 0)\n",
            "\n",
            "16 steps took 6.38 seconds\n",
            "Epoch: 0 batch_num: 64 val_rmse: 0.97\n",
            "Still best_val_rmse: 0.7603 (from epoch 0)\n",
            "\n",
            "16 steps took 6.39 seconds\n",
            "Epoch: 0 batch_num: 80 val_rmse: 0.686\n",
            "New best_val_rmse: 0.686\n",
            "\n",
            "16 steps took 6.44 seconds\n",
            "Epoch: 0 batch_num: 96 val_rmse: 0.9913\n",
            "Still best_val_rmse: 0.686 (from epoch 0)\n",
            "\n",
            "16 steps took 6.39 seconds\n",
            "Epoch: 0 batch_num: 112 val_rmse: 1.072\n",
            "Still best_val_rmse: 0.686 (from epoch 0)\n",
            "\n",
            "16 steps took 6.45 seconds\n",
            "Epoch: 0 batch_num: 128 val_rmse: 1.105\n",
            "Still best_val_rmse: 0.686 (from epoch 0)\n",
            "\n",
            "16 steps took 6.39 seconds\n",
            "Epoch: 0 batch_num: 144 val_rmse: 1.091\n",
            "Still best_val_rmse: 0.686 (from epoch 0)\n",
            "\n",
            "16 steps took 6.39 seconds\n",
            "Epoch: 0 batch_num: 160 val_rmse: 1.088\n",
            "Still best_val_rmse: 0.686 (from epoch 0)\n",
            "\n",
            "16 steps took 6.38 seconds\n",
            "Epoch: 0 batch_num: 176 val_rmse: 1.09\n",
            "Still best_val_rmse: 0.686 (from epoch 0)\n",
            "\n",
            "16 steps took 6.44 seconds\n",
            "Epoch: 0 batch_num: 192 val_rmse: 1.087\n",
            "Still best_val_rmse: 0.686 (from epoch 0)\n",
            "\n",
            "16 steps took 6.41 seconds\n",
            "Epoch: 0 batch_num: 208 val_rmse: 1.088\n",
            "Still best_val_rmse: 0.686 (from epoch 0)\n",
            "\n",
            "16 steps took 6.4 seconds\n",
            "Epoch: 0 batch_num: 224 val_rmse: 1.088\n",
            "Still best_val_rmse: 0.686 (from epoch 0)\n",
            "\n",
            "16 steps took 6.4 seconds\n",
            "Epoch: 0 batch_num: 240 val_rmse: 1.11\n",
            "Still best_val_rmse: 0.686 (from epoch 0)\n",
            "\n",
            "16 steps took 6.45 seconds\n",
            "Epoch: 0 batch_num: 256 val_rmse: 1.087\n",
            "Still best_val_rmse: 0.686 (from epoch 0)\n",
            "\n",
            "16 steps took 6.43 seconds\n",
            "Epoch: 0 batch_num: 272 val_rmse: 1.091\n",
            "Still best_val_rmse: 0.686 (from epoch 0)\n",
            "\n",
            "16 steps took 6.65 seconds\n",
            "Epoch: 1 batch_num: 5 val_rmse: 1.092\n",
            "Still best_val_rmse: 0.686 (from epoch 0)\n",
            "\n",
            "16 steps took 6.41 seconds\n",
            "Epoch: 1 batch_num: 21 val_rmse: 1.088\n",
            "Still best_val_rmse: 0.686 (from epoch 0)\n",
            "\n",
            "16 steps took 6.41 seconds\n",
            "Epoch: 1 batch_num: 37 val_rmse: 1.096\n",
            "Still best_val_rmse: 0.686 (from epoch 0)\n",
            "\n",
            "16 steps took 6.45 seconds\n",
            "Epoch: 1 batch_num: 53 val_rmse: 1.099\n",
            "Still best_val_rmse: 0.686 (from epoch 0)\n",
            "\n",
            "16 steps took 6.39 seconds\n",
            "Epoch: 1 batch_num: 69 val_rmse: 1.088\n",
            "Still best_val_rmse: 0.686 (from epoch 0)\n",
            "\n",
            "16 steps took 6.41 seconds\n",
            "Epoch: 1 batch_num: 85 val_rmse: 1.089\n",
            "Still best_val_rmse: 0.686 (from epoch 0)\n",
            "\n",
            "16 steps took 6.4 seconds\n",
            "Epoch: 1 batch_num: 101 val_rmse: 1.087\n",
            "Still best_val_rmse: 0.686 (from epoch 0)\n",
            "\n",
            "16 steps took 6.46 seconds\n",
            "Epoch: 1 batch_num: 117 val_rmse: 1.087\n",
            "Still best_val_rmse: 0.686 (from epoch 0)\n",
            "\n",
            "16 steps took 6.43 seconds\n",
            "Epoch: 1 batch_num: 133 val_rmse: 1.087\n",
            "Still best_val_rmse: 0.686 (from epoch 0)\n",
            "\n",
            "16 steps took 6.41 seconds\n",
            "Epoch: 1 batch_num: 149 val_rmse: 1.091\n",
            "Still best_val_rmse: 0.686 (from epoch 0)\n",
            "\n",
            "16 steps took 6.4 seconds\n",
            "Epoch: 1 batch_num: 165 val_rmse: 1.092\n",
            "Still best_val_rmse: 0.686 (from epoch 0)\n",
            "\n",
            "16 steps took 6.43 seconds\n",
            "Epoch: 1 batch_num: 181 val_rmse: 1.087\n",
            "Still best_val_rmse: 0.686 (from epoch 0)\n",
            "\n",
            "16 steps took 6.44 seconds\n",
            "Epoch: 1 batch_num: 197 val_rmse: 1.087\n",
            "Still best_val_rmse: 0.686 (from epoch 0)\n",
            "\n",
            "16 steps took 6.46 seconds\n",
            "Epoch: 1 batch_num: 213 val_rmse: 1.087\n",
            "Still best_val_rmse: 0.686 (from epoch 0)\n",
            "\n",
            "16 steps took 6.45 seconds\n",
            "Epoch: 1 batch_num: 229 val_rmse: 1.087\n",
            "Still best_val_rmse: 0.686 (from epoch 0)\n",
            "\n",
            "16 steps took 6.39 seconds\n",
            "Epoch: 1 batch_num: 245 val_rmse: 1.087\n",
            "Still best_val_rmse: 0.686 (from epoch 0)\n",
            "\n",
            "16 steps took 6.39 seconds\n",
            "Epoch: 1 batch_num: 261 val_rmse: 1.087\n",
            "Still best_val_rmse: 0.686 (from epoch 0)\n",
            "\n",
            "16 steps took 6.41 seconds\n",
            "Epoch: 1 batch_num: 277 val_rmse: 1.088\n",
            "Still best_val_rmse: 0.686 (from epoch 0)\n",
            "\n",
            "16 steps took 6.68 seconds\n",
            "Epoch: 2 batch_num: 10 val_rmse: 1.087\n",
            "Still best_val_rmse: 0.686 (from epoch 0)\n",
            "\n",
            "16 steps took 6.4 seconds\n",
            "Epoch: 2 batch_num: 26 val_rmse: 1.087\n",
            "Still best_val_rmse: 0.686 (from epoch 0)\n",
            "\n",
            "16 steps took 6.39 seconds\n",
            "Epoch: 2 batch_num: 42 val_rmse: 1.088\n",
            "Still best_val_rmse: 0.686 (from epoch 0)\n",
            "\n",
            "16 steps took 6.39 seconds\n",
            "Epoch: 2 batch_num: 58 val_rmse: 1.087\n",
            "Still best_val_rmse: 0.686 (from epoch 0)\n",
            "\n",
            "16 steps took 6.44 seconds\n",
            "Epoch: 2 batch_num: 74 val_rmse: 1.087\n",
            "Still best_val_rmse: 0.686 (from epoch 0)\n",
            "\n",
            "16 steps took 6.42 seconds\n",
            "Epoch: 2 batch_num: 90 val_rmse: 1.087\n",
            "Still best_val_rmse: 0.686 (from epoch 0)\n",
            "\n",
            "16 steps took 6.4 seconds\n",
            "Epoch: 2 batch_num: 106 val_rmse: 1.087\n",
            "Still best_val_rmse: 0.686 (from epoch 0)\n",
            "\n",
            "16 steps took 6.4 seconds\n",
            "Epoch: 2 batch_num: 122 val_rmse: 1.087\n",
            "Still best_val_rmse: 0.686 (from epoch 0)\n",
            "\n",
            "16 steps took 6.47 seconds\n",
            "Epoch: 2 batch_num: 138 val_rmse: 1.087\n",
            "Still best_val_rmse: 0.686 (from epoch 0)\n",
            "\n",
            "16 steps took 6.43 seconds\n",
            "Epoch: 2 batch_num: 154 val_rmse: 1.087\n",
            "Still best_val_rmse: 0.686 (from epoch 0)\n",
            "\n",
            "16 steps took 6.42 seconds\n",
            "Epoch: 2 batch_num: 170 val_rmse: 1.087\n",
            "Still best_val_rmse: 0.686 (from epoch 0)\n",
            "\n",
            "16 steps took 6.42 seconds\n",
            "Epoch: 2 batch_num: 186 val_rmse: 1.087\n",
            "Still best_val_rmse: 0.686 (from epoch 0)\n",
            "\n",
            "16 steps took 6.44 seconds\n",
            "Epoch: 2 batch_num: 202 val_rmse: 1.087\n",
            "Still best_val_rmse: 0.686 (from epoch 0)\n",
            "\n",
            "16 steps took 6.44 seconds\n",
            "Epoch: 2 batch_num: 218 val_rmse: 1.087\n",
            "Still best_val_rmse: 0.686 (from epoch 0)\n",
            "\n",
            "16 steps took 6.43 seconds\n",
            "Epoch: 2 batch_num: 234 val_rmse: 1.087\n",
            "Still best_val_rmse: 0.686 (from epoch 0)\n",
            "\n",
            "16 steps took 6.44 seconds\n",
            "Epoch: 2 batch_num: 250 val_rmse: 1.087\n",
            "Still best_val_rmse: 0.686 (from epoch 0)\n",
            "\n",
            "16 steps took 6.41 seconds\n",
            "Epoch: 2 batch_num: 266 val_rmse: 1.087\n",
            "Still best_val_rmse: 0.686 (from epoch 0)\n",
            "\n",
            "16 steps took 6.43 seconds\n",
            "Epoch: 2 batch_num: 282 val_rmse: 1.087\n",
            "Still best_val_rmse: 0.686 (from epoch 0)\n",
            "\n",
            "Performance estimates:\n",
            "[0.5517595301857675, 0.47238553319350035, 0.68600796339148]\n",
            "Mean: 0.5700510089235826\n",
            "\n",
            "Fold 4/5\n",
            "\n",
            "16 steps took 7.0 seconds\n",
            "Epoch: 0 batch_num: 16 val_rmse: 0.9571\n",
            "New best_val_rmse: 0.9571\n",
            "\n",
            "16 steps took 6.42 seconds\n",
            "Epoch: 0 batch_num: 32 val_rmse: 0.939\n",
            "New best_val_rmse: 0.939\n",
            "\n",
            "16 steps took 6.41 seconds\n",
            "Epoch: 0 batch_num: 48 val_rmse: 0.9854\n",
            "Still best_val_rmse: 0.939 (from epoch 0)\n",
            "\n",
            "16 steps took 6.41 seconds\n",
            "Epoch: 0 batch_num: 64 val_rmse: 1.11\n",
            "Still best_val_rmse: 0.939 (from epoch 0)\n",
            "\n",
            "16 steps took 6.43 seconds\n",
            "Epoch: 0 batch_num: 80 val_rmse: 1.022\n",
            "Still best_val_rmse: 0.939 (from epoch 0)\n",
            "\n",
            "16 steps took 6.41 seconds\n",
            "Epoch: 0 batch_num: 96 val_rmse: 0.9246\n",
            "New best_val_rmse: 0.9246\n",
            "\n",
            "16 steps took 6.41 seconds\n",
            "Epoch: 0 batch_num: 112 val_rmse: 1.068\n",
            "Still best_val_rmse: 0.9246 (from epoch 0)\n",
            "\n",
            "16 steps took 6.38 seconds\n",
            "Epoch: 0 batch_num: 128 val_rmse: 1.023\n",
            "Still best_val_rmse: 0.9246 (from epoch 0)\n",
            "\n",
            "16 steps took 6.43 seconds\n",
            "Epoch: 0 batch_num: 144 val_rmse: 1.026\n",
            "Still best_val_rmse: 0.9246 (from epoch 0)\n",
            "\n",
            "16 steps took 6.41 seconds\n",
            "Epoch: 0 batch_num: 160 val_rmse: 1.023\n",
            "Still best_val_rmse: 0.9246 (from epoch 0)\n",
            "\n",
            "16 steps took 6.4 seconds\n",
            "Epoch: 0 batch_num: 176 val_rmse: 1.062\n",
            "Still best_val_rmse: 0.9246 (from epoch 0)\n",
            "\n",
            "16 steps took 6.41 seconds\n",
            "Epoch: 0 batch_num: 192 val_rmse: 1.021\n",
            "Still best_val_rmse: 0.9246 (from epoch 0)\n",
            "\n",
            "16 steps took 6.45 seconds\n",
            "Epoch: 0 batch_num: 208 val_rmse: 1.044\n",
            "Still best_val_rmse: 0.9246 (from epoch 0)\n",
            "\n",
            "16 steps took 6.44 seconds\n",
            "Epoch: 0 batch_num: 224 val_rmse: 1.03\n",
            "Still best_val_rmse: 0.9246 (from epoch 0)\n",
            "\n",
            "16 steps took 6.43 seconds\n",
            "Epoch: 0 batch_num: 240 val_rmse: 1.037\n",
            "Still best_val_rmse: 0.9246 (from epoch 0)\n",
            "\n",
            "16 steps took 6.41 seconds\n",
            "Epoch: 0 batch_num: 256 val_rmse: 1.026\n",
            "Still best_val_rmse: 0.9246 (from epoch 0)\n",
            "\n",
            "16 steps took 6.39 seconds\n",
            "Epoch: 0 batch_num: 272 val_rmse: 1.035\n",
            "Still best_val_rmse: 0.9246 (from epoch 0)\n",
            "\n",
            "16 steps took 6.59 seconds\n",
            "Epoch: 1 batch_num: 5 val_rmse: 1.022\n",
            "Still best_val_rmse: 0.9246 (from epoch 0)\n",
            "\n",
            "16 steps took 6.41 seconds\n",
            "Epoch: 1 batch_num: 21 val_rmse: 1.057\n",
            "Still best_val_rmse: 0.9246 (from epoch 0)\n",
            "\n",
            "16 steps took 6.43 seconds\n",
            "Epoch: 1 batch_num: 37 val_rmse: 1.022\n",
            "Still best_val_rmse: 0.9246 (from epoch 0)\n",
            "\n",
            "16 steps took 6.4 seconds\n",
            "Epoch: 1 batch_num: 53 val_rmse: 1.022\n",
            "Still best_val_rmse: 0.9246 (from epoch 0)\n",
            "\n",
            "16 steps took 6.39 seconds\n",
            "Epoch: 1 batch_num: 69 val_rmse: 1.037\n",
            "Still best_val_rmse: 0.9246 (from epoch 0)\n",
            "\n",
            "16 steps took 6.39 seconds\n",
            "Epoch: 1 batch_num: 85 val_rmse: 1.065\n",
            "Still best_val_rmse: 0.9246 (from epoch 0)\n",
            "\n",
            "16 steps took 6.45 seconds\n",
            "Epoch: 1 batch_num: 101 val_rmse: 1.047\n",
            "Still best_val_rmse: 0.9246 (from epoch 0)\n",
            "\n",
            "16 steps took 6.42 seconds\n",
            "Epoch: 1 batch_num: 117 val_rmse: 1.031\n",
            "Still best_val_rmse: 0.9246 (from epoch 0)\n",
            "\n",
            "16 steps took 6.39 seconds\n",
            "Epoch: 1 batch_num: 133 val_rmse: 1.021\n",
            "Still best_val_rmse: 0.9246 (from epoch 0)\n",
            "\n",
            "16 steps took 6.4 seconds\n",
            "Epoch: 1 batch_num: 149 val_rmse: 1.022\n",
            "Still best_val_rmse: 0.9246 (from epoch 0)\n",
            "\n",
            "16 steps took 6.44 seconds\n",
            "Epoch: 1 batch_num: 165 val_rmse: 1.025\n",
            "Still best_val_rmse: 0.9246 (from epoch 0)\n",
            "\n",
            "16 steps took 6.47 seconds\n",
            "Epoch: 1 batch_num: 181 val_rmse: 1.04\n",
            "Still best_val_rmse: 0.9246 (from epoch 0)\n",
            "\n",
            "16 steps took 6.39 seconds\n",
            "Epoch: 1 batch_num: 197 val_rmse: 1.027\n",
            "Still best_val_rmse: 0.9246 (from epoch 0)\n",
            "\n",
            "16 steps took 6.43 seconds\n",
            "Epoch: 1 batch_num: 213 val_rmse: 1.029\n",
            "Still best_val_rmse: 0.9246 (from epoch 0)\n",
            "\n",
            "16 steps took 6.39 seconds\n",
            "Epoch: 1 batch_num: 229 val_rmse: 1.045\n",
            "Still best_val_rmse: 0.9246 (from epoch 0)\n",
            "\n",
            "16 steps took 6.44 seconds\n",
            "Epoch: 1 batch_num: 245 val_rmse: 1.021\n",
            "Still best_val_rmse: 0.9246 (from epoch 0)\n",
            "\n",
            "16 steps took 6.41 seconds\n",
            "Epoch: 1 batch_num: 261 val_rmse: 1.021\n",
            "Still best_val_rmse: 0.9246 (from epoch 0)\n",
            "\n",
            "16 steps took 6.4 seconds\n",
            "Epoch: 1 batch_num: 277 val_rmse: 1.021\n",
            "Still best_val_rmse: 0.9246 (from epoch 0)\n",
            "\n",
            "16 steps took 6.6 seconds\n",
            "Epoch: 2 batch_num: 10 val_rmse: 1.022\n",
            "Still best_val_rmse: 0.9246 (from epoch 0)\n",
            "\n",
            "16 steps took 6.44 seconds\n",
            "Epoch: 2 batch_num: 26 val_rmse: 1.021\n",
            "Still best_val_rmse: 0.9246 (from epoch 0)\n",
            "\n",
            "16 steps took 6.4 seconds\n",
            "Epoch: 2 batch_num: 42 val_rmse: 1.021\n",
            "Still best_val_rmse: 0.9246 (from epoch 0)\n",
            "\n",
            "16 steps took 6.46 seconds\n",
            "Epoch: 2 batch_num: 58 val_rmse: 1.021\n",
            "Still best_val_rmse: 0.9246 (from epoch 0)\n",
            "\n",
            "16 steps took 6.46 seconds\n",
            "Epoch: 2 batch_num: 74 val_rmse: 1.022\n",
            "Still best_val_rmse: 0.9246 (from epoch 0)\n",
            "\n",
            "16 steps took 6.4 seconds\n",
            "Epoch: 2 batch_num: 90 val_rmse: 1.027\n",
            "Still best_val_rmse: 0.9246 (from epoch 0)\n",
            "\n",
            "16 steps took 6.42 seconds\n",
            "Epoch: 2 batch_num: 106 val_rmse: 1.022\n",
            "Still best_val_rmse: 0.9246 (from epoch 0)\n",
            "\n",
            "16 steps took 6.41 seconds\n",
            "Epoch: 2 batch_num: 122 val_rmse: 1.021\n",
            "Still best_val_rmse: 0.9246 (from epoch 0)\n",
            "\n",
            "16 steps took 6.44 seconds\n",
            "Epoch: 2 batch_num: 138 val_rmse: 1.024\n",
            "Still best_val_rmse: 0.9246 (from epoch 0)\n",
            "\n",
            "16 steps took 6.4 seconds\n",
            "Epoch: 2 batch_num: 154 val_rmse: 1.023\n",
            "Still best_val_rmse: 0.9246 (from epoch 0)\n",
            "\n",
            "16 steps took 6.42 seconds\n",
            "Epoch: 2 batch_num: 170 val_rmse: 1.024\n",
            "Still best_val_rmse: 0.9246 (from epoch 0)\n",
            "\n",
            "16 steps took 6.4 seconds\n",
            "Epoch: 2 batch_num: 186 val_rmse: 1.023\n",
            "Still best_val_rmse: 0.9246 (from epoch 0)\n",
            "\n",
            "16 steps took 6.43 seconds\n",
            "Epoch: 2 batch_num: 202 val_rmse: 1.023\n",
            "Still best_val_rmse: 0.9246 (from epoch 0)\n",
            "\n",
            "16 steps took 6.38 seconds\n",
            "Epoch: 2 batch_num: 218 val_rmse: 1.024\n",
            "Still best_val_rmse: 0.9246 (from epoch 0)\n",
            "\n",
            "16 steps took 6.4 seconds\n",
            "Epoch: 2 batch_num: 234 val_rmse: 1.024\n",
            "Still best_val_rmse: 0.9246 (from epoch 0)\n",
            "\n",
            "16 steps took 6.39 seconds\n",
            "Epoch: 2 batch_num: 250 val_rmse: 1.025\n",
            "Still best_val_rmse: 0.9246 (from epoch 0)\n",
            "\n",
            "16 steps took 6.45 seconds\n",
            "Epoch: 2 batch_num: 266 val_rmse: 1.024\n",
            "Still best_val_rmse: 0.9246 (from epoch 0)\n",
            "\n",
            "16 steps took 6.43 seconds\n",
            "Epoch: 2 batch_num: 282 val_rmse: 1.024\n",
            "Still best_val_rmse: 0.9246 (from epoch 0)\n",
            "\n",
            "Performance estimates:\n",
            "[0.5517595301857675, 0.47238553319350035, 0.68600796339148, 0.9246178666765156]\n",
            "Mean: 0.6586927233618158\n",
            "\n",
            "Fold 5/5\n",
            "\n",
            "16 steps took 6.94 seconds\n",
            "Epoch: 0 batch_num: 16 val_rmse: 0.951\n",
            "New best_val_rmse: 0.951\n",
            "\n",
            "16 steps took 6.43 seconds\n",
            "Epoch: 0 batch_num: 32 val_rmse: 0.7051\n",
            "New best_val_rmse: 0.7051\n",
            "\n",
            "16 steps took 6.43 seconds\n",
            "Epoch: 0 batch_num: 48 val_rmse: 0.7545\n",
            "Still best_val_rmse: 0.7051 (from epoch 0)\n",
            "\n",
            "16 steps took 6.44 seconds\n",
            "Epoch: 0 batch_num: 64 val_rmse: 1.132\n",
            "Still best_val_rmse: 0.7051 (from epoch 0)\n",
            "\n",
            "16 steps took 6.44 seconds\n",
            "Epoch: 0 batch_num: 80 val_rmse: 0.7214\n",
            "Still best_val_rmse: 0.7051 (from epoch 0)\n",
            "\n",
            "16 steps took 6.38 seconds\n",
            "Epoch: 0 batch_num: 96 val_rmse: 0.7017\n",
            "New best_val_rmse: 0.7017\n",
            "\n",
            "16 steps took 6.4 seconds\n",
            "Epoch: 0 batch_num: 112 val_rmse: 0.7499\n",
            "Still best_val_rmse: 0.7017 (from epoch 0)\n",
            "\n",
            "16 steps took 6.41 seconds\n",
            "Epoch: 0 batch_num: 128 val_rmse: 0.7177\n",
            "Still best_val_rmse: 0.7017 (from epoch 0)\n",
            "\n",
            "16 steps took 6.45 seconds\n",
            "Epoch: 0 batch_num: 144 val_rmse: 0.6287\n",
            "New best_val_rmse: 0.6287\n",
            "\n",
            "16 steps took 6.41 seconds\n",
            "Epoch: 0 batch_num: 160 val_rmse: 0.7194\n",
            "Still best_val_rmse: 0.6287 (from epoch 0)\n",
            "\n",
            "16 steps took 6.39 seconds\n",
            "Epoch: 0 batch_num: 176 val_rmse: 0.5883\n",
            "New best_val_rmse: 0.5883\n",
            "\n",
            "16 steps took 6.44 seconds\n",
            "Epoch: 0 batch_num: 192 val_rmse: 0.5739\n",
            "New best_val_rmse: 0.5739\n",
            "\n",
            "16 steps took 6.38 seconds\n",
            "Epoch: 0 batch_num: 208 val_rmse: 0.6986\n",
            "Still best_val_rmse: 0.5739 (from epoch 0)\n",
            "\n",
            "16 steps took 6.39 seconds\n",
            "Epoch: 0 batch_num: 224 val_rmse: 0.6579\n",
            "Still best_val_rmse: 0.5739 (from epoch 0)\n",
            "\n",
            "16 steps took 6.39 seconds\n",
            "Epoch: 0 batch_num: 240 val_rmse: 0.576\n",
            "Still best_val_rmse: 0.5739 (from epoch 0)\n",
            "\n",
            "16 steps took 6.46 seconds\n",
            "Epoch: 0 batch_num: 256 val_rmse: 0.5781\n",
            "Still best_val_rmse: 0.5739 (from epoch 0)\n",
            "\n",
            "16 steps took 6.45 seconds\n",
            "Epoch: 0 batch_num: 272 val_rmse: 0.579\n",
            "Still best_val_rmse: 0.5739 (from epoch 0)\n",
            "\n",
            "16 steps took 6.64 seconds\n",
            "Epoch: 1 batch_num: 5 val_rmse: 0.5845\n",
            "Still best_val_rmse: 0.5739 (from epoch 0)\n",
            "\n",
            "16 steps took 6.4 seconds\n",
            "Epoch: 1 batch_num: 21 val_rmse: 0.699\n",
            "Still best_val_rmse: 0.5739 (from epoch 0)\n",
            "\n",
            "16 steps took 6.47 seconds\n",
            "Epoch: 1 batch_num: 37 val_rmse: 0.6007\n",
            "Still best_val_rmse: 0.5739 (from epoch 0)\n",
            "\n",
            "16 steps took 6.45 seconds\n",
            "Epoch: 1 batch_num: 53 val_rmse: 0.5997\n",
            "Still best_val_rmse: 0.5739 (from epoch 0)\n",
            "\n",
            "16 steps took 6.4 seconds\n",
            "Epoch: 1 batch_num: 69 val_rmse: 1.016\n",
            "Still best_val_rmse: 0.5739 (from epoch 0)\n",
            "\n",
            "16 steps took 6.39 seconds\n",
            "Epoch: 1 batch_num: 85 val_rmse: 0.5558\n",
            "New best_val_rmse: 0.5558\n",
            "\n",
            "16 steps took 6.46 seconds\n",
            "Epoch: 1 batch_num: 101 val_rmse: 0.5368\n",
            "New best_val_rmse: 0.5368\n",
            "\n",
            "16 steps took 6.41 seconds\n",
            "Epoch: 1 batch_num: 117 val_rmse: 0.6951\n",
            "Still best_val_rmse: 0.5368 (from epoch 1)\n",
            "\n",
            "16 steps took 6.44 seconds\n",
            "Epoch: 1 batch_num: 133 val_rmse: 0.5067\n",
            "New best_val_rmse: 0.5067\n",
            "\n",
            "16 steps took 6.41 seconds\n",
            "Epoch: 1 batch_num: 149 val_rmse: 0.6582\n",
            "Still best_val_rmse: 0.5067 (from epoch 1)\n",
            "\n",
            "16 steps took 6.39 seconds\n",
            "Epoch: 1 batch_num: 165 val_rmse: 0.6094\n",
            "Still best_val_rmse: 0.5067 (from epoch 1)\n",
            "\n",
            "16 steps took 6.38 seconds\n",
            "Epoch: 1 batch_num: 181 val_rmse: 0.5278\n",
            "Still best_val_rmse: 0.5067 (from epoch 1)\n",
            "\n",
            "16 steps took 6.44 seconds\n",
            "Epoch: 1 batch_num: 197 val_rmse: 0.61\n",
            "Still best_val_rmse: 0.5067 (from epoch 1)\n",
            "\n",
            "16 steps took 6.41 seconds\n",
            "Epoch: 1 batch_num: 213 val_rmse: 0.5089\n",
            "Still best_val_rmse: 0.5067 (from epoch 1)\n",
            "\n",
            "16 steps took 6.4 seconds\n",
            "Epoch: 1 batch_num: 229 val_rmse: 0.505\n",
            "New best_val_rmse: 0.505\n",
            "\n",
            "16 steps took 6.45 seconds\n",
            "Epoch: 1 batch_num: 245 val_rmse: 0.5017\n",
            "New best_val_rmse: 0.5017\n",
            "\n",
            "16 steps took 6.43 seconds\n",
            "Epoch: 1 batch_num: 261 val_rmse: 0.5328\n",
            "Still best_val_rmse: 0.5017 (from epoch 1)\n",
            "\n",
            "16 steps took 6.41 seconds\n",
            "Epoch: 1 batch_num: 277 val_rmse: 0.5064\n",
            "Still best_val_rmse: 0.5017 (from epoch 1)\n",
            "\n",
            "16 steps took 6.65 seconds\n",
            "Epoch: 2 batch_num: 10 val_rmse: 0.4958\n",
            "New best_val_rmse: 0.4958\n",
            "\n",
            "8 steps took 3.21 seconds\n",
            "Epoch: 2 batch_num: 18 val_rmse: 0.5254\n",
            "Still best_val_rmse: 0.4958 (from epoch 2)\n",
            "\n",
            "16 steps took 6.44 seconds\n",
            "Epoch: 2 batch_num: 34 val_rmse: 0.5519\n",
            "Still best_val_rmse: 0.4958 (from epoch 2)\n",
            "\n",
            "16 steps took 6.4 seconds\n",
            "Epoch: 2 batch_num: 50 val_rmse: 0.4911\n",
            "New best_val_rmse: 0.4911\n",
            "\n",
            "8 steps took 3.21 seconds\n",
            "Epoch: 2 batch_num: 58 val_rmse: 0.4939\n",
            "Still best_val_rmse: 0.4911 (from epoch 2)\n",
            "\n",
            "8 steps took 3.2 seconds\n",
            "Epoch: 2 batch_num: 66 val_rmse: 0.4887\n",
            "New best_val_rmse: 0.4887\n",
            "\n",
            "4 steps took 1.62 seconds\n",
            "Epoch: 2 batch_num: 70 val_rmse: 0.4876\n",
            "New best_val_rmse: 0.4876\n",
            "\n",
            "4 steps took 1.6 seconds\n",
            "Epoch: 2 batch_num: 74 val_rmse: 0.4874\n",
            "New best_val_rmse: 0.4874\n",
            "\n",
            "4 steps took 1.63 seconds\n",
            "Epoch: 2 batch_num: 78 val_rmse: 0.4883\n",
            "Still best_val_rmse: 0.4874 (from epoch 2)\n",
            "\n",
            "4 steps took 1.61 seconds\n",
            "Epoch: 2 batch_num: 82 val_rmse: 0.5029\n",
            "Still best_val_rmse: 0.4874 (from epoch 2)\n",
            "\n",
            "16 steps took 6.4 seconds\n",
            "Epoch: 2 batch_num: 98 val_rmse: 0.4894\n",
            "Still best_val_rmse: 0.4874 (from epoch 2)\n",
            "\n",
            "4 steps took 1.6 seconds\n",
            "Epoch: 2 batch_num: 102 val_rmse: 0.4925\n",
            "Still best_val_rmse: 0.4874 (from epoch 2)\n",
            "\n",
            "8 steps took 3.2 seconds\n",
            "Epoch: 2 batch_num: 110 val_rmse: 0.4858\n",
            "New best_val_rmse: 0.4858\n",
            "\n",
            "4 steps took 1.64 seconds\n",
            "Epoch: 2 batch_num: 114 val_rmse: 0.4851\n",
            "New best_val_rmse: 0.4851\n",
            "\n",
            "4 steps took 1.62 seconds\n",
            "Epoch: 2 batch_num: 118 val_rmse: 0.4868\n",
            "Still best_val_rmse: 0.4851 (from epoch 2)\n",
            "\n",
            "4 steps took 1.6 seconds\n",
            "Epoch: 2 batch_num: 122 val_rmse: 0.4941\n",
            "Still best_val_rmse: 0.4851 (from epoch 2)\n",
            "\n",
            "8 steps took 3.19 seconds\n",
            "Epoch: 2 batch_num: 130 val_rmse: 0.5023\n",
            "Still best_val_rmse: 0.4851 (from epoch 2)\n",
            "\n",
            "16 steps took 6.39 seconds\n",
            "Epoch: 2 batch_num: 146 val_rmse: 0.4916\n",
            "Still best_val_rmse: 0.4851 (from epoch 2)\n",
            "\n",
            "8 steps took 3.21 seconds\n",
            "Epoch: 2 batch_num: 154 val_rmse: 0.4925\n",
            "Still best_val_rmse: 0.4851 (from epoch 2)\n",
            "\n",
            "8 steps took 3.21 seconds\n",
            "Epoch: 2 batch_num: 162 val_rmse: 0.5005\n",
            "Still best_val_rmse: 0.4851 (from epoch 2)\n",
            "\n",
            "16 steps took 6.41 seconds\n",
            "Epoch: 2 batch_num: 178 val_rmse: 0.4858\n",
            "Still best_val_rmse: 0.4851 (from epoch 2)\n",
            "\n",
            "4 steps took 1.6 seconds\n",
            "Epoch: 2 batch_num: 182 val_rmse: 0.4828\n",
            "New best_val_rmse: 0.4828\n",
            "\n",
            "4 steps took 1.62 seconds\n",
            "Epoch: 2 batch_num: 186 val_rmse: 0.4825\n",
            "New best_val_rmse: 0.4825\n",
            "\n",
            "4 steps took 1.61 seconds\n",
            "Epoch: 2 batch_num: 190 val_rmse: 0.484\n",
            "Still best_val_rmse: 0.4825 (from epoch 2)\n",
            "\n",
            "4 steps took 1.62 seconds\n",
            "Epoch: 2 batch_num: 194 val_rmse: 0.4862\n",
            "Still best_val_rmse: 0.4825 (from epoch 2)\n",
            "\n",
            "4 steps took 1.61 seconds\n",
            "Epoch: 2 batch_num: 198 val_rmse: 0.4889\n",
            "Still best_val_rmse: 0.4825 (from epoch 2)\n",
            "\n",
            "4 steps took 1.61 seconds\n",
            "Epoch: 2 batch_num: 202 val_rmse: 0.4906\n",
            "Still best_val_rmse: 0.4825 (from epoch 2)\n",
            "\n",
            "8 steps took 3.21 seconds\n",
            "Epoch: 2 batch_num: 210 val_rmse: 0.4885\n",
            "Still best_val_rmse: 0.4825 (from epoch 2)\n",
            "\n",
            "4 steps took 1.6 seconds\n",
            "Epoch: 2 batch_num: 214 val_rmse: 0.4867\n",
            "Still best_val_rmse: 0.4825 (from epoch 2)\n",
            "\n",
            "4 steps took 1.62 seconds\n",
            "Epoch: 2 batch_num: 218 val_rmse: 0.485\n",
            "Still best_val_rmse: 0.4825 (from epoch 2)\n",
            "\n",
            "4 steps took 1.61 seconds\n",
            "Epoch: 2 batch_num: 222 val_rmse: 0.4844\n",
            "Still best_val_rmse: 0.4825 (from epoch 2)\n",
            "\n",
            "4 steps took 1.61 seconds\n",
            "Epoch: 2 batch_num: 226 val_rmse: 0.4847\n",
            "Still best_val_rmse: 0.4825 (from epoch 2)\n",
            "\n",
            "4 steps took 1.62 seconds\n",
            "Epoch: 2 batch_num: 230 val_rmse: 0.4856\n",
            "Still best_val_rmse: 0.4825 (from epoch 2)\n",
            "\n",
            "4 steps took 1.61 seconds\n",
            "Epoch: 2 batch_num: 234 val_rmse: 0.4857\n",
            "Still best_val_rmse: 0.4825 (from epoch 2)\n",
            "\n",
            "4 steps took 1.61 seconds\n",
            "Epoch: 2 batch_num: 238 val_rmse: 0.4855\n",
            "Still best_val_rmse: 0.4825 (from epoch 2)\n",
            "\n",
            "4 steps took 1.6 seconds\n",
            "Epoch: 2 batch_num: 242 val_rmse: 0.4855\n",
            "Still best_val_rmse: 0.4825 (from epoch 2)\n",
            "\n",
            "4 steps took 1.61 seconds\n",
            "Epoch: 2 batch_num: 246 val_rmse: 0.4858\n",
            "Still best_val_rmse: 0.4825 (from epoch 2)\n",
            "\n",
            "4 steps took 1.61 seconds\n",
            "Epoch: 2 batch_num: 250 val_rmse: 0.4862\n",
            "Still best_val_rmse: 0.4825 (from epoch 2)\n",
            "\n",
            "4 steps took 1.6 seconds\n",
            "Epoch: 2 batch_num: 254 val_rmse: 0.4863\n",
            "Still best_val_rmse: 0.4825 (from epoch 2)\n",
            "\n",
            "4 steps took 1.61 seconds\n",
            "Epoch: 2 batch_num: 258 val_rmse: 0.4865\n",
            "Still best_val_rmse: 0.4825 (from epoch 2)\n",
            "\n",
            "4 steps took 1.61 seconds\n",
            "Epoch: 2 batch_num: 262 val_rmse: 0.4867\n",
            "Still best_val_rmse: 0.4825 (from epoch 2)\n",
            "\n",
            "4 steps took 1.6 seconds\n",
            "Epoch: 2 batch_num: 266 val_rmse: 0.4867\n",
            "Still best_val_rmse: 0.4825 (from epoch 2)\n",
            "\n",
            "4 steps took 1.6 seconds\n",
            "Epoch: 2 batch_num: 270 val_rmse: 0.4866\n",
            "Still best_val_rmse: 0.4825 (from epoch 2)\n",
            "\n",
            "4 steps took 1.6 seconds\n",
            "Epoch: 2 batch_num: 274 val_rmse: 0.4866\n",
            "Still best_val_rmse: 0.4825 (from epoch 2)\n",
            "\n",
            "4 steps took 1.6 seconds\n",
            "Epoch: 2 batch_num: 278 val_rmse: 0.4865\n",
            "Still best_val_rmse: 0.4825 (from epoch 2)\n",
            "\n",
            "4 steps took 1.62 seconds\n",
            "Epoch: 2 batch_num: 282 val_rmse: 0.4865\n",
            "Still best_val_rmse: 0.4825 (from epoch 2)\n",
            "\n",
            "Performance estimates:\n",
            "[0.5517595301857675, 0.47238553319350035, 0.68600796339148, 0.9246178666765156, 0.4825116644245419]\n",
            "Mean: 0.623456511574361\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.256017,
          "end_time": "2021-07-10T17:19:20.427150",
          "exception": false,
          "start_time": "2021-07-10T17:19:20.171133",
          "status": "completed"
        },
        "tags": [],
        "id": "exposed-cornell"
      },
      "source": [
        "# Inference"
      ],
      "id": "exposed-cornell"
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.254779,
          "end_time": "2021-07-10T17:19:20.959408",
          "exception": false,
          "start_time": "2021-07-10T17:19:20.704629",
          "status": "completed"
        },
        "tags": [],
        "id": "pacific-explanation"
      },
      "source": [
        ""
      ],
      "id": "pacific-explanation",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-10T17:19:21.485458Z",
          "iopub.status.busy": "2021-07-10T17:19:21.484260Z",
          "iopub.status.idle": "2021-07-10T17:19:21.509269Z",
          "shell.execute_reply": "2021-07-10T17:19:21.508604Z"
        },
        "papermill": {
          "duration": 0.29265,
          "end_time": "2021-07-10T17:19:21.509426",
          "exception": false,
          "start_time": "2021-07-10T17:19:21.216776",
          "status": "completed"
        },
        "tags": [],
        "id": "speaking-authority"
      },
      "source": [
        "test_dataset = LitDataset(test_df, inference_only=True)"
      ],
      "id": "speaking-authority",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-10T17:19:22.019508Z",
          "iopub.status.busy": "2021-07-10T17:19:22.018338Z",
          "iopub.status.idle": "2021-07-10T17:19:51.285549Z",
          "shell.execute_reply": "2021-07-10T17:19:51.284997Z"
        },
        "papermill": {
          "duration": 29.52399,
          "end_time": "2021-07-10T17:19:51.285732",
          "exception": false,
          "start_time": "2021-07-10T17:19:21.761742",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "destroyed-interval",
        "outputId": "fc7939e6-e661-4d7b-ef42-382be4baf411"
      },
      "source": [
        "all_predictions = np.zeros((len(list_val_rmse), len(test_df)))\n",
        "\n",
        "test_dataset = LitDataset(test_df, inference_only=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                         drop_last=False, shuffle=False, num_workers=2)\n",
        "\n",
        "for index in range(len(list_val_rmse)):            \n",
        "    model_path = f\"model_{index + 1}.pth\"\n",
        "    print(f\"\\nUsing {model_path}\")\n",
        "                        \n",
        "    model = LitModel()\n",
        "    model.load_state_dict(torch.load(model_path))    \n",
        "    model.to(DEVICE)\n",
        "    \n",
        "    all_predictions[index] = predict(model, test_loader)\n",
        "    \n",
        "    del model\n",
        "    gc.collect()"
      ],
      "id": "destroyed-interval",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Using model_1.pth\n",
            "\n",
            "Using model_2.pth\n",
            "\n",
            "Using model_3.pth\n",
            "\n",
            "Using model_4.pth\n",
            "\n",
            "Using model_5.pth\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-10T17:19:51.815506Z",
          "iopub.status.busy": "2021-07-10T17:19:51.814838Z",
          "iopub.status.idle": "2021-07-10T17:19:52.977737Z",
          "shell.execute_reply": "2021-07-10T17:19:52.977091Z"
        },
        "papermill": {
          "duration": 1.434043,
          "end_time": "2021-07-10T17:19:52.977904",
          "exception": false,
          "start_time": "2021-07-10T17:19:51.543861",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meaningful-petersburg",
        "outputId": "49c98bbb-184c-440d-cdcf-61dd5ff92e26"
      },
      "source": [
        "predictions = all_predictions.mean(axis=0)\n",
        "submission_df.target = predictions\n",
        "print(submission_df)\n",
        "#submission_df.to_csv(\"submission.csv\", index=False)"
      ],
      "id": "meaningful-petersburg",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "          id    target\n",
            "0  c0f722661 -0.643145\n",
            "1  f0953f0a5 -0.414170\n",
            "2  0df072751 -0.417073\n",
            "3  04caf4e0c -1.851957\n",
            "4  0e63f8bea -1.585950\n",
            "5  12537fe78 -0.971105\n",
            "6  965e592c0  0.070687\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KS-MlzVLkY1i"
      },
      "source": [
        "## Upload model"
      ],
      "id": "KS-MlzVLkY1i"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_28uXt0kYSU",
        "outputId": "c0e20558-1ab0-46e5-fbf4-b6fcea575392"
      },
      "source": [
        "# !mkdir -p ./output/\n",
        "# !cp -f ./model* ./output/\n",
        "\n",
        "!mkdir -p /content/drive/MyDrive/CommonLit/output/roberta-large-meanpooling\n",
        "!content -f /content/model* /content/drive/MyDrive/CommonLit/output/roberta-large-meanpooling\n",
        "\n",
        "#CHANGEME\n",
        "!cp -f /content/drive/MyDrive/kaggle/commonlit/pretrained-roberta-base/dataset-metadata.json ./output/dataset-metadata.json\n",
        "!sed -i -e \"s/roberta-base/roberta-large-`TZ=JST-9 date +\"%Y%m%d%H%M%S\"`/\" ./output/dataset-metadata.json\n",
        "!sed -i -e \"s/Roberta-base/Roberta-large-`TZ=JST-9 date +\"%m%d%H%M%S\"`/\" ./output/dataset-metadata.json\n",
        "!kaggle datasets create -p ./output/"
      ],
      "id": "x_28uXt0kYSU",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: content: command not found\n",
            "cp: cannot stat '/content/drive/MyDrive/kaggle/commonlit/pretrained-roberta-base/dataset-metadata.json': No such file or directory\n",
            "sed: can't read ./output/dataset-metadata.json: No such file or directory\n",
            "sed: can't read ./output/dataset-metadata.json: No such file or directory\n",
            "Metadata file not found: dataset-metadata.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iD1Cot2sk7A_",
        "outputId": "54999f06-064c-4bfe-bbdb-8f3889549165"
      },
      "source": [
        "!cat ./output/dataset-metadata.json"
      ],
      "id": "iD1Cot2sk7A_",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cat: ./output/dataset-metadata.json: No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}