{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 2702.979398,
      "end_time": "2021-07-10T17:19:56.452391",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2021-07-10T16:34:53.472993",
      "version": "2.3.3"
    },
    "colab": {
      "name": "roberta-large-maunish-meanpooling_pretrain_linerstep200.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cpptake/CommonLit/blob/main/roberta_large_maunish_meanpooling_pretrain_linerstep200.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQ0rq3v_JOk_"
      },
      "source": [
        "# Result\n",
        "[0.4920285660500482, 0.5739917178848876, 0.48154602282415837, 0.5807143194874086, 0.47822446205969066]\n",
        "* Mean: 0.5213010176612387\n",
        "\n",
        "* Mean pooling を使うとFoldごとにLossばらつく傾向あり。"
      ],
      "id": "CQ0rq3v_JOk_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrbX1a6A8Doq"
      },
      "source": [
        "# Prerequisite"
      ],
      "id": "UrbX1a6A8Doq"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdDxPgc_8SBn",
        "outputId": "1471131f-decb-48de-a013-8a7ab5391cfe"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "YdDxPgc_8SBn",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-21rfg88O1A",
        "outputId": "b4d46aa3-ad22-4102-9f1c-02a3d4c3dafb"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "id": "Q-21rfg88O1A",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Jul 20 15:10:22 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    26W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMaOL7upRq2w"
      },
      "source": [
        "## Install same version of library as Kaggle Notebook"
      ],
      "id": "KMaOL7upRq2w"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBS2Zv7DR2tk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2f045be-4a88-47bc-bdbf-dec4380d7282"
      },
      "source": [
        "# # cp でrequirementsをカレントdirにコピー\n",
        "# !cp -f /content/drive/MyDrive/kaggle/commonlit/roberta-large/requirements.txt ./\n",
        "# !cat ./requirements.txt\n",
        "!mkdir /content/drive/MyDrive/CommonLit/input/commonlitreadabilityprize/roberta-large-mlm\n",
        "!unzip -n /content/drive/MyDrive/CommonLit/input/commonlitreadabilityprize/Roberta-large-mlm.zip -d /content/drive/MyDrive/CommonLit/input/commonlitreadabilityprize/roberta-large-mlm"
      ],
      "id": "LBS2Zv7DR2tk",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/content/drive/MyDrive/CommonLit/input/commonlitreadabilityprize/roberta-large-mlm’: File exists\n",
            "unzip:  cannot find or open /content/drive/MyDrive/CommonLit/input/commonlitreadabilityprize/Roberta-large-mlm.zip, /content/drive/MyDrive/CommonLit/input/commonlitreadabilityprize/Roberta-large-mlm.zip.zip or /content/drive/MyDrive/CommonLit/input/commonlitreadabilityprize/Roberta-large-mlm.zip.ZIP.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7y1hmfoS-Q1",
        "outputId": "b88418ad-2525-41fa-b049-7cc33eca04fa"
      },
      "source": [
        "!pip uninstall -r /content/drive/MyDrive/CommonLit/working/requirements.txt -y"
      ],
      "id": "k7y1hmfoS-Q1",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found existing installation: pandas 1.2.3\n",
            "Uninstalling pandas-1.2.3:\n",
            "  Successfully uninstalled pandas-1.2.3\n",
            "Found existing installation: sklearn 0.0\n",
            "Uninstalling sklearn-0.0:\n",
            "  Successfully uninstalled sklearn-0.0\n",
            "Found existing installation: sklearn-pandas 2.1.0\n",
            "Uninstalling sklearn-pandas-2.1.0:\n",
            "  Successfully uninstalled sklearn-pandas-2.1.0\n",
            "Found existing installation: torch 1.7.0\n",
            "Uninstalling torch-1.7.0:\n",
            "  Successfully uninstalled torch-1.7.0\n",
            "Found existing installation: torchmetrics 0.2.0\n",
            "Uninstalling torchmetrics-0.2.0:\n",
            "  Successfully uninstalled torchmetrics-0.2.0\n",
            "Found existing installation: torchvision 0.8.1\n",
            "Uninstalling torchvision-0.8.1:\n",
            "  Successfully uninstalled torchvision-0.8.1\n",
            "Found existing installation: transformers 4.5.1\n",
            "Uninstalling transformers-4.5.1:\n",
            "  Successfully uninstalled transformers-4.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 889
        },
        "id": "DhM67RrVQYws",
        "outputId": "b5e23bb1-06cb-416d-ff4c-277d718f7bc5"
      },
      "source": [
        "!pip install -r /content/drive/MyDrive/CommonLit/working/requirements.txt"
      ],
      "id": "DhM67RrVQYws",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pandas==1.2.3\n",
            "  Using cached pandas-1.2.3-cp37-cp37m-manylinux1_x86_64.whl (9.9 MB)\n",
            "Collecting sklearn==0.0\n",
            "  Using cached sklearn-0.0-py2.py3-none-any.whl\n",
            "Collecting sklearn-pandas==2.1.0\n",
            "  Using cached sklearn_pandas-2.1.0-py2.py3-none-any.whl (10 kB)\n",
            "Collecting torch==1.7.0\n",
            "  Using cached torch-1.7.0-cp37-cp37m-manylinux1_x86_64.whl (776.7 MB)\n",
            "Collecting torchmetrics==0.2.0\n",
            "  Using cached torchmetrics-0.2.0-py3-none-any.whl (176 kB)\n",
            "Collecting torchvision==0.8.1\n",
            "  Using cached torchvision-0.8.1-cp37-cp37m-manylinux1_x86_64.whl (12.7 MB)\n",
            "Collecting transformers==4.5.1\n",
            "  Using cached transformers-4.5.1-py3-none-any.whl (2.1 MB)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.2.3->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 1)) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.2.3->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 1)) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from pandas==1.2.3->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 1)) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn==0.0->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 2)) (0.24.2)\n",
            "Requirement already satisfied: scipy>=1.5.1 in /usr/local/lib/python3.7/dist-packages (from sklearn-pandas==2.1.0->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 3)) (1.7.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 4)) (0.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 4)) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 4)) (0.16.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.8.1->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 7)) (7.1.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 8)) (4.41.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 8)) (0.10.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 8)) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 8)) (4.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 8)) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 8)) (21.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 8)) (0.0.45)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.1->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 8)) (2019.12.20)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas==1.2.3->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn==0.0->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 2)) (1.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn==0.0->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 2)) (2.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.5.1->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 8)) (3.5.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.5.1->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 8)) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 8)) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 8)) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 8)) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.1->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 8)) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.1->-r /content/drive/MyDrive/CommonLit/working/requirements.txt (line 8)) (7.1.2)\n",
            "Installing collected packages: torch, pandas, transformers, torchvision, torchmetrics, sklearn-pandas, sklearn\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.10.0 requires torch==1.9.0, but you have torch 1.7.0 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas~=1.1.0; python_version >= \"3.0\", but you have pandas 1.2.3 which is incompatible.\u001b[0m\n",
            "Successfully installed pandas-1.2.3 sklearn-0.0 sklearn-pandas-2.1.0 torch-1.7.0 torchmetrics-0.2.0 torchvision-0.8.1 transformers-4.5.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pandas"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBMkIBcxUIb2",
        "outputId": "fe0a7e51-2d26-4d19-dd1a-b9af91b31d29"
      },
      "source": [
        "!pip freeze |grep -e random -e math -e numpy -e pandas -e torch -e transformers -e sklearn -e gc"
      ],
      "id": "HBMkIBcxUIb2",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mpmath==1.2.1\n",
            "numpy==1.19.5\n",
            "pandas==1.2.3\n",
            "pandas-datareader==0.9.0\n",
            "pandas-gbq==0.13.3\n",
            "pandas-profiling==1.4.1\n",
            "sklearn==0.0\n",
            "sklearn-pandas==2.1.0\n",
            "tensorflow-gcs-config==2.5.0\n",
            "torch==1.7.0\n",
            "torchmetrics==0.2.0\n",
            "torchsummary==1.5.1\n",
            "torchtext==0.10.0\n",
            "torchvision==0.8.1\n",
            "transformers==4.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "De2yfzg48VPx"
      },
      "source": [
        "## Prepare dataset"
      ],
      "id": "De2yfzg48VPx"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YZk0yJH8mWO"
      },
      "source": [
        "### kaggle.json"
      ],
      "id": "_YZk0yJH8mWO"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEuB9fOD8j8l"
      },
      "source": [
        "# !mkdir -p /root/.kaggle/\n",
        "# !cp ./drive/MyDrive/kaggle/commonlit/kaggle.json ~/.kaggle/kaggle.json\n",
        "# # !chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# !pip install -q kaggle\n",
        "# !mkdir /root/.kaggle\n",
        "# !cp /content/drive/MyDrive/Colab\\ Notebooks/kaggle.json /root/.kaggle/"
      ],
      "id": "DEuB9fOD8j8l",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oKBPd9H8q0L"
      },
      "source": [
        "### Competition dataset"
      ],
      "id": "4oKBPd9H8q0L"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lv4fLHsg8vsO"
      },
      "source": [
        "# !mkdir -p /content/drive/MyDrive/CommonLit/input/commonlitreadabilityprize/\n",
        "# !kaggle competitions download -c commonlitreadabilityprize -p /content/drive/MyDrive/CommonLit/input/commonlitreadabilityprize/\n",
        "# !cp -f ./drive/MyDrive/kaggle/commonlit/train_stratiKfold.csv.zip /content/drive/MyDrive/CommonLit/input/commonlitreadabilityprize/"
      ],
      "id": "lv4fLHsg8vsO",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1QcZXR79EO2"
      },
      "source": [
        "# !unzip -o ../input/commonlitreadabilityprize/train.csv.zip -d ../input/commonlitreadabilityprize/\n",
        "# !unzip -o ../input/commonlitreadabilityprize/train_stratiKfold.csv.zip -d ../input/commonlitreadabilityprize/"
      ],
      "id": "R1QcZXR79EO2",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAJaSyc89GiB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c5b123b-46d1-4695-a146-bcb50d19ccdf"
      },
      "source": [
        "!ls ../input/commonlitreadabilityprize/"
      ],
      "id": "FAJaSyc89GiB",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ls: cannot access '../input/commonlitreadabilityprize/': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03O1cJpp9HAO"
      },
      "source": [
        "### Pretrained RoBERTa Large \n",
        "- Pretrain RoBERTa Large in the same way as this notebook\n",
        "  - https://www.kaggle.com/maunish/clrp-pytorch-roberta-pretrain\n",
        "- Dataset:\n",
        "  - https://www.kaggle.com/iamnishipy/roberta-large-20210712191259-mlm"
      ],
      "id": "03O1cJpp9HAO"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DFyAMzl9Rs_"
      },
      "source": [
        "# !mkdir -p /content/drive/MyDrive/CommonLit/input/commonlitreadabilityprize/pretrained-model/\n",
        "# !kaggle datasets download iamnishipy/roberta-large-20210712191259-mlm"
      ],
      "id": "7DFyAMzl9Rs_",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Izsn9ikg99Bn"
      },
      "source": [
        "# !unzip -o ./roberta-large-20210712191259-mlm.zip -d ../input/commonlitreadabilityprize/pretrained-model/"
      ],
      "id": "Izsn9ikg99Bn",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWsjc3K2UJrO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90937363-b0cb-4c1b-bc26-e52c00abdc0d"
      },
      "source": [
        "!ls ../input/commonlitreadabilityprize/pretrained-model/"
      ],
      "id": "WWsjc3K2UJrO",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ls: cannot access '../input/commonlitreadabilityprize/pretrained-model/': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEGf6ny98zoo"
      },
      "source": [
        ""
      ],
      "id": "AEGf6ny98zoo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.020873,
          "end_time": "2021-07-10T16:35:02.554689",
          "exception": false,
          "start_time": "2021-07-10T16:35:02.533816",
          "status": "completed"
        },
        "tags": [],
        "id": "central-liberia"
      },
      "source": [
        "# Overview\n",
        "This is kernel is almost the same as [Lightweight Roberta solution in PyTorch](https://www.kaggle.com/andretugan/lightweight-roberta-solution-in-pytorch), but instead of \"roberta-base\", it starts from [Maunish's pre-trained model](https://www.kaggle.com/maunish/clrp-roberta-base).\n",
        "\n",
        "Acknowledgments: some ideas were taken from kernels by [Torch](https://www.kaggle.com/rhtsingh) and [Maunish](https://www.kaggle.com/maunish)."
      ],
      "id": "central-liberia"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dsgr4s1G-m73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1946f2c3-b559-4b64-843b-50a2c8346e52"
      },
      "source": [
        "!pip install transformers accelerate datasets"
      ],
      "id": "Dsgr4s1G-m73",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.5.1)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.7/dist-packages (0.3.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: pyaml>=20.4.0 in /usr/local/lib/python3.7/dist-packages (from accelerate) (20.4.0)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from accelerate) (1.7.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyaml>=20.4.0->accelerate) (3.13)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->accelerate) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->accelerate) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.7/dist-packages (from torch>=1.4.0->accelerate) (0.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.2.3)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2021.7.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: huggingface-hub<0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.0.14)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2021-07-10T16:35:02.605794Z",
          "iopub.status.busy": "2021-07-10T16:35:02.602341Z",
          "iopub.status.idle": "2021-07-10T16:35:11.998468Z",
          "shell.execute_reply": "2021-07-10T16:35:11.999041Z",
          "shell.execute_reply.started": "2021-07-10T16:33:36.630414Z"
        },
        "papermill": {
          "duration": 9.425549,
          "end_time": "2021-07-10T16:35:11.999387",
          "exception": false,
          "start_time": "2021-07-10T16:35:02.573838",
          "status": "completed"
        },
        "tags": [],
        "id": "classical-garage"
      },
      "source": [
        "import os\n",
        "import math\n",
        "import random\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from transformers import AdamW\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModel\n",
        "from transformers import AutoConfig\n",
        "from transformers import get_cosine_schedule_with_warmup,get_linear_schedule_with_warmup\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "import gc\n",
        "gc.enable()"
      ],
      "id": "classical-garage",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.017537,
          "end_time": "2021-07-10T16:35:12.036899",
          "exception": false,
          "start_time": "2021-07-10T16:35:12.019362",
          "status": "completed"
        },
        "tags": [],
        "id": "challenging-bottle"
      },
      "source": [
        "## Prepare dataset"
      ],
      "id": "challenging-bottle"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-10T16:35:12.966641Z",
          "iopub.status.busy": "2021-07-10T16:35:12.965516Z",
          "iopub.status.idle": "2021-07-10T16:35:12.969362Z",
          "shell.execute_reply": "2021-07-10T16:35:12.968739Z",
          "shell.execute_reply.started": "2021-06-30T12:51:59.456435Z"
        },
        "papermill": {
          "duration": 0.076388,
          "end_time": "2021-07-10T16:35:12.969525",
          "exception": false,
          "start_time": "2021-07-10T16:35:12.893137",
          "status": "completed"
        },
        "tags": [],
        "id": "measured-cornwall"
      },
      "source": [
        "NUM_FOLDS = 5\n",
        "NUM_EPOCHS = 3\n",
        "BATCH_SIZE = 8\n",
        "MAX_LEN = 248\n",
        "EVAL_SCHEDULE = [(0.50, 16), (0.49, 8), (0.48, 4), (0.47, 2), (-1., 1)]\n",
        "\n",
        "ROBERTA_PATH = \"/content/drive/MyDrive/CommonLit/input/commonlitreadabilityprize/roberta-large-mlm/clrp_roberta_large\"\n",
        "TOKENIZER_PATH = \"/content/drive/MyDrive/CommonLit/input/commonlitreadabilityprize/roberta-large-mlm/clrp_roberta_large\"\n",
        "# ROBERTA_PATH = \"/content/drive/MyDrive/CommonLit/input/commonlitreadabilityprize/roberta-large\"\n",
        "# TOKENIZER_PATH = \"/content/drive/MyDrive/CommonLit/input/commonlitreadabilityprize/roberta-large\"\n",
        "\n",
        "# ROBERTA_PATH = \"../input/clrp-roberta-base/clrp_roberta_base\"\n",
        "# TOKENIZER_PATH = \"../input/clrp-roberta-base/clrp_roberta_base\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "id": "measured-cornwall",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-10T16:35:13.013652Z",
          "iopub.status.busy": "2021-07-10T16:35:13.012651Z",
          "iopub.status.idle": "2021-07-10T16:35:13.016079Z",
          "shell.execute_reply": "2021-07-10T16:35:13.015529Z",
          "shell.execute_reply.started": "2021-06-30T12:51:59.470504Z"
        },
        "papermill": {
          "duration": 0.028052,
          "end_time": "2021-07-10T16:35:13.016225",
          "exception": false,
          "start_time": "2021-07-10T16:35:12.988173",
          "status": "completed"
        },
        "tags": [],
        "id": "sitting-brook"
      },
      "source": [
        "def set_random_seed(random_seed):\n",
        "    random.seed(random_seed)\n",
        "    np.random.seed(random_seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n",
        "\n",
        "    torch.manual_seed(random_seed)\n",
        "    torch.cuda.manual_seed(random_seed)\n",
        "    torch.cuda.manual_seed_all(random_seed)\n",
        "\n",
        "    torch.backends.cudnn.deterministic = True"
      ],
      "id": "sitting-brook",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-10T16:35:13.059695Z",
          "iopub.status.busy": "2021-07-10T16:35:13.059033Z",
          "iopub.status.idle": "2021-07-10T16:35:13.181860Z",
          "shell.execute_reply": "2021-07-10T16:35:13.181008Z",
          "shell.execute_reply.started": "2021-06-30T12:51:59.485325Z"
        },
        "papermill": {
          "duration": 0.147365,
          "end_time": "2021-07-10T16:35:13.182094",
          "exception": false,
          "start_time": "2021-07-10T16:35:13.034729",
          "status": "completed"
        },
        "tags": [],
        "id": "banner-plastic"
      },
      "source": [
        "train_df = pd.read_csv(\"/content/drive/MyDrive/CommonLit/input/commonlitreadabilityprize/train.csv\")\n",
        "\n",
        "# Remove incomplete entries if any.\n",
        "train_df.drop(train_df[(train_df.target == 0) & (train_df.standard_error == 0)].index,\n",
        "              inplace=True)\n",
        "train_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "test_df = pd.read_csv(\"/content/drive/MyDrive/CommonLit/input/commonlitreadabilityprize/test.csv\")\n",
        "submission_df = pd.read_csv(\"/content/drive/MyDrive/CommonLit/input/commonlitreadabilityprize/sample_submission.csv\")"
      ],
      "id": "banner-plastic",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-10T16:35:13.223654Z",
          "iopub.status.busy": "2021-07-10T16:35:13.222955Z",
          "iopub.status.idle": "2021-07-10T16:35:13.465852Z",
          "shell.execute_reply": "2021-07-10T16:35:13.464700Z",
          "shell.execute_reply.started": "2021-06-30T12:51:59.537207Z"
        },
        "papermill": {
          "duration": 0.265264,
          "end_time": "2021-07-10T16:35:13.466048",
          "exception": false,
          "start_time": "2021-07-10T16:35:13.200784",
          "status": "completed"
        },
        "tags": [],
        "id": "unavailable-philadelphia"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)"
      ],
      "id": "unavailable-philadelphia",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.018281,
          "end_time": "2021-07-10T16:35:13.502997",
          "exception": false,
          "start_time": "2021-07-10T16:35:13.484716",
          "status": "completed"
        },
        "tags": [],
        "id": "intermediate-brand"
      },
      "source": [
        "# Dataset"
      ],
      "id": "intermediate-brand"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-10T16:35:13.549331Z",
          "iopub.status.busy": "2021-07-10T16:35:13.548372Z",
          "iopub.status.idle": "2021-07-10T16:35:13.552476Z",
          "shell.execute_reply": "2021-07-10T16:35:13.551942Z",
          "shell.execute_reply.started": "2021-06-30T12:51:59.797452Z"
        },
        "papermill": {
          "duration": 0.031082,
          "end_time": "2021-07-10T16:35:13.552607",
          "exception": false,
          "start_time": "2021-07-10T16:35:13.521525",
          "status": "completed"
        },
        "tags": [],
        "id": "adopted-prayer"
      },
      "source": [
        "class LitDataset(Dataset):\n",
        "    def __init__(self, df, inference_only=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.df = df        \n",
        "        self.inference_only = inference_only\n",
        "        self.text = df.excerpt.tolist()\n",
        "        #self.text = [text.replace(\"\\n\", \" \") for text in self.text]\n",
        "        \n",
        "        if not self.inference_only:\n",
        "            self.target = torch.tensor(df.target.values, dtype=torch.float32)        \n",
        "    \n",
        "        self.encoded = tokenizer.batch_encode_plus(\n",
        "            self.text,\n",
        "            padding = 'max_length',            \n",
        "            max_length = MAX_LEN,\n",
        "            truncation = True,\n",
        "            return_attention_mask=True\n",
        "        )        \n",
        " \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    \n",
        "    def __getitem__(self, index):        \n",
        "        input_ids = torch.tensor(self.encoded['input_ids'][index])\n",
        "        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n",
        "        \n",
        "        if self.inference_only:\n",
        "            return (input_ids, attention_mask)            \n",
        "        else:\n",
        "            target = self.target[index]\n",
        "            return (input_ids, attention_mask, target)"
      ],
      "id": "adopted-prayer",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.018016,
          "end_time": "2021-07-10T16:35:13.588706",
          "exception": false,
          "start_time": "2021-07-10T16:35:13.570690",
          "status": "completed"
        },
        "tags": [],
        "id": "sonic-cooperative"
      },
      "source": [
        "# Model\n",
        "The model is inspired by the one from [Maunish](https://www.kaggle.com/maunish/clrp-roberta-svm)."
      ],
      "id": "sonic-cooperative"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-10T16:35:13.662281Z",
          "iopub.status.busy": "2021-07-10T16:35:13.661368Z",
          "iopub.status.idle": "2021-07-10T16:35:13.679333Z",
          "shell.execute_reply": "2021-07-10T16:35:13.680266Z",
          "shell.execute_reply.started": "2021-06-30T12:51:59.811644Z"
        },
        "papermill": {
          "duration": 0.063207,
          "end_time": "2021-07-10T16:35:13.680545",
          "exception": false,
          "start_time": "2021-07-10T16:35:13.617338",
          "status": "completed"
        },
        "tags": [],
        "id": "listed-coordinate"
      },
      "source": [
        "class LitModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n",
        "        config.update({\"output_hidden_states\":True, \n",
        "                       \"hidden_dropout_prob\": 0.0,\n",
        "                       \"layer_norm_eps\": 1e-7})                       \n",
        "        \n",
        "        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config)\n",
        "        #https://towardsdatascience.com/attention-based-deep-multiple-instance-learning-1bb3df857e24\n",
        "        # 768: node fully connected layer\n",
        "        # 512: node attention layer\n",
        "        # self.attention = nn.Sequential(            \n",
        "        #     nn.Linear(768, 512),            \n",
        "        #     nn.Tanh(),                       \n",
        "        #     nn.Linear(512, 1),\n",
        "        #     nn.Softmax(dim=1)\n",
        "        # )        \n",
        "\n",
        "        # self.regressor = nn.Sequential(                        \n",
        "        #     nn.Linear(768, 1)                        \n",
        "        # )\n",
        "\n",
        "\n",
        "        #768 -> 1024\n",
        "        #512 -> 768\n",
        "        self.attention = nn.Sequential(            \n",
        "            nn.Linear(1024, 768),            \n",
        "            nn.Tanh(),                       \n",
        "            nn.Linear(768, 1),\n",
        "            nn.Softmax(dim=1)\n",
        "        )        \n",
        "\n",
        "        self.regressor = nn.Sequential(                        \n",
        "            nn.Linear(1024, 1)                        \n",
        "        )\n",
        "        \n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # roberta_output = self.roberta(input_ids=input_ids,\n",
        "        #                               attention_mask=attention_mask)\n",
        "        \n",
        "        roberta_output = self.roberta(input_ids=input_ids,\n",
        "                                      attention_mask=attention_mask)\n",
        "        \n",
        "        last_hidden_state = roberta_output[0]\n",
        "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
        "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
        "        sum_mask = input_mask_expanded.sum(1)\n",
        "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
        "        mean_embeddings = sum_embeddings / sum_mask\n",
        "\n",
        "        # print(mean_embeddings.shape)\n",
        "\n",
        "        logits = self.regressor(mean_embeddings)\n",
        "        \n",
        "        preds = logits.squeeze(-1).squeeze(-1)\n",
        "\n",
        "        return preds\n",
        "\n",
        "        #### nishipy original ####\n",
        "        # # There are a total of 13 layers of hidden states.\n",
        "        # # 1 for the embedding layer, and 12 for the 12 Roberta layers.\n",
        "        # # We take the hidden states from the last Roberta layer.\n",
        "        # last_layer_hidden_states = roberta_output.hidden_states[-1]\n",
        "\n",
        "        # # The number of cells is MAX_LEN.\n",
        "        # # The size of the hidden state of each cell is 768 (for roberta-base).\n",
        "        # # In order to condense hidden states of all cells to a context vector,\n",
        "        # # we compute a weighted average of the hidden states of all cells.\n",
        "        # # We compute the weight of each cell, using the attention neural network.\n",
        "        # weights = self.attention(last_layer_hidden_states)\n",
        "                \n",
        "        # # weights.shape is BATCH_SIZE x MAX_LEN x 1\n",
        "        # # last_layer_hidden_states.shape is BATCH_SIZE x MAX_LEN x 768        \n",
        "        # # Now we compute context_vector as the weighted average.\n",
        "        # # context_vector.shape is BATCH_SIZE x 768\n",
        "        # context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n",
        "        \n",
        "        # # Now we reduce the context vector to the prediction score.\n",
        "        # return self.regressor(context_vector)"
      ],
      "id": "listed-coordinate",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-10T16:35:13.776053Z",
          "iopub.status.busy": "2021-07-10T16:35:13.774791Z",
          "iopub.status.idle": "2021-07-10T16:35:13.782919Z",
          "shell.execute_reply": "2021-07-10T16:35:13.784211Z",
          "shell.execute_reply.started": "2021-06-30T12:51:59.831662Z"
        },
        "papermill": {
          "duration": 0.069033,
          "end_time": "2021-07-10T16:35:13.784378",
          "exception": false,
          "start_time": "2021-07-10T16:35:13.715345",
          "status": "completed"
        },
        "tags": [],
        "id": "marked-citation"
      },
      "source": [
        "def eval_mse(model, data_loader):\n",
        "    \"\"\"Evaluates the mean squared error of the |model| on |data_loader|\"\"\"\n",
        "    model.eval()            \n",
        "    mse_sum = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_num, (input_ids, attention_mask, target) in enumerate(data_loader):\n",
        "            input_ids = input_ids.to(DEVICE)\n",
        "            attention_mask = attention_mask.to(DEVICE)                        \n",
        "            target = target.to(DEVICE)           \n",
        "            \n",
        "            pred = model(input_ids, attention_mask)                       \n",
        "\n",
        "            mse_sum += nn.MSELoss(reduction=\"sum\")(pred.flatten(), target).item()\n",
        "                \n",
        "\n",
        "    return mse_sum / len(data_loader.dataset)"
      ],
      "id": "marked-citation",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-10T16:35:13.860639Z",
          "iopub.status.busy": "2021-07-10T16:35:13.859518Z",
          "iopub.status.idle": "2021-07-10T16:35:13.865246Z",
          "shell.execute_reply": "2021-07-10T16:35:13.867010Z",
          "shell.execute_reply.started": "2021-06-30T12:51:59.844758Z"
        },
        "papermill": {
          "duration": 0.049393,
          "end_time": "2021-07-10T16:35:13.867227",
          "exception": false,
          "start_time": "2021-07-10T16:35:13.817834",
          "status": "completed"
        },
        "tags": [],
        "id": "associate-astrology"
      },
      "source": [
        "def predict(model, data_loader):\n",
        "    \"\"\"Returns an np.array with predictions of the |model| on |data_loader|\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    result = np.zeros(len(data_loader.dataset))    \n",
        "    index = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch_num, (input_ids, attention_mask) in enumerate(data_loader):\n",
        "            input_ids = input_ids.to(DEVICE)\n",
        "            attention_mask = attention_mask.to(DEVICE)\n",
        "                        \n",
        "            pred = model(input_ids, attention_mask)                        \n",
        "\n",
        "            result[index : index + pred.shape[0]] = pred.flatten().to(\"cpu\")\n",
        "            index += pred.shape[0]\n",
        "\n",
        "    return result"
      ],
      "id": "associate-astrology",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-10T16:35:13.957154Z",
          "iopub.status.busy": "2021-07-10T16:35:13.955894Z",
          "iopub.status.idle": "2021-07-10T16:35:13.970258Z",
          "shell.execute_reply": "2021-07-10T16:35:13.971515Z",
          "shell.execute_reply.started": "2021-06-30T12:51:59.863562Z"
        },
        "papermill": {
          "duration": 0.064065,
          "end_time": "2021-07-10T16:35:13.971767",
          "exception": false,
          "start_time": "2021-07-10T16:35:13.907702",
          "status": "completed"
        },
        "tags": [],
        "id": "impressed-minnesota"
      },
      "source": [
        "def train(model, model_path, train_loader, val_loader,\n",
        "          optimizer, scheduler=None, num_epochs=NUM_EPOCHS):    \n",
        "    best_val_rmse = None\n",
        "    best_epoch = 0\n",
        "    step = 0\n",
        "    last_eval_step = 0\n",
        "    eval_period = EVAL_SCHEDULE[0][1]    \n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):                           \n",
        "        val_rmse = None         \n",
        "\n",
        "        for batch_num, (input_ids, attention_mask, target) in enumerate(train_loader):\n",
        "            input_ids = input_ids.to(DEVICE)\n",
        "            attention_mask = attention_mask.to(DEVICE)            \n",
        "            target = target.to(DEVICE)                        \n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            model.train()\n",
        "\n",
        "            pred = model(input_ids, attention_mask)\n",
        "                                                        \n",
        "            mse = nn.MSELoss(reduction=\"mean\")(pred.flatten(), target)\n",
        "                        \n",
        "            mse.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "            if scheduler:\n",
        "                scheduler.step()\n",
        "            \n",
        "            if step >= last_eval_step + eval_period:\n",
        "                # Evaluate the model on val_loader.\n",
        "                elapsed_seconds = time.time() - start\n",
        "                num_steps = step - last_eval_step\n",
        "                print(f\"\\n{num_steps} steps took {elapsed_seconds:0.3} seconds\")\n",
        "                last_eval_step = step\n",
        "                \n",
        "                val_rmse = math.sqrt(eval_mse(model, val_loader))                            \n",
        "\n",
        "                print(f\"Epoch: {epoch} batch_num: {batch_num}\", \n",
        "                      f\"val_rmse: {val_rmse:0.4}\")\n",
        "\n",
        "                for rmse, period in EVAL_SCHEDULE:\n",
        "                    if val_rmse >= rmse:\n",
        "                        eval_period = period\n",
        "                        break                               \n",
        "                \n",
        "                if not best_val_rmse or val_rmse < best_val_rmse:                    \n",
        "                    best_val_rmse = val_rmse\n",
        "                    best_epoch = epoch\n",
        "                    torch.save(model.state_dict(), model_path)\n",
        "                    print(f\"New best_val_rmse: {best_val_rmse:0.4}\")\n",
        "                else:       \n",
        "                    print(f\"Still best_val_rmse: {best_val_rmse:0.4}\",\n",
        "                          f\"(from epoch {best_epoch})\")                                    \n",
        "                    \n",
        "                start = time.time()\n",
        "                                            \n",
        "            step += 1\n",
        "                        \n",
        "    \n",
        "    return best_val_rmse"
      ],
      "id": "impressed-minnesota",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-10T16:35:14.053687Z",
          "iopub.status.busy": "2021-07-10T16:35:14.052560Z",
          "iopub.status.idle": "2021-07-10T16:35:14.060346Z",
          "shell.execute_reply": "2021-07-10T16:35:14.062050Z",
          "shell.execute_reply.started": "2021-06-30T12:51:59.879987Z"
        },
        "papermill": {
          "duration": 0.054887,
          "end_time": "2021-07-10T16:35:14.062272",
          "exception": false,
          "start_time": "2021-07-10T16:35:14.007385",
          "status": "completed"
        },
        "tags": [],
        "id": "handled-trouble"
      },
      "source": [
        "#怪しい\n",
        "def create_optimizer(model):\n",
        "    #model.named_parameters():\n",
        "    #Base -> 205\n",
        "    #Large -> 397\n",
        "    named_parameters = list(model.named_parameters())    \n",
        "\n",
        "    #Base\n",
        "    # roberta_parameters = named_parameters[:197]    \n",
        "    # attention_parameters = named_parameters[199:203]\n",
        "    # regressor_parameters = named_parameters[203:]\n",
        "    \n",
        "    #Large\n",
        "    roberta_parameters = named_parameters[:389]    \n",
        "    attention_parameters = named_parameters[391:395]\n",
        "    regressor_parameters = named_parameters[395:]\n",
        "        \n",
        "    attention_group = [params for (name, params) in attention_parameters]\n",
        "    regressor_group = [params for (name, params) in regressor_parameters]\n",
        "\n",
        "    parameters = []\n",
        "    parameters.append({\"params\": attention_group})\n",
        "    parameters.append({\"params\": regressor_group})\n",
        "\n",
        "    for layer_num, (name, params) in enumerate(roberta_parameters):\n",
        "        weight_decay = 0.0 if \"bias\" in name else 0.01\n",
        "        lr = 2e-5\n",
        "        #roberta-base: \n",
        "        # if layer_num >= 69: #4/12layers       \n",
        "        #     lr = 5e-5\n",
        "        # if layer_num >= 1f33: #8/12layers\n",
        "        #     lr = 1e-4\n",
        "        #roberta-large\n",
        "        if layer_num >= 133: #8/24layers     \n",
        "            lr = 5e-5\n",
        "        if layer_num >= 261: #16/24layers\n",
        "            lr = 1e-4\n",
        "\n",
        "\n",
        "        parameters.append({\"params\": params,\n",
        "                           \"weight_decay\": weight_decay,\n",
        "                           \"lr\": lr})\n",
        "\n",
        "    return AdamW(parameters)"
      ],
      "id": "handled-trouble",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SjNJPllCHBG"
      },
      "source": [
        "# SEED = 1000\n",
        "# kfold = KFold(n_splits=NUM_FOLDS, random_state=SEED, shuffle=True)\n",
        "# for fold, (train_indices, val_indices) in enumerate(kfold.split(train_df)):\n",
        "#     print(fold)\n",
        "#     print('------------')\n",
        "#     print(val_indices)"
      ],
      "id": "9SjNJPllCHBG",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcrbAtOaJaVS"
      },
      "source": [
        "# Debug"
      ],
      "id": "KcrbAtOaJaVS"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhG0dfPfJZpl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "809c6a02-567a-4b8c-b365-402823d3fab1"
      },
      "source": [
        "model = LitModel().to(DEVICE)"
      ],
      "id": "vhG0dfPfJZpl",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at /content/drive/MyDrive/CommonLit/input/commonlitreadabilityprize/roberta-large-mlm/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9ogBaNfODQ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09397b72-97a6-4908-f41e-46e9e5caed66"
      },
      "source": [
        "named_parameters = list(model.named_parameters())\n",
        "len(named_parameters)"
      ],
      "id": "r9ogBaNfODQ8",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "397"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtGlz9UyNEPt"
      },
      "source": [
        "# for name, param in model.named_parameters():\n",
        "#     print(name)"
      ],
      "id": "XtGlz9UyNEPt",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxgwubuBJcY1"
      },
      "source": [
        ""
      ],
      "id": "MxgwubuBJcY1",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMrGZvBFJcOe"
      },
      "source": [
        ""
      ],
      "id": "nMrGZvBFJcOe",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtUUBp-MJXoa"
      },
      "source": [
        "# Train\n"
      ],
      "id": "jtUUBp-MJXoa"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibNIM2txGvBf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fbc7407-81f0-4475-f1cb-869be590749a"
      },
      "source": [
        "# output にフォルダ作成\n",
        "!mkdir /content/drive/MyDrive/CommonLit/output/roberta-large-meanpooling-pretrain_linerstep200"
      ],
      "id": "ibNIM2txGvBf",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/content/drive/MyDrive/CommonLit/output/roberta-large-meanpooling-pretrain_linerstep200’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-10T16:35:14.144962Z",
          "iopub.status.busy": "2021-07-10T16:35:14.141009Z",
          "iopub.status.idle": "2021-07-10T17:19:19.916633Z",
          "shell.execute_reply": "2021-07-10T17:19:19.915678Z"
        },
        "papermill": {
          "duration": 2645.82007,
          "end_time": "2021-07-10T17:19:19.916828",
          "exception": false,
          "start_time": "2021-07-10T16:35:14.096758",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "internal-filename",
        "outputId": "1d7f92a1-c9e5-413d-deb3-c028a7113775"
      },
      "source": [
        "gc.collect()\n",
        "\n",
        "SEED = 1000\n",
        "list_val_rmse = []\n",
        "PATH = '/content/drive/MyDrive/CommonLit/output/roberta-large-meanpooling-pretrain_linerstep200/'\n",
        "\n",
        "kfold = KFold(n_splits=NUM_FOLDS, random_state=SEED, shuffle=True)\n",
        "\n",
        "for fold, (train_indices, val_indices) in enumerate(kfold.split(train_df)):    \n",
        "    print(f\"\\nFold {fold + 1}/{NUM_FOLDS}\")\n",
        "    model_path = PATH + f\"model_{fold + 1}.pth\"\n",
        "        \n",
        "    set_random_seed(SEED + fold)\n",
        "    \n",
        "    train_dataset = LitDataset(train_df.loc[train_indices])    \n",
        "    val_dataset = LitDataset(train_df.loc[val_indices])    \n",
        "        \n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                              drop_last=True, shuffle=True, num_workers=2)    \n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,\n",
        "                            drop_last=False, shuffle=False, num_workers=2)    \n",
        "        \n",
        "    set_random_seed(SEED + fold)    \n",
        "    \n",
        "    model = LitModel().to(DEVICE)\n",
        "    \n",
        "    optimizer = create_optimizer(model)                        \n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_training_steps=NUM_EPOCHS * len(train_loader),\n",
        "        num_warmup_steps=200)#original 50\n",
        "    \n",
        "    list_val_rmse.append(train(model, model_path, train_loader,\n",
        "                               val_loader, optimizer, scheduler=scheduler))\n",
        "\n",
        "    del model\n",
        "    gc.collect()\n",
        "    \n",
        "    print(\"\\nPerformance estimates:\")\n",
        "    print(list_val_rmse)\n",
        "    print(\"Mean:\", np.array(list_val_rmse).mean())\n",
        "    "
      ],
      "id": "internal-filename",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Fold 1/5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at /content/drive/MyDrive/CommonLit/input/commonlitreadabilityprize/roberta-large-mlm/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "16 steps took 7.0 seconds\n",
            "Epoch: 0 batch_num: 16 val_rmse: 0.991\n",
            "New best_val_rmse: 0.991\n",
            "\n",
            "16 steps took 6.38 seconds\n",
            "Epoch: 0 batch_num: 32 val_rmse: 0.725\n",
            "New best_val_rmse: 0.725\n",
            "\n",
            "16 steps took 6.41 seconds\n",
            "Epoch: 0 batch_num: 48 val_rmse: 0.8091\n",
            "Still best_val_rmse: 0.725 (from epoch 0)\n",
            "\n",
            "16 steps took 6.57 seconds\n",
            "Epoch: 0 batch_num: 64 val_rmse: 0.6524\n",
            "New best_val_rmse: 0.6524\n",
            "\n",
            "16 steps took 6.43 seconds\n",
            "Epoch: 0 batch_num: 80 val_rmse: 0.6921\n",
            "Still best_val_rmse: 0.6524 (from epoch 0)\n",
            "\n",
            "16 steps took 6.49 seconds\n",
            "Epoch: 0 batch_num: 96 val_rmse: 0.6418\n",
            "New best_val_rmse: 0.6418\n",
            "\n",
            "16 steps took 6.44 seconds\n",
            "Epoch: 0 batch_num: 112 val_rmse: 0.6287\n",
            "New best_val_rmse: 0.6287\n",
            "\n",
            "16 steps took 6.42 seconds\n",
            "Epoch: 0 batch_num: 128 val_rmse: 0.6554\n",
            "Still best_val_rmse: 0.6287 (from epoch 0)\n",
            "\n",
            "16 steps took 6.53 seconds\n",
            "Epoch: 0 batch_num: 144 val_rmse: 0.6552\n",
            "Still best_val_rmse: 0.6287 (from epoch 0)\n",
            "\n",
            "16 steps took 6.49 seconds\n",
            "Epoch: 0 batch_num: 160 val_rmse: 0.5943\n",
            "New best_val_rmse: 0.5943\n",
            "\n",
            "16 steps took 6.47 seconds\n",
            "Epoch: 0 batch_num: 176 val_rmse: 0.5904\n",
            "New best_val_rmse: 0.5904\n",
            "\n",
            "16 steps took 6.43 seconds\n",
            "Epoch: 0 batch_num: 192 val_rmse: 0.7292\n",
            "Still best_val_rmse: 0.5904 (from epoch 0)\n",
            "\n",
            "16 steps took 6.52 seconds\n",
            "Epoch: 0 batch_num: 208 val_rmse: 0.6257\n",
            "Still best_val_rmse: 0.5904 (from epoch 0)\n",
            "\n",
            "16 steps took 6.53 seconds\n",
            "Epoch: 0 batch_num: 224 val_rmse: 0.8229\n",
            "Still best_val_rmse: 0.5904 (from epoch 0)\n",
            "\n",
            "16 steps took 6.43 seconds\n",
            "Epoch: 0 batch_num: 240 val_rmse: 0.5699\n",
            "New best_val_rmse: 0.5699\n",
            "\n",
            "16 steps took 6.46 seconds\n",
            "Epoch: 0 batch_num: 256 val_rmse: 0.5987\n",
            "Still best_val_rmse: 0.5699 (from epoch 0)\n",
            "\n",
            "16 steps took 6.54 seconds\n",
            "Epoch: 0 batch_num: 272 val_rmse: 0.6034\n",
            "Still best_val_rmse: 0.5699 (from epoch 0)\n",
            "\n",
            "16 steps took 6.7 seconds\n",
            "Epoch: 1 batch_num: 5 val_rmse: 0.5793\n",
            "Still best_val_rmse: 0.5699 (from epoch 0)\n",
            "\n",
            "16 steps took 6.41 seconds\n",
            "Epoch: 1 batch_num: 21 val_rmse: 0.5833\n",
            "Still best_val_rmse: 0.5699 (from epoch 0)\n",
            "\n",
            "16 steps took 6.43 seconds\n",
            "Epoch: 1 batch_num: 37 val_rmse: 1.122\n",
            "Still best_val_rmse: 0.5699 (from epoch 0)\n",
            "\n",
            "16 steps took 6.4 seconds\n",
            "Epoch: 1 batch_num: 53 val_rmse: 1.024\n",
            "Still best_val_rmse: 0.5699 (from epoch 0)\n",
            "\n",
            "16 steps took 6.41 seconds\n",
            "Epoch: 1 batch_num: 69 val_rmse: 1.018\n",
            "Still best_val_rmse: 0.5699 (from epoch 0)\n",
            "\n",
            "16 steps took 6.39 seconds\n",
            "Epoch: 1 batch_num: 85 val_rmse: 1.028\n",
            "Still best_val_rmse: 0.5699 (from epoch 0)\n",
            "\n",
            "16 steps took 6.42 seconds\n",
            "Epoch: 1 batch_num: 101 val_rmse: 1.02\n",
            "Still best_val_rmse: 0.5699 (from epoch 0)\n",
            "\n",
            "16 steps took 6.45 seconds\n",
            "Epoch: 1 batch_num: 117 val_rmse: 1.092\n",
            "Still best_val_rmse: 0.5699 (from epoch 0)\n",
            "\n",
            "16 steps took 6.4 seconds\n",
            "Epoch: 1 batch_num: 133 val_rmse: 1.062\n",
            "Still best_val_rmse: 0.5699 (from epoch 0)\n",
            "\n",
            "16 steps took 6.39 seconds\n",
            "Epoch: 1 batch_num: 149 val_rmse: 1.026\n",
            "Still best_val_rmse: 0.5699 (from epoch 0)\n",
            "\n",
            "16 steps took 6.4 seconds\n",
            "Epoch: 1 batch_num: 165 val_rmse: 1.018\n",
            "Still best_val_rmse: 0.5699 (from epoch 0)\n",
            "\n",
            "16 steps took 6.48 seconds\n",
            "Epoch: 1 batch_num: 181 val_rmse: 1.031\n",
            "Still best_val_rmse: 0.5699 (from epoch 0)\n",
            "\n",
            "16 steps took 6.44 seconds\n",
            "Epoch: 1 batch_num: 197 val_rmse: 1.018\n",
            "Still best_val_rmse: 0.5699 (from epoch 0)\n",
            "\n",
            "16 steps took 6.41 seconds\n",
            "Epoch: 1 batch_num: 213 val_rmse: 1.018\n",
            "Still best_val_rmse: 0.5699 (from epoch 0)\n",
            "\n",
            "16 steps took 6.41 seconds\n",
            "Epoch: 1 batch_num: 229 val_rmse: 1.023\n",
            "Still best_val_rmse: 0.5699 (from epoch 0)\n",
            "\n",
            "16 steps took 6.44 seconds\n",
            "Epoch: 1 batch_num: 245 val_rmse: 1.02\n",
            "Still best_val_rmse: 0.5699 (from epoch 0)\n",
            "\n",
            "16 steps took 6.42 seconds\n",
            "Epoch: 1 batch_num: 261 val_rmse: 1.018\n",
            "Still best_val_rmse: 0.5699 (from epoch 0)\n",
            "\n",
            "16 steps took 6.45 seconds\n",
            "Epoch: 1 batch_num: 277 val_rmse: 1.018\n",
            "Still best_val_rmse: 0.5699 (from epoch 0)\n",
            "\n",
            "16 steps took 6.65 seconds\n",
            "Epoch: 2 batch_num: 10 val_rmse: 1.03\n",
            "Still best_val_rmse: 0.5699 (from epoch 0)\n",
            "\n",
            "16 steps took 6.4 seconds\n",
            "Epoch: 2 batch_num: 26 val_rmse: 1.02\n",
            "Still best_val_rmse: 0.5699 (from epoch 0)\n",
            "\n",
            "16 steps took 6.4 seconds\n",
            "Epoch: 2 batch_num: 42 val_rmse: 1.025\n",
            "Still best_val_rmse: 0.5699 (from epoch 0)\n",
            "\n",
            "16 steps took 6.44 seconds\n",
            "Epoch: 2 batch_num: 58 val_rmse: 1.042\n",
            "Still best_val_rmse: 0.5699 (from epoch 0)\n",
            "\n",
            "16 steps took 6.43 seconds\n",
            "Epoch: 2 batch_num: 74 val_rmse: 1.024\n",
            "Still best_val_rmse: 0.5699 (from epoch 0)\n",
            "\n",
            "16 steps took 6.39 seconds\n",
            "Epoch: 2 batch_num: 90 val_rmse: 1.018\n",
            "Still best_val_rmse: 0.5699 (from epoch 0)\n",
            "\n",
            "16 steps took 6.41 seconds\n",
            "Epoch: 2 batch_num: 106 val_rmse: 1.021\n",
            "Still best_val_rmse: 0.5699 (from epoch 0)\n",
            "\n",
            "16 steps took 6.4 seconds\n",
            "Epoch: 2 batch_num: 122 val_rmse: 1.018\n",
            "Still best_val_rmse: 0.5699 (from epoch 0)\n",
            "\n",
            "16 steps took 6.45 seconds\n",
            "Epoch: 2 batch_num: 138 val_rmse: 1.022\n",
            "Still best_val_rmse: 0.5699 (from epoch 0)\n",
            "\n",
            "16 steps took 6.41 seconds\n",
            "Epoch: 2 batch_num: 154 val_rmse: 1.019\n",
            "Still best_val_rmse: 0.5699 (from epoch 0)\n",
            "\n",
            "16 steps took 6.41 seconds\n",
            "Epoch: 2 batch_num: 170 val_rmse: 1.019\n",
            "Still best_val_rmse: 0.5699 (from epoch 0)\n",
            "\n",
            "16 steps took 6.4 seconds\n",
            "Epoch: 2 batch_num: 186 val_rmse: 1.019\n",
            "Still best_val_rmse: 0.5699 (from epoch 0)\n",
            "\n",
            "16 steps took 6.44 seconds\n",
            "Epoch: 2 batch_num: 202 val_rmse: 1.019\n",
            "Still best_val_rmse: 0.5699 (from epoch 0)\n",
            "\n",
            "16 steps took 6.41 seconds\n",
            "Epoch: 2 batch_num: 218 val_rmse: 1.018\n",
            "Still best_val_rmse: 0.5699 (from epoch 0)\n",
            "\n",
            "16 steps took 6.42 seconds\n",
            "Epoch: 2 batch_num: 234 val_rmse: 1.018\n",
            "Still best_val_rmse: 0.5699 (from epoch 0)\n",
            "\n",
            "16 steps took 6.39 seconds\n",
            "Epoch: 2 batch_num: 250 val_rmse: 1.018\n",
            "Still best_val_rmse: 0.5699 (from epoch 0)\n",
            "\n",
            "16 steps took 6.45 seconds\n",
            "Epoch: 2 batch_num: 266 val_rmse: 1.019\n",
            "Still best_val_rmse: 0.5699 (from epoch 0)\n",
            "\n",
            "16 steps took 6.42 seconds\n",
            "Epoch: 2 batch_num: 282 val_rmse: 1.019\n",
            "Still best_val_rmse: 0.5699 (from epoch 0)\n",
            "\n",
            "Performance estimates:\n",
            "[0.5699093717282384]\n",
            "Mean: 0.5699093717282384\n",
            "\n",
            "Fold 2/5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at /content/drive/MyDrive/CommonLit/input/commonlitreadabilityprize/roberta-large-mlm/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "16 steps took 7.02 seconds\n",
            "Epoch: 0 batch_num: 16 val_rmse: 1.318\n",
            "New best_val_rmse: 1.318\n",
            "\n",
            "16 steps took 6.41 seconds\n",
            "Epoch: 0 batch_num: 32 val_rmse: 0.8782\n",
            "New best_val_rmse: 0.8782\n",
            "\n",
            "16 steps took 6.4 seconds\n",
            "Epoch: 0 batch_num: 48 val_rmse: 0.7439\n",
            "New best_val_rmse: 0.7439\n",
            "\n",
            "16 steps took 6.42 seconds\n",
            "Epoch: 0 batch_num: 64 val_rmse: 0.7936\n",
            "Still best_val_rmse: 0.7439 (from epoch 0)\n",
            "\n",
            "16 steps took 6.47 seconds\n",
            "Epoch: 0 batch_num: 80 val_rmse: 0.6668\n",
            "New best_val_rmse: 0.6668\n",
            "\n",
            "16 steps took 6.4 seconds\n",
            "Epoch: 0 batch_num: 96 val_rmse: 0.737\n",
            "Still best_val_rmse: 0.6668 (from epoch 0)\n",
            "\n",
            "16 steps took 6.56 seconds\n",
            "Epoch: 0 batch_num: 112 val_rmse: 0.7118\n",
            "Still best_val_rmse: 0.6668 (from epoch 0)\n",
            "\n",
            "16 steps took 6.54 seconds\n",
            "Epoch: 0 batch_num: 128 val_rmse: 0.6291\n",
            "New best_val_rmse: 0.6291\n",
            "\n",
            "16 steps took 6.44 seconds\n",
            "Epoch: 0 batch_num: 144 val_rmse: 0.7072\n",
            "Still best_val_rmse: 0.6291 (from epoch 0)\n",
            "\n",
            "16 steps took 6.55 seconds\n",
            "Epoch: 0 batch_num: 160 val_rmse: 0.6978\n",
            "Still best_val_rmse: 0.6291 (from epoch 0)\n",
            "\n",
            "16 steps took 6.58 seconds\n",
            "Epoch: 0 batch_num: 176 val_rmse: 0.7537\n",
            "Still best_val_rmse: 0.6291 (from epoch 0)\n",
            "\n",
            "16 steps took 6.47 seconds\n",
            "Epoch: 0 batch_num: 192 val_rmse: 0.7087\n",
            "Still best_val_rmse: 0.6291 (from epoch 0)\n",
            "\n",
            "16 steps took 6.43 seconds\n",
            "Epoch: 0 batch_num: 208 val_rmse: 0.6047\n",
            "New best_val_rmse: 0.6047\n",
            "\n",
            "16 steps took 6.41 seconds\n",
            "Epoch: 0 batch_num: 224 val_rmse: 0.6213\n",
            "Still best_val_rmse: 0.6047 (from epoch 0)\n",
            "\n",
            "16 steps took 6.54 seconds\n",
            "Epoch: 0 batch_num: 240 val_rmse: 0.8031\n",
            "Still best_val_rmse: 0.6047 (from epoch 0)\n",
            "\n",
            "16 steps took 6.51 seconds\n",
            "Epoch: 0 batch_num: 256 val_rmse: 1.028\n",
            "Still best_val_rmse: 0.6047 (from epoch 0)\n",
            "\n",
            "16 steps took 6.4 seconds\n",
            "Epoch: 0 batch_num: 272 val_rmse: 1.025\n",
            "Still best_val_rmse: 0.6047 (from epoch 0)\n",
            "\n",
            "16 steps took 6.64 seconds\n",
            "Epoch: 1 batch_num: 5 val_rmse: 1.101\n",
            "Still best_val_rmse: 0.6047 (from epoch 0)\n",
            "\n",
            "16 steps took 6.44 seconds\n",
            "Epoch: 1 batch_num: 21 val_rmse: 1.056\n",
            "Still best_val_rmse: 0.6047 (from epoch 0)\n",
            "\n",
            "16 steps took 6.43 seconds\n",
            "Epoch: 1 batch_num: 37 val_rmse: 1.03\n",
            "Still best_val_rmse: 0.6047 (from epoch 0)\n",
            "\n",
            "16 steps took 6.41 seconds\n",
            "Epoch: 1 batch_num: 53 val_rmse: 1.032\n",
            "Still best_val_rmse: 0.6047 (from epoch 0)\n",
            "\n",
            "16 steps took 6.41 seconds\n",
            "Epoch: 1 batch_num: 69 val_rmse: 1.074\n",
            "Still best_val_rmse: 0.6047 (from epoch 0)\n",
            "\n",
            "16 steps took 6.38 seconds\n",
            "Epoch: 1 batch_num: 85 val_rmse: 1.063\n",
            "Still best_val_rmse: 0.6047 (from epoch 0)\n",
            "\n",
            "16 steps took 6.4 seconds\n",
            "Epoch: 1 batch_num: 101 val_rmse: 1.028\n",
            "Still best_val_rmse: 0.6047 (from epoch 0)\n",
            "\n",
            "16 steps took 6.39 seconds\n",
            "Epoch: 1 batch_num: 117 val_rmse: 1.026\n",
            "Still best_val_rmse: 0.6047 (from epoch 0)\n",
            "\n",
            "16 steps took 6.45 seconds\n",
            "Epoch: 1 batch_num: 133 val_rmse: 1.026\n",
            "Still best_val_rmse: 0.6047 (from epoch 0)\n",
            "\n",
            "16 steps took 6.41 seconds\n",
            "Epoch: 1 batch_num: 149 val_rmse: 1.038\n",
            "Still best_val_rmse: 0.6047 (from epoch 0)\n",
            "\n",
            "16 steps took 6.39 seconds\n",
            "Epoch: 1 batch_num: 165 val_rmse: 1.042\n",
            "Still best_val_rmse: 0.6047 (from epoch 0)\n",
            "\n",
            "16 steps took 6.39 seconds\n",
            "Epoch: 1 batch_num: 181 val_rmse: 1.032\n",
            "Still best_val_rmse: 0.6047 (from epoch 0)\n",
            "\n",
            "16 steps took 6.42 seconds\n",
            "Epoch: 1 batch_num: 197 val_rmse: 1.026\n",
            "Still best_val_rmse: 0.6047 (from epoch 0)\n",
            "\n",
            "16 steps took 6.42 seconds\n",
            "Epoch: 1 batch_num: 213 val_rmse: 1.034\n",
            "Still best_val_rmse: 0.6047 (from epoch 0)\n",
            "\n",
            "16 steps took 6.39 seconds\n",
            "Epoch: 1 batch_num: 229 val_rmse: 1.026\n",
            "Still best_val_rmse: 0.6047 (from epoch 0)\n",
            "\n",
            "16 steps took 6.38 seconds\n",
            "Epoch: 1 batch_num: 245 val_rmse: 1.027\n",
            "Still best_val_rmse: 0.6047 (from epoch 0)\n",
            "\n",
            "16 steps took 6.42 seconds\n",
            "Epoch: 1 batch_num: 261 val_rmse: 1.036\n",
            "Still best_val_rmse: 0.6047 (from epoch 0)\n",
            "\n",
            "16 steps took 6.43 seconds\n",
            "Epoch: 1 batch_num: 277 val_rmse: 1.045\n",
            "Still best_val_rmse: 0.6047 (from epoch 0)\n",
            "\n",
            "16 steps took 6.65 seconds\n",
            "Epoch: 2 batch_num: 10 val_rmse: 1.037\n",
            "Still best_val_rmse: 0.6047 (from epoch 0)\n",
            "\n",
            "16 steps took 6.37 seconds\n",
            "Epoch: 2 batch_num: 26 val_rmse: 1.028\n",
            "Still best_val_rmse: 0.6047 (from epoch 0)\n",
            "\n",
            "16 steps took 6.4 seconds\n",
            "Epoch: 2 batch_num: 42 val_rmse: 1.036\n",
            "Still best_val_rmse: 0.6047 (from epoch 0)\n",
            "\n",
            "16 steps took 6.42 seconds\n",
            "Epoch: 2 batch_num: 58 val_rmse: 1.028\n",
            "Still best_val_rmse: 0.6047 (from epoch 0)\n",
            "\n",
            "16 steps took 6.38 seconds\n",
            "Epoch: 2 batch_num: 74 val_rmse: 1.027\n",
            "Still best_val_rmse: 0.6047 (from epoch 0)\n",
            "\n",
            "16 steps took 6.42 seconds\n",
            "Epoch: 2 batch_num: 90 val_rmse: 1.057\n",
            "Still best_val_rmse: 0.6047 (from epoch 0)\n",
            "\n",
            "16 steps took 6.38 seconds\n",
            "Epoch: 2 batch_num: 106 val_rmse: 1.026\n",
            "Still best_val_rmse: 0.6047 (from epoch 0)\n",
            "\n",
            "16 steps took 6.39 seconds\n",
            "Epoch: 2 batch_num: 122 val_rmse: 1.027\n",
            "Still best_val_rmse: 0.6047 (from epoch 0)\n",
            "\n",
            "16 steps took 6.38 seconds\n",
            "Epoch: 2 batch_num: 138 val_rmse: 1.027\n",
            "Still best_val_rmse: 0.6047 (from epoch 0)\n",
            "\n",
            "16 steps took 6.43 seconds\n",
            "Epoch: 2 batch_num: 154 val_rmse: 1.026\n",
            "Still best_val_rmse: 0.6047 (from epoch 0)\n",
            "\n",
            "16 steps took 6.42 seconds\n",
            "Epoch: 2 batch_num: 170 val_rmse: 1.026\n",
            "Still best_val_rmse: 0.6047 (from epoch 0)\n",
            "\n",
            "16 steps took 6.38 seconds\n",
            "Epoch: 2 batch_num: 186 val_rmse: 1.026\n",
            "Still best_val_rmse: 0.6047 (from epoch 0)\n",
            "\n",
            "16 steps took 6.38 seconds\n",
            "Epoch: 2 batch_num: 202 val_rmse: 1.028\n",
            "Still best_val_rmse: 0.6047 (from epoch 0)\n",
            "\n",
            "16 steps took 6.41 seconds\n",
            "Epoch: 2 batch_num: 218 val_rmse: 1.027\n",
            "Still best_val_rmse: 0.6047 (from epoch 0)\n",
            "\n",
            "16 steps took 6.42 seconds\n",
            "Epoch: 2 batch_num: 234 val_rmse: 1.027\n",
            "Still best_val_rmse: 0.6047 (from epoch 0)\n",
            "\n",
            "16 steps took 6.37 seconds\n",
            "Epoch: 2 batch_num: 250 val_rmse: 1.027\n",
            "Still best_val_rmse: 0.6047 (from epoch 0)\n",
            "\n",
            "16 steps took 6.38 seconds\n",
            "Epoch: 2 batch_num: 266 val_rmse: 1.027\n",
            "Still best_val_rmse: 0.6047 (from epoch 0)\n",
            "\n",
            "16 steps took 6.38 seconds\n",
            "Epoch: 2 batch_num: 282 val_rmse: 1.027\n",
            "Still best_val_rmse: 0.6047 (from epoch 0)\n",
            "\n",
            "Performance estimates:\n",
            "[0.5699093717282384, 0.6047088267629217]\n",
            "Mean: 0.5873090992455801\n",
            "\n",
            "Fold 3/5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at /content/drive/MyDrive/CommonLit/input/commonlitreadabilityprize/roberta-large-mlm/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "16 steps took 6.97 seconds\n",
            "Epoch: 0 batch_num: 16 val_rmse: 1.132\n",
            "New best_val_rmse: 1.132\n",
            "\n",
            "16 steps took 6.38 seconds\n",
            "Epoch: 0 batch_num: 32 val_rmse: 1.018\n",
            "New best_val_rmse: 1.018\n",
            "\n",
            "16 steps took 6.4 seconds\n",
            "Epoch: 0 batch_num: 48 val_rmse: 0.7914\n",
            "New best_val_rmse: 0.7914\n",
            "\n",
            "16 steps took 6.41 seconds\n",
            "Epoch: 0 batch_num: 64 val_rmse: 0.8238\n",
            "Still best_val_rmse: 0.7914 (from epoch 0)\n",
            "\n",
            "16 steps took 6.48 seconds\n",
            "Epoch: 0 batch_num: 80 val_rmse: 0.6624\n",
            "New best_val_rmse: 0.6624\n",
            "\n",
            "16 steps took 6.39 seconds\n",
            "Epoch: 0 batch_num: 96 val_rmse: 0.7462\n",
            "Still best_val_rmse: 0.6624 (from epoch 0)\n",
            "\n",
            "16 steps took 6.52 seconds\n",
            "Epoch: 0 batch_num: 112 val_rmse: 0.6702\n",
            "Still best_val_rmse: 0.6624 (from epoch 0)\n",
            "\n",
            "16 steps took 6.53 seconds\n",
            "Epoch: 0 batch_num: 128 val_rmse: 0.7321\n",
            "Still best_val_rmse: 0.6624 (from epoch 0)\n",
            "\n",
            "16 steps took 6.45 seconds\n",
            "Epoch: 0 batch_num: 144 val_rmse: 0.6924\n",
            "Still best_val_rmse: 0.6624 (from epoch 0)\n",
            "\n",
            "16 steps took 6.42 seconds\n",
            "Epoch: 0 batch_num: 160 val_rmse: 0.6126\n",
            "New best_val_rmse: 0.6126\n",
            "\n",
            "16 steps took 6.4 seconds\n",
            "Epoch: 0 batch_num: 176 val_rmse: 0.7067\n",
            "Still best_val_rmse: 0.6126 (from epoch 0)\n",
            "\n",
            "16 steps took 6.52 seconds\n",
            "Epoch: 0 batch_num: 192 val_rmse: 0.7244\n",
            "Still best_val_rmse: 0.6126 (from epoch 0)\n",
            "\n",
            "16 steps took 6.54 seconds\n",
            "Epoch: 0 batch_num: 208 val_rmse: 0.6413\n",
            "Still best_val_rmse: 0.6126 (from epoch 0)\n",
            "\n",
            "16 steps took 6.41 seconds\n",
            "Epoch: 0 batch_num: 224 val_rmse: 0.6072\n",
            "New best_val_rmse: 0.6072\n",
            "\n",
            "16 steps took 6.41 seconds\n",
            "Epoch: 0 batch_num: 240 val_rmse: 0.5746\n",
            "New best_val_rmse: 0.5746\n",
            "\n",
            "16 steps took 6.42 seconds\n",
            "Epoch: 0 batch_num: 256 val_rmse: 0.6323\n",
            "Still best_val_rmse: 0.5746 (from epoch 0)\n",
            "\n",
            "16 steps took 6.53 seconds\n",
            "Epoch: 0 batch_num: 272 val_rmse: 0.5362\n",
            "New best_val_rmse: 0.5362\n",
            "\n",
            "16 steps took 6.62 seconds\n",
            "Epoch: 1 batch_num: 5 val_rmse: 0.5826\n",
            "Still best_val_rmse: 0.5362 (from epoch 0)\n",
            "\n",
            "16 steps took 6.54 seconds\n",
            "Epoch: 1 batch_num: 21 val_rmse: 0.6243\n",
            "Still best_val_rmse: 0.5362 (from epoch 0)\n",
            "\n",
            "16 steps took 6.48 seconds\n",
            "Epoch: 1 batch_num: 37 val_rmse: 0.5315\n",
            "New best_val_rmse: 0.5315\n",
            "\n",
            "16 steps took 6.43 seconds\n",
            "Epoch: 1 batch_num: 53 val_rmse: 0.5544\n",
            "Still best_val_rmse: 0.5315 (from epoch 1)\n",
            "\n",
            "16 steps took 6.52 seconds\n",
            "Epoch: 1 batch_num: 69 val_rmse: 0.6469\n",
            "Still best_val_rmse: 0.5315 (from epoch 1)\n",
            "\n",
            "16 steps took 6.49 seconds\n",
            "Epoch: 1 batch_num: 85 val_rmse: 0.5389\n",
            "Still best_val_rmse: 0.5315 (from epoch 1)\n",
            "\n",
            "16 steps took 6.45 seconds\n",
            "Epoch: 1 batch_num: 101 val_rmse: 0.5174\n",
            "New best_val_rmse: 0.5174\n",
            "\n",
            "16 steps took 6.41 seconds\n",
            "Epoch: 1 batch_num: 117 val_rmse: 0.5775\n",
            "Still best_val_rmse: 0.5174 (from epoch 1)\n",
            "\n",
            "16 steps took 6.54 seconds\n",
            "Epoch: 1 batch_num: 133 val_rmse: 0.59\n",
            "Still best_val_rmse: 0.5174 (from epoch 1)\n",
            "\n",
            "16 steps took 6.49 seconds\n",
            "Epoch: 1 batch_num: 149 val_rmse: 0.5135\n",
            "New best_val_rmse: 0.5135\n",
            "\n",
            "16 steps took 6.41 seconds\n",
            "Epoch: 1 batch_num: 165 val_rmse: 0.5164\n",
            "Still best_val_rmse: 0.5135 (from epoch 1)\n",
            "\n",
            "16 steps took 6.49 seconds\n",
            "Epoch: 1 batch_num: 181 val_rmse: 0.5213\n",
            "Still best_val_rmse: 0.5135 (from epoch 1)\n",
            "\n",
            "16 steps took 6.51 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.256017,
          "end_time": "2021-07-10T17:19:20.427150",
          "exception": false,
          "start_time": "2021-07-10T17:19:20.171133",
          "status": "completed"
        },
        "tags": [],
        "id": "exposed-cornell"
      },
      "source": [
        "# Inference"
      ],
      "id": "exposed-cornell"
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.254779,
          "end_time": "2021-07-10T17:19:20.959408",
          "exception": false,
          "start_time": "2021-07-10T17:19:20.704629",
          "status": "completed"
        },
        "tags": [],
        "id": "pacific-explanation"
      },
      "source": [
        ""
      ],
      "id": "pacific-explanation",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-10T17:19:21.485458Z",
          "iopub.status.busy": "2021-07-10T17:19:21.484260Z",
          "iopub.status.idle": "2021-07-10T17:19:21.509269Z",
          "shell.execute_reply": "2021-07-10T17:19:21.508604Z"
        },
        "papermill": {
          "duration": 0.29265,
          "end_time": "2021-07-10T17:19:21.509426",
          "exception": false,
          "start_time": "2021-07-10T17:19:21.216776",
          "status": "completed"
        },
        "tags": [],
        "id": "speaking-authority"
      },
      "source": [
        "test_dataset = LitDataset(test_df, inference_only=True)"
      ],
      "id": "speaking-authority",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-10T17:19:22.019508Z",
          "iopub.status.busy": "2021-07-10T17:19:22.018338Z",
          "iopub.status.idle": "2021-07-10T17:19:51.285549Z",
          "shell.execute_reply": "2021-07-10T17:19:51.284997Z"
        },
        "papermill": {
          "duration": 29.52399,
          "end_time": "2021-07-10T17:19:51.285732",
          "exception": false,
          "start_time": "2021-07-10T17:19:21.761742",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "destroyed-interval",
        "outputId": "e8b6aa74-49a1-4dc6-e6c6-9f32550a0016"
      },
      "source": [
        "all_predictions = np.zeros((len(list_val_rmse), len(test_df)))\n",
        "\n",
        "test_dataset = LitDataset(test_df, inference_only=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                         drop_last=False, shuffle=False, num_workers=2)\n",
        "\n",
        "for index in range(len(list_val_rmse)):            \n",
        "    # model_path = f\"/content/drive/MyDrive/CommonLit/output/roberta-large-meanpooling-pretrain/model_{index + 1}.pth\"\n",
        "    model_path = PATH + f\"model_{fold + 1}.pth\"\n",
        "    print(f\"\\nUsing {model_path}\")\n",
        "                        \n",
        "    model = LitModel()\n",
        "    model.load_state_dict(torch.load(model_path))    \n",
        "    model.to(DEVICE)\n",
        "    \n",
        "    all_predictions[index] = predict(model, test_loader)\n",
        "    \n",
        "    del model\n",
        "    gc.collect()"
      ],
      "id": "destroyed-interval",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Using /content/drive/MyDrive/CommonLit/output/roberta-large-meanpooling-pretrain_steps200/model_5.pth\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at /content/drive/MyDrive/CommonLit/input/commonlitreadabilityprize/roberta-large-mlm/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Using /content/drive/MyDrive/CommonLit/output/roberta-large-meanpooling-pretrain_steps200/model_5.pth\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at /content/drive/MyDrive/CommonLit/input/commonlitreadabilityprize/roberta-large-mlm/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Using /content/drive/MyDrive/CommonLit/output/roberta-large-meanpooling-pretrain_steps200/model_5.pth\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at /content/drive/MyDrive/CommonLit/input/commonlitreadabilityprize/roberta-large-mlm/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Using /content/drive/MyDrive/CommonLit/output/roberta-large-meanpooling-pretrain_steps200/model_5.pth\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at /content/drive/MyDrive/CommonLit/input/commonlitreadabilityprize/roberta-large-mlm/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Using /content/drive/MyDrive/CommonLit/output/roberta-large-meanpooling-pretrain_steps200/model_5.pth\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at /content/drive/MyDrive/CommonLit/input/commonlitreadabilityprize/roberta-large-mlm/clrp_roberta_large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-07-10T17:19:51.815506Z",
          "iopub.status.busy": "2021-07-10T17:19:51.814838Z",
          "iopub.status.idle": "2021-07-10T17:19:52.977737Z",
          "shell.execute_reply": "2021-07-10T17:19:52.977091Z"
        },
        "papermill": {
          "duration": 1.434043,
          "end_time": "2021-07-10T17:19:52.977904",
          "exception": false,
          "start_time": "2021-07-10T17:19:51.543861",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meaningful-petersburg",
        "outputId": "1f46eb1f-da9c-4118-b875-ab90860c7e9b"
      },
      "source": [
        "predictions = all_predictions.mean(axis=0)\n",
        "submission_df.target = predictions\n",
        "print(submission_df)\n",
        "#submission_df.to_csv(\"submission.csv\", index=False)"
      ],
      "id": "meaningful-petersburg",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "          id    target\n",
            "0  c0f722661 -0.312008\n",
            "1  f0953f0a5 -0.460103\n",
            "2  0df072751 -0.416939\n",
            "3  04caf4e0c -2.408086\n",
            "4  0e63f8bea -1.988951\n",
            "5  12537fe78 -1.163463\n",
            "6  965e592c0  0.345657\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KS-MlzVLkY1i"
      },
      "source": [
        "## Upload model"
      ],
      "id": "KS-MlzVLkY1i"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABRGJe13CbpL",
        "outputId": "502f22fe-02bf-4866-e31e-071f3aca627d"
      },
      "source": [
        "!kaggle datasets init -p /content/drive/MyDrive/CommonLit/input\n",
        "\n",
        "\n",
        "!pip install -q kaggle\n",
        "!mkdir /root/.kaggle\n",
        "!cp /content/drive/MyDrive/Colab\\ Notebooks/kaggle.json /root/.kaggle/"
      ],
      "id": "ABRGJe13CbpL",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data package template written to: /content/drive/MyDrive/CommonLit/input/dataset-metadata.json\n",
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_28uXt0kYSU",
        "outputId": "ac081fc7-2e56-49e6-a3ed-1fbdf319324e"
      },
      "source": [
        "!mkdir -p ./output/\n",
        "# !cp -f ./model* ./output/\n",
        "!cp -f /content/drive/MyDrive/CommonLit/output/roberta-large-meanpooling-pretrain_steps200/model*\n",
        "!echo done\n",
        "#CHANGEME\n",
        "!cp -f /content/drive/MyDrive/CommonLit/input/commonlitreadabilityprize/dataset-metadata.json /content/output/dataset-metadata.json\n",
        "!echo done\n",
        "!sed -i -e \"s/roberta-base/roberta-large-`TZ=JST-9 date +\"%Y%m%d%H%M%S\"`/\" ./output/dataset-metadata.json\n",
        "!sed -i -e \"s/Roberta-base/Roberta-large-`TZ=JST-9 date +\"%m%d%H%M%S\"`/\" ./output/dataset-metadata.json\n",
        "!echo done\n",
        "!cat ./output/dataset-metadata.json\n",
        "!kaggle datasets create -p /content/output/"
      ],
      "id": "x_28uXt0kYSU",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cp: target '/content/drive/MyDrive/CommonLit/output/roberta-large-meanpooling-pretrain_steps200/model_5.pth' is not a directory\n",
            "done\n",
            "done\n",
            "done\n",
            "{\n",
            "  \"licenses\": [\n",
            "    {\n",
            "      \"name\": \"CC0-1.0\"\n",
            "    }\n",
            "  ], \n",
            "  \"id\": \"takeshikobayashi/roberta-large-maunish-meanpooling_pretrain_step200\", \n",
            "  \"title\": \"roberta-large-maunish-meanpooling_pretrain_step200\"\n",
            "}400 - Bad Request\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iD1Cot2sk7A_",
        "outputId": "40410e2c-fd14-42c7-db5e-6369fb9aa62f"
      },
      "source": [
        "!cat ./output/dataset-metadata.json"
      ],
      "id": "iD1Cot2sk7A_",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"licenses\": [\n",
            "    {\n",
            "      \"name\": \"CC0-1.0\"\n",
            "    }\n",
            "  ], \n",
            "  \"id\": \"takeshikobayashi/roberta-large-maunish-meanpooling_pretrain_step200\", \n",
            "  \"title\": \"roberta-large-maunish-meanpooling_pretrain_step200\"\n",
            "}"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uggb4d7eGsdM"
      },
      "source": [
        ""
      ],
      "id": "uggb4d7eGsdM",
      "execution_count": 36,
      "outputs": []
    }
  ]
}