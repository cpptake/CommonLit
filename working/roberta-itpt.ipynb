{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"colab":{"name":"commonlit-readability-prize-roberta-torch-itpt.ipynb","provenance":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"sRtie5mWP2na"},"source":["## Further Pre-training\n","\n","Besides the training data of a target task, we can further pre-train a transformer on the data from the same domain.\n","\n","![image.png](https://media.springernature.com/original/springer-static/image/chp%3A10.1007%2F978-3-030-32381-3_16/MediaObjects/489562_1_En_16_Fig1_HTML.png)\n","\n","The Transformer models are pre-trained on the general domain corpus. For a text classification task / regression task in a specific domain, such as Readability Assesment, its data\n","distribution may be different from a transformer trained on a different corpus e.g. RoBERTa trained on BookCorpus, Wiki, CC-News, OpenWebText, Stories. Therefore the idea is, we can further pre-train the transformer with masked language model and next sentence prediction tasks on the domain-specific data. Three further pretraining approaches are performed:\n","\n","1) `Within-task pre-training (ITPT)`, in which transformer is further pre-trained on the training data of a target task. `This Kernel.`\n","\n","2) `In-domain pre-training (IDPT)`, in which the pretraining data is obtained from the same domain of a target task. For example, there are several different sentiment classification tasks, which have a similar data distribution. We can further pre-train the transformer on the combined training data from these tasks.\n","\n","3) `Cross-domain pre-training (CDPT)`, in which the pretraining data is obtained from both the same and other different domains to a target task.\n","\n","#### Reference: [How to finetune BERT for Text Classification ?](https://arxiv.org/pdf/1905.05583.pdf)\n","\n","> Note: This Kernel implements ITPT i.e. Within-Task Pretraining. First we will pretrain a RoBERTa model and then utilize the same for further finetuing tasks using different strategies."]},{"cell_type":"markdown","metadata":{"id":"U_V_GYrWP2ne"},"source":["#### Code Reference: \n","`Transformer Examples` - https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm_no_trainer.py\n","\n","> 90-95% of the code is from this great `run_mlm_no_trainer.py script` from `HuggingFace Examples Repository`. I have merely `changed few lines to adjust the code according to my task`. \n","    \n","    P.S. Make sure to understand everything instead of blindly copying the code."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zH4fzK99QEXj","executionInfo":{"status":"ok","timestamp":1625750164683,"user_tz":-540,"elapsed":420,"user":{"displayName":"小林健史","photoUrl":"","userId":"11137570562930914237"}},"outputId":"c8d5985e-9736-4be4-e3e5-f05b1481e28d"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Thu Jul  8 13:16:06 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   32C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iRd1r2reQEUj","executionInfo":{"status":"ok","timestamp":1626502980198,"user_tz":-540,"elapsed":29759,"user":{"displayName":"小林健史","photoUrl":"","userId":"11137570562930914237"}},"outputId":"5ca41b51-f9d0-4022-f90e-0692b72b540e"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","!pip install -q kaggle\n","!mkdir /root/.kaggle\n","!cp /content/drive/MyDrive/Colab\\ Notebooks/kaggle.json /root/.kaggle/"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hcyl5u1-QESG"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WIw7aTQBQEPi"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PNhh-31nQENJ"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F29DNPFTQEKb"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cU1dLZbqQEH2"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uglDpzp6QD9P"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3glPx3KfP2nf"},"source":["### Install Dependencies"]},{"cell_type":"code","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-06-26T12:20:04.86078Z","iopub.execute_input":"2021-06-26T12:20:04.861283Z","iopub.status.idle":"2021-06-26T12:20:13.752333Z","shell.execute_reply.started":"2021-06-26T12:20:04.861168Z","shell.execute_reply":"2021-06-26T12:20:13.751336Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"UAQvtKtkP2nh","executionInfo":{"status":"ok","timestamp":1625750208017,"user_tz":-540,"elapsed":12297,"user":{"displayName":"小林健史","photoUrl":"","userId":"11137570562930914237"}},"outputId":"16dcb814-fe09-41b6-c864-0643819f2ae3"},"source":["!pip install transformers datasets accelerate\n","!pip install transformers\n","!pip install colorama"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/1a/41c644c963249fd7f3836d926afa1e3f1cc234a1c40d80c5f03ad8f6f1b2/transformers-4.8.2-py3-none-any.whl (2.5MB)\n","\u001b[K     |████████████████████████████████| 2.5MB 13.7MB/s \n","\u001b[?25hCollecting datasets\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/27/9c91ddee87b06d2de12f134c5171a49890427e398389f07f6463485723c3/datasets-1.9.0-py3-none-any.whl (262kB)\n","\u001b[K     |████████████████████████████████| 266kB 51.0MB/s \n","\u001b[?25hCollecting accelerate\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/fa/d173d923c953d930702066894abf128a7e5258c6f64cf088d2c5a83f46a3/accelerate-0.3.0-py3-none-any.whl (49kB)\n","\u001b[K     |████████████████████████████████| 51kB 9.2MB/s \n","\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.5.0)\n","Collecting huggingface-hub==0.0.12\n","  Downloading https://files.pythonhosted.org/packages/2f/ee/97e253668fda9b17e968b3f97b2f8e53aa0127e8807d24a547687423fe0b/huggingface_hub-0.0.12-py3-none-any.whl\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n","\u001b[K     |████████████████████████████████| 901kB 53.9MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Collecting tokenizers<0.11,>=0.10.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n","\u001b[K     |████████████████████████████████| 3.3MB 51.1MB/s \n","\u001b[?25hCollecting xxhash\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/4f/0a862cad26aa2ed7a7cd87178cbbfa824fc1383e472d63596a0d018374e7/xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243kB)\n","\u001b[K     |████████████████████████████████| 245kB 52.5MB/s \n","\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n","Collecting fsspec>=2021.05.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/3a/666e63625a19883ae8e1674099e631f9737bd5478c4790e5ad49c5ac5261/fsspec-2021.6.1-py3-none-any.whl (115kB)\n","\u001b[K     |████████████████████████████████| 122kB 60.1MB/s \n","\u001b[?25hRequirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n","Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from accelerate) (1.9.0+cu102)\n","Collecting pyaml>=20.4.0\n","  Downloading https://files.pythonhosted.org/packages/15/c4/1310a054d33abc318426a956e7d6df0df76a6ddfa9c66f6310274fb75d42/pyaml-20.4.0-py2.py3-none-any.whl\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n","Installing collected packages: huggingface-hub, sacremoses, tokenizers, transformers, xxhash, fsspec, datasets, pyaml, accelerate\n","Successfully installed accelerate-0.3.0 datasets-1.9.0 fsspec-2021.6.1 huggingface-hub-0.0.12 pyaml-20.4.0 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.8.2 xxhash-2.0.2\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.8.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.5.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Collecting colorama\n","  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n","Installing collected packages: colorama\n","Successfully installed colorama-0.4.4\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"n5fWt8vxP2nk"},"source":["### Load CommonLit Readability Data"]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-06-26T12:40:04.433181Z","iopub.execute_input":"2021-06-26T12:40:04.433495Z","iopub.status.idle":"2021-06-26T12:40:04.83693Z","shell.execute_reply.started":"2021-06-26T12:40:04.433467Z","shell.execute_reply":"2021-06-26T12:40:04.83612Z"},"trusted":true,"id":"XFH0WAYWP2nm"},"source":["import pandas as pd\n","import numpy as np\n","\n","train = pd.read_csv('/content/drive/MyDrive/CommonLit/input/commonlitreadabilityprize/train.csv')#../input/commonlitreadabilityprize/train.csv\n","test = pd.read_csv('/content/drive/MyDrive/CommonLit/input/commonlitreadabilityprize/test.csv')#../input/commonlitreadabilityprize/train.csv\n","\n","mlm_data = train[['excerpt']]\n","mlm_data = mlm_data.rename(columns={'excerpt':'text'})\n","mlm_data.to_csv('mlm_data.csv', index=False)\n","\n","mlm_data_val = test[['excerpt']]\n","mlm_data_val = mlm_data_val.rename(columns={'excerpt':'text'})\n","mlm_data_val.to_csv('mlm_data_val.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-06-26T12:40:04.838432Z","iopub.execute_input":"2021-06-26T12:40:04.838906Z","iopub.status.idle":"2021-06-26T12:40:04.854604Z","shell.execute_reply.started":"2021-06-26T12:40:04.838869Z","shell.execute_reply":"2021-06-26T12:40:04.853556Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/","height":414},"id":"w-7cRqMvP2oP","executionInfo":{"status":"ok","timestamp":1625750279853,"user_tz":-540,"elapsed":24,"user":{"displayName":"小林健史","photoUrl":"","userId":"11137570562930914237"}},"outputId":"8ee59d32-4ca7-45d8-f63e-43480e34137b"},"source":["mlm_data"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>When the young people returned to the ballroom...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>All through dinner time, Mrs. Fayre was somewh...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>As Roger had predicted, the snow departed as q...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>And outside before the palace a great garden w...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Once upon a time there were Three Bears who li...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2829</th>\n","      <td>When you think of dinosaurs and where they liv...</td>\n","    </tr>\n","    <tr>\n","      <th>2830</th>\n","      <td>So what is a solid? Solids are usually hard be...</td>\n","    </tr>\n","    <tr>\n","      <th>2831</th>\n","      <td>The second state of matter we will discuss is ...</td>\n","    </tr>\n","    <tr>\n","      <th>2832</th>\n","      <td>Solids are shapes that you can actually touch....</td>\n","    </tr>\n","    <tr>\n","      <th>2833</th>\n","      <td>Animals are made of many cells. They eat thing...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2834 rows × 1 columns</p>\n","</div>"],"text/plain":["                                                   text\n","0     When the young people returned to the ballroom...\n","1     All through dinner time, Mrs. Fayre was somewh...\n","2     As Roger had predicted, the snow departed as q...\n","3     And outside before the palace a great garden w...\n","4     Once upon a time there were Three Bears who li...\n","...                                                 ...\n","2829  When you think of dinosaurs and where they liv...\n","2830  So what is a solid? Solids are usually hard be...\n","2831  The second state of matter we will discuss is ...\n","2832  Solids are shapes that you can actually touch....\n","2833  Animals are made of many cells. They eat thing...\n","\n","[2834 rows x 1 columns]"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"dlrfJJiZP2oQ"},"source":["### Import Dependencies"]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-06-26T12:38:53.937867Z","iopub.execute_input":"2021-06-26T12:38:53.938326Z","iopub.status.idle":"2021-06-26T12:38:55.124129Z","shell.execute_reply.started":"2021-06-26T12:38:53.938284Z","shell.execute_reply":"2021-06-26T12:38:55.123026Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/","height":242},"id":"2AgNL0BlP2oR","executionInfo":{"status":"error","timestamp":1625750288013,"user_tz":-540,"elapsed":2783,"user":{"displayName":"小林健史","photoUrl":"","userId":"11137570562930914237"}},"outputId":"bf6ca748-5492-4685-cb17-e56a913162c0"},"source":["import argparse\n","import logging\n","import math\n","import os\n","import random\n","\n","# import datasets\n","# from datasets import load_dataset\n","from tqdm.auto import tqdm\n","# from accelerate import Accelerator\n","\n","import torch\n","from torch.utils.data import DataLoader\n","\n","import transformers\n","# from transformers import (\n","#     CONFIG_MAPPING, \n","#     MODEL_MAPPING, \n","#     AdamW, \n","#     AutoConfig, \n","#     AutoModelForMaskedLM, \n","#     AutoTokenizer, \n","#     DataCollatorForLanguageModeling, \n","#     SchedulerType, \n","#     get_scheduler, \n","#     set_seed\n","# )\n","\n","logger = logging.getLogger(__name__)\n","MODEL_CONFIG_CLASSES = list(MODEL_MAPPING.keys())\n","MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n","\n","# from pprint import pprint\n","# pprint(MODEL_TYPES, width=3, compact=True)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-ec6eede95199>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mMODEL_CONFIG_CLASSES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_MAPPING\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0mMODEL_TYPES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_type\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mconf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mMODEL_CONFIG_CLASSES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'MODEL_MAPPING' is not defined"]}]},{"cell_type":"code","metadata":{"id":"ZjmrvgUvP2oS"},"source":["from transformers import CONFIG_MAPPING\n","#     CONFIG_MAPPING, \n","#     MODEL_MAPPING, \n","#     AdamW, \n","#     AutoConfig, \n","#     AutoModelForMaskedLM, \n","#     AutoTokenizer, \n","#     DataCollatorForLanguageModeling, \n","#     SchedulerType, \n","#     get_scheduler, \n","#     set_seed\n","# )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K66EiNAPP2oT"},"source":["### Config"]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-06-26T12:20:15.918196Z","iopub.status.idle":"2021-06-26T12:20:15.918911Z"},"trusted":true,"id":"OsqvtKDbP2oT"},"source":["class TrainConfig:\n","    train_file= 'mlm_data.csv'\n","    validation_file = 'mlm_data.csv'\n","    validation_split_percentage= 5\n","    pad_to_max_length= True\n","    model_name_or_path= 'robertlarge'#roberta-base\n","    config_name= 'robertalarge'#roberta-base\n","    tokenizer_name= 'robertalarge'#roberta-base\n","    use_slow_tokenizer= True\n","    per_device_train_batch_size= 8\n","    per_device_eval_batch_size= 8\n","    learning_rate= 5e-5\n","    weight_decay= 0.0\n","    num_train_epochs= 1 # change to 5\n","    max_train_steps= None\n","    gradient_accumulation_steps= 1\n","    lr_scheduler_type= 'constant_with_warmup'\n","    num_warmup_steps= 0\n","    output_dir= 'output'\n","    seed= 2021\n","    model_type= 'roberta'\n","    max_seq_length= None\n","    line_by_line= False\n","    preprocessing_num_workers= 4\n","    overwrite_cache= True\n","    mlm_probability= 0.15\n","\n","config = TrainConfig()\n","\n","if config.train_file is not None:\n","    extension = config.train_file.split(\".\")[-1]\n","    assert extension in [\"csv\", \"json\", \"txt\"], \"`train_file` should be a csv, json or txt file.\"\n","if config.validation_file is not None:\n","    extension = config.validation_file.split(\".\")[-1]\n","    assert extension in [\"csv\", \"json\", \"txt\"], \"`validation_file` should be a csv, json or txt file.\"\n","if config.output_dir is not None:\n","    os.makedirs(config.output_dir, exist_ok=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iz-tUu--P2oV"},"source":["### Run"]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-06-26T12:20:15.920365Z","iopub.status.idle":"2021-06-26T12:20:15.920941Z"},"trusted":true,"id":"gclJgVn5P2oV"},"source":["def main():\n","    args = TrainConfig()\n","    accelerator = Accelerator()\n","    logging.basicConfig(\n","        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n","        datefmt=\"%m/%d/%Y %H:%M:%S\",\n","        level=logging.INFO,\n","    )\n","    logger.info(accelerator.state)\n","    logger.setLevel(logging.INFO if accelerator.is_local_main_process else logging.ERROR)\n","\n","    if accelerator.is_local_main_process:\n","        datasets.utils.logging.set_verbosity_warning()\n","        transformers.utils.logging.set_verbosity_info()\n","    else:\n","        datasets.utils.logging.set_verbosity_error()\n","        transformers.utils.logging.set_verbosity_error()\n","    if args.seed is not None:\n","        set_seed(args.seed)\n","\n","    data_files = {}\n","    if args.train_file is not None:\n","        data_files[\"train\"] = args.train_file\n","    if args.validation_file is not None:\n","        data_files[\"validation\"] = args.validation_file\n","    extension = args.train_file.split(\".\")[-1]\n","    if extension == \"txt\":\n","        extension = \"text\"\n","    raw_datasets = load_dataset(extension, data_files=data_files)\n","    \n","    if args.config_name:\n","        config = AutoConfig.from_pretrained(args.config_name)\n","    elif config.model_name_or_path:\n","        config = AutoConfig.from_pretrained(args.model_name_or_path)\n","    else:\n","        config = CONFIG_MAPPING[args.model_type]()\n","        logger.warning(\"You are instantiating a new config instance from scratch.\")\n","\n","    if args.tokenizer_name:\n","        tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, use_fast=not args.use_slow_tokenizer)\n","    elif args.model_name_or_path:\n","        tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, use_fast=not args.use_slow_tokenizer)\n","    else:\n","        raise ValueError(\n","            \"You are instantiating a new tokenizer from scratch. This is not supported by this script.\"\n","            \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n","        )\n","    \n","    if args.model_name_or_path:\n","        model = AutoModelForMaskedLM.from_pretrained(\n","            args.model_name_or_path,\n","            from_tf=bool(\".ckpt\" in args.model_name_or_path),\n","            config=config,\n","        )\n","    else:\n","        logger.info(\"Training new model from scratch\")\n","        model = AutoModelForMaskedLM.from_config(config)\n","\n","    model.resize_token_embeddings(len(tokenizer))\n","\n","    column_names = raw_datasets[\"train\"].column_names\n","    text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n","\n","    if args.max_seq_length is None:\n","        max_seq_length = tokenizer.model_max_length\n","        if max_seq_length > 1024:\n","            logger.warning(\n","                f\"The tokenizer picked seems to have a very large `model_max_length` ({tokenizer.model_max_length}). \"\n","                \"Picking 1024 instead. You can change that default value by passing --max_seq_length xxx.\"\n","            )\n","            max_seq_length = 1024\n","    else:\n","        if args.max_seq_length > tokenizer.model_max_length:\n","            logger.warning(\n","                f\"The max_seq_length passed ({args.max_seq_length}) is larger than the maximum length for the\"\n","                f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\"\n","            )\n","        max_seq_length = min(args.max_seq_length, tokenizer.model_max_length)\n","\n","    def tokenize_function(examples):\n","        return tokenizer(examples[text_column_name], return_special_tokens_mask=True)\n","\n","    tokenized_datasets = raw_datasets.map(\n","        tokenize_function,\n","        batched=True,\n","        num_proc=args.preprocessing_num_workers,\n","        remove_columns=column_names,\n","        load_from_cache_file=not args.overwrite_cache,\n","    )\n","\n","    def group_texts(examples):\n","        concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n","        total_length = len(concatenated_examples[list(examples.keys())[0]])\n","        total_length = (total_length // max_seq_length) * max_seq_length\n","        result = {\n","            k: [t[i : i + max_seq_length] for i in range(0, total_length, max_seq_length)]\n","            for k, t in concatenated_examples.items()\n","        }\n","        return result\n","\n","    tokenized_datasets = tokenized_datasets.map(\n","        group_texts,\n","        batched=True,\n","        num_proc=args.preprocessing_num_workers,\n","        load_from_cache_file=not args.overwrite_cache,\n","    )\n","    train_dataset = tokenized_datasets[\"train\"]\n","    eval_dataset = tokenized_datasets[\"validation\"]\n","\n","    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=args.mlm_probability)\n","    train_dataloader = DataLoader(\n","        train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size\n","    )\n","    eval_dataloader = DataLoader(eval_dataset, collate_fn=data_collator, batch_size=args.per_device_eval_batch_size)\n","\n","    no_decay = [\"bias\", \"LayerNorm.weight\"]\n","    optimizer_grouped_parameters = [\n","        {\n","            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n","            \"weight_decay\": args.weight_decay,\n","        },\n","        {\n","            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n","            \"weight_decay\": 0.0,\n","        },\n","    ]\n","    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n","\n","    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n","        model, optimizer, train_dataloader, eval_dataloader\n","    )\n","\n","    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n","    if args.max_train_steps is None:\n","        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n","    else:\n","        args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n","\n","    lr_scheduler = get_scheduler(\n","        name=args.lr_scheduler_type,\n","        optimizer=optimizer,\n","        num_warmup_steps=args.num_warmup_steps,\n","        num_training_steps=args.max_train_steps,\n","    )\n","\n","    total_batch_size = args.per_device_train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n","\n","    logger.info(\"***** Running training *****\")\n","    logger.info(f\"  Num examples = {len(train_dataset)}\")\n","    logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n","    logger.info(f\"  Instantaneous batch size per device = {args.per_device_train_batch_size}\")\n","    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n","    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n","    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n","    # Only show the progress bar once on each machine.\n","    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n","    completed_steps = 0\n","\n","    for epoch in range(args.num_train_epochs):\n","        model.train()\n","        for step, batch in enumerate(train_dataloader):\n","            outputs = model(**batch)\n","            loss = outputs.loss\n","            loss = loss / args.gradient_accumulation_steps\n","            accelerator.backward(loss)\n","            if step % args.gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:\n","                optimizer.step()\n","                lr_scheduler.step()\n","                optimizer.zero_grad()\n","                progress_bar.update(1)\n","                completed_steps += 1\n","\n","            if completed_steps >= args.max_train_steps:\n","                break\n","\n","        model.eval()\n","        losses = []\n","        for step, batch in enumerate(eval_dataloader):\n","            with torch.no_grad():\n","                outputs = model(**batch)\n","\n","            loss = outputs.loss\n","            losses.append(accelerator.gather(loss.repeat(args.per_device_eval_batch_size)))\n","\n","        losses = torch.cat(losses)\n","        losses = losses[: len(eval_dataset)]\n","        perplexity = math.exp(torch.mean(losses))\n","\n","        logger.info(f\"epoch {epoch}: perplexity: {perplexity}\")\n","\n","    if args.output_dir is not None:\n","        accelerator.wait_for_everyone()\n","        unwrapped_model = accelerator.unwrap_model(model)\n","        unwrapped_model.save_pretrained(args.output_dir, save_function=accelerator.save)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"execution":{"iopub.status.busy":"2021-06-26T12:20:15.922053Z","iopub.status.idle":"2021-06-26T12:20:15.922647Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/","height":313},"id":"PN47YorfP2ox","executionInfo":{"status":"error","timestamp":1625750289037,"user_tz":-540,"elapsed":8,"user":{"displayName":"小林健史","photoUrl":"","userId":"11137570562930914237"}},"outputId":"ae4446c1-df69-4649-be9c-507f474b4b15"},"source":["if __name__ == \"__main__\":\n","    main()"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-972361fa1b80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-7-ab38354fd119>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0maccelerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAccelerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     logging.basicConfig(\n\u001b[1;32m      5\u001b[0m         \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'TrainConfig' is not defined"]}]},{"cell_type":"code","metadata":{"id":"1_loKT2JRLe6"},"source":[""],"execution_count":null,"outputs":[]}]}